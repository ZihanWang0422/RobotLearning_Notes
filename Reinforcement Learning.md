## Reinforcement Learning

[Python3 é¢å‘å¯¹è±¡ | èœé¸Ÿæ•™ç¨‹](https://www.runoob.com/python3/python3-class.html)

[è˜‘è‡ä¹¦EasyRL](https://datawhalechina.github.io/easy-rl/#/)

## Chapter 1 Intro

![alt text](image-1.png)



1.1å¥–åŠ±

ç”±ç¯å¢ƒç»™çš„ä¸€ç§æ ‡é‡çš„åé¦ˆä¿¡å·ï¼ˆscalar feedback signalï¼‰ï¼Œè¿™ç§ä¿¡å·å¯æ˜¾ç¤ºæ™ºèƒ½ä½“åœ¨æŸä¸€æ­¥é‡‡å–æŸä¸ªç­–ç•¥çš„è¡¨ç°å¦‚ä½•ã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®çš„å°±æ˜¯æœ€å¤§åŒ–æ™ºèƒ½ä½“å¯ä»¥è·å¾—çš„å¥–åŠ±ï¼Œæ™ºèƒ½ä½“åœ¨ç¯å¢ƒé‡Œé¢å­˜åœ¨çš„ç›® çš„å°±æ˜¯æœ€å¤§åŒ–å®ƒçš„æœŸæœ›çš„ç´¯ç§¯å¥–åŠ±ï¼ˆexpected cumulative rewardï¼‰ã€‚



1.2 æ™ºèƒ½ä½“ç»„æˆéƒ¨åˆ†

- **ç­–ç•¥ï¼ˆpolicyï¼‰**ï¼šå°†è¾“å…¥çš„çŠ¶æ€å˜æˆå¯èƒ½é‡‡å–çš„åŠ¨ä½œçš„æ¦‚ç‡ï¼Œé€šå¸¸é‡‡ç”¨éšæœºç­–ç•¥ï¼Œåœ¨å­¦ä¹ æ—¶å¯ä»¥é€šè¿‡å¼•å…¥ä¸€å®šçš„éšæœºæ€§æ¥æ›´å¥½åœ°æ¢ç´¢ç¯å¢ƒã€‚

**éšæœºæ€§ç­–ç•¥ï¼ˆÏ€å‡½æ•°ï¼‰**ï¼š$\pi(a|s)=p\left(a_{t}=a|s_{t}=s\right)$è¾“å…¥çŠ¶æ€sï¼Œè¾“å‡ºæ™ºèƒ½ä½“æ‰€æœ‰åŠ¨ä½œå¾—æ¦‚ç‡ã€‚

**ç¡®å®šæ€§ç­–ç•¥**ï¼š æ™ºèƒ½ä½“ç›´æ¥é‡‡å–æœ€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œ$a^*=\arg\max_a\pi(a\mid s)$



- **ä»·å€¼å‡½æ•°ï¼ˆvalue functionï¼‰**ã€‚æˆ‘ä»¬ç”¨ä»·å€¼å‡½æ•°æ¥å¯¹å½“å‰çŠ¶æ€è¿›è¡Œè¯„ä¼°ã€‚ä»·å€¼å‡½æ•°ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“è¿› å…¥æŸä¸ªçŠ¶æ€åï¼Œå¯ä»¥å¯¹åé¢çš„å¥–åŠ±å¸¦æ¥å¤šå¤§çš„å½±å“ã€‚ä»·å€¼å‡½æ•°å€¼è¶Šå¤§ï¼Œè¯´æ˜æ™ºèƒ½ä½“è¿›å…¥è¿™ä¸ªçŠ¶æ€è¶Šæœ‰åˆ©ã€‚

  â€‹

- **æ¨¡å‹ï¼ˆmodelï¼‰**ã€‚æ¨¡å‹è¡¨ç¤ºæ™ºèƒ½ä½“å¯¹ç¯å¢ƒçš„çŠ¶æ€è¿›è¡Œç†è§£ï¼Œå®ƒå†³å®šäº†ç¯å¢ƒä¸­ä¸–ç•Œçš„è¿è¡Œæ–¹å¼ã€‚Â 

  â€‹



1.3 æ™ºèƒ½ä½“ç±»å‹

* **åŸºäºä»·å€¼çš„æ™ºèƒ½ä½“**ï¼šæ˜¾å¼åœ°å­¦ä¹ ä»·å€¼å‡½æ•°ï¼Œéšå¼åœ°å­¦ä¹ å®ƒçš„ç­–ç•¥ã€‚ç­–ç•¥æ˜¯å…¶ä»å­¦åˆ°çš„ä»·å€¼å‡½æ•°é‡Œé¢æ¨ç®—å‡ºæ¥çš„ï¼ˆQ-learning/Sarsaï¼‰



* **åŸºäºç­–ç•¥çš„æ™ºèƒ½ä½“**ï¼šç›´æ¥å­¦ä¹ ç­–ç•¥ï¼Œæˆ‘ä»¬ç»™å®ƒä¸€ä¸ªçŠ¶æ€ï¼Œå®ƒå°±ä¼šè¾“å‡ºå¯¹åº”åŠ¨ä½œçš„æ¦‚ç‡ã€‚ï¼ˆPolicy Gradient/PGï¼‰

  â€‹

```python
import gymnasium as gym

#åˆ›å»ºä¸€ä¸ªenv å‚æ•°åŒ…æ‹¬åœºæ™¯ç±»å‹ã€æ¸²æŸ“æ¨¡å¼
env = gym.make("LunarLander-v3", render_mode="human")
observation, info = env.reset()

#é¦–å…ˆè®¾ç½®eposode_overä¸ºflaseï¼Œè¿›å…¥è®­ç»ƒå¾ªç¯
episode_over = False
while not episode_over:
    action = env.action_space.sample()  # agent policy that uses the observation and info
    observation, reward, terminated, truncated, info = env.step(action) #æ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œï¼Œæ™ºèƒ½ä½“ä»æ›´æ–°åçš„ç¯å¢ƒæ¥æ”¶åˆ°æ–°çš„è§‚æµ‹ä»¥åŠæ‰§è¡ŒåŠ¨ä½œçš„å¥–åŠ±ï¼›

    episode_over = terminated or truncated

env.close()
```



## Chapter 2Â Markov decision process(MDP)

### 2.1é©¬å°”ç§‘å¤«è¿‡ç¨‹

1ã€éšæœºè¿‡ç¨‹ï¼š

å·²çŸ¥å†å²ä¿¡æ¯ï¼ˆS1, ..., Stï¼‰æ—¶ä¸‹ä¸€ä¸ªæ—¶åˆ»çŠ¶æ€ä¸ºSt+1çš„æ¦‚ç‡è¡¨ç¤ºæˆ$P(S_{t+1}|S_1,\ldots,S_t)$

2ã€é©¬å°”ç§‘å¤«æ€§è´¨ï¼šï¼ˆ**çŠ¶æ€è½¬ç§»**ï¼‰

å½“ä¸”ä»…å½“æŸæ—¶åˆ»çš„çŠ¶æ€åªå–å†³äºä¸Šä¸€æ—¶åˆ»çš„çŠ¶æ€æ—¶$P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\ldots,S_t)$

3ã€é©¬å°”ç§‘å¤«é“¾ï¼š<S, P>

nä¸ªçŠ¶æ€ï¼šS =Â {S1, S2, ... , Sn}

çŠ¶æ€è½¬ç§»çŸ©é˜µPï¼š$\mathcal{P}=\begin{bmatrix}P(s_1|s_1)&\cdots&P(s_n|s_1)\\\vdots&\ddots&\vdots\\P(s_1|s_n)&\cdots&P(s_n|s_n)\end{bmatrix}$Â  ï¼ˆæ¯ä¸€è¡Œå’Œä¸º1ï¼‰



### 2.2é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ï¼ˆMRPï¼‰

1. é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ï¼š

$\langle\mathcal{s},\mathcal{P},r,\gamma\rangle$

* $\mathcal{s}$æ˜¯æœ‰é™çŠ¶æ€çš„é›†åˆã€‚

* $\mathcal{P}$æ˜¯çŠ¶æ€è½¬ç§»çŸ©é˜µã€‚

* $r$æ˜¯å¥–åŠ±å‡½æ•°ï¼Œ**æŸä¸ªçŠ¶æ€$s$çš„å¥–åŠ± $r(s)$æŒ‡è½¬ç§»åˆ°è¯¥çŠ¶æ€æ—¶å¯ä»¥è·å¾—å¥–åŠ±çš„æœŸæœ›**ã€‚

* $\gamma$æ˜¯æŠ˜æ‰£å› å­ (discount factor), $\gamma$çš„å–å€¼èŒƒå›´ä¸º[0,1)ã€‚å¼•å…¥æŠ˜æ‰£å› å­çš„ç†ç”±ä¸ºè¿œæœŸåˆ©ç›Šå…·æœ‰ä¸€å®šä¸ç¡®å®šæ€§ï¼Œæœ‰æ—¶æˆ‘ä»¬æ›´å¸Œæœ›èƒ½å¤Ÿå°½å¿«è·å¾—ä¸€äº›å¥–åŠ±ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹è¿œæœŸåˆ©ç›Šæ‰“ä¸€äº›æŠ˜æ‰£ã€‚æ¥è¿‘ 1 çš„Î³æ›´å…³æ³¨é•¿æœŸçš„ç´¯è®¡å¥–åŠ±ï¼Œæ¥è¿‘ 0 çš„Î³æ›´è€ƒè™‘çŸ­æœŸå¥–åŠ±ã€‚

  â€‹


#### 2. å›æŠ¥ï¼š

åœ¨ä¸€ä¸ªé©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ä¸­ï¼Œä»ç¬¬tæ—¶åˆ»çŠ¶æ€å¼€å§‹ï¼Œç›´åˆ°ç»ˆæ­¢çŠ¶æ€æ—¶ï¼Œæ‰€æœ‰å¥–åŠ±çš„è¡°å‡ä¹‹å’Œç§°ä¸º**å›æŠ¥**Gtï¼ˆReturnï¼‰

$$G_t=r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+\cdots=\sum_{k=0}^\infty\gamma^kr_{t+k}$$

å…¶ä¸­**r_tè¡¨ç¤ºåœ¨tæ—¶åˆ»è·å¾—çš„å¥–åŠ±**

```python 
import numpy as np
np.random.seed(0)
# å®šä¹‰çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µP
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0]  # å®šä¹‰å¥–åŠ±å‡½æ•°
gamma = 0.5  # å®šä¹‰æŠ˜æ‰£å› å­


# ç»™å®šä¸€æ¡åºåˆ—,è®¡ç®—ä»æŸä¸ªç´¢å¼•ï¼ˆèµ·å§‹çŠ¶æ€ï¼‰å¼€å§‹åˆ°åºåˆ—æœ€åï¼ˆç»ˆæ­¢çŠ¶æ€ï¼‰å¾—åˆ°çš„å›æŠ¥
def compute_return(start_index, chain, gamma):
    G = 0
    for i in range(len(chain)-1, -1, len(chain)):
        G = gamma * G + rewards[chain[i] - 1] #ä½¿ç”¨reversedå…ˆç®—chainæœ€åä¸€ä½ï¼Œæ¯ä¸€æ¬¡éƒ½ä¼šå¯¹å…¶è¿›è¡Œä¹˜gamma
    return G


# ä¸€ä¸ªçŠ¶æ€åºåˆ—,s1-s2-s3-s6
chain = [1, 2, 3, 6]
start_index = 0
G = compute_return(start_index, chain, gamma)
print("æ ¹æ®æœ¬åºåˆ—è®¡ç®—å¾—åˆ°å›æŠ¥ä¸ºï¼š%sã€‚" % G)
```



#### 3. ä»·å€¼å‡½æ•°ï¼š

ï¼ˆ1ï¼‰é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªçŠ¶æ€çš„æœŸæœ›å›æŠ¥ï¼ˆå³ä»è¿™ä¸ªçŠ¶æ€å‡ºå‘çš„æœªæ¥ç´¯ç§¯å¥–åŠ±çš„æœŸæœ›ï¼‰è¢«ç§°ä¸ºè¿™ä¸ªçŠ¶æ€çš„**ä»·å€¼**ï¼ˆvalueï¼‰ã€‚

ğŸ™Œæ³¨æ„ä»·å€¼æ˜¯é’ˆå¯¹æŸä¸ªèµ·å§‹çŠ¶æ€tçš„!!!

ï¼ˆ2ï¼‰æ‰€æœ‰çŠ¶æ€çš„ä»·å€¼å°±ç»„æˆäº†**ä»·å€¼å‡½æ•°**ï¼ˆvalue functionï¼‰**V(s)**ï¼Œä»·å€¼å‡½æ•°çš„è¾“å…¥ä¸ºæŸä¸ªçŠ¶æ€ï¼Œè¾“å‡ºä¸ºè¿™ä¸ªçŠ¶æ€çš„ä»·å€¼ã€‚

$$\begin{aligned}V(s)&=\mathbb{E}[G_t|s_t=s]\\&=\mathbb{E}[r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+\ldots|s_t=s]\\&=\mathbb{E}[r_t+\gamma(r_{t+1}+\gamma r_{t+2}+\ldots)|s_t=s]\\&=\mathbb{E}[r_t+\gamma G_{t+1}|s_t=s]\\&=\mathbb{E}[r_t+\gamma V(s_{t+1})|s_t=s]\end{aligned}$$

å¯¹æœ€åä¸€å¼è¿›è¡Œåˆ†è§£ï¼Œå…¶ä¸­ï¼Œå³æ—¶å¥–åŠ±çš„æœŸæœ›ä¸ºå¥–åŠ±å‡½æ•°çš„è¾“å‡ºï¼š$\mathbb{E}[r_t|s_t=s]=R(s)$

å‰©ä½™éƒ¨åˆ†$\mathbb{E}[\gamma V(s_{t+1})|s_{t}=s]$Â æ ¹æ®ä»çŠ¶æ€så‡ºå‘çš„è½¬ç§»æ¦‚ç‡å¯ä»¥å¾—åˆ°ï¼›

å› æ­¤ç­‰å¼ç­‰ä»·ä¸º**è´å°”æ›¼æ–¹ç¨‹**ï¼š

âœ¨$V(s)=R(s)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s)V(s^{\prime})$

ï¼ˆ3ï¼‰çŸ©é˜µå½¢å¼

$$\mathcal{V}=\mathcal{R}+\gamma\mathcal{PV}\\\begin{bmatrix}V(s_1)\\V(s_2)\\\cdots\\V(s_n)\end{bmatrix}=\begin{bmatrix}r(s_1)\\r(s_2)\\\cdots\\r(s_n)\end{bmatrix}+\gamma\begin{bmatrix}P(s_1|s_1)&p(s_2|s_1)&\ldots&P(s_n|s_1)\\P(s_1|s_2)&P(s_2|s_2)&\ldots&P(s_n|s_2)\\\cdots\\P(s_1|s_n)&P(s_2|s_n)&\ldots&P(s_n|s_n)\end{bmatrix}\begin{bmatrix}V(s_1)\\V(s_2)\\\ldots\\V(s_n)\end{bmatrix}$$

è§£æè§£ï¼š$$\mathcal{V}=(I-\gamma\mathcal{P})^{-1}\mathcal{R}$$Â æ—¶é—´å¤æ‚åº¦ä¸ºO(n^3)

```python
def compute(P, rewards, gamma, states_num):
    ''' åˆ©ç”¨è´å°”æ›¼æ–¹ç¨‹çš„çŸ©é˜µå½¢å¼è®¡ç®—è§£æè§£,states_numæ˜¯MRPçš„çŠ¶æ€æ•° '''
    rewards = np.array(rewards).reshape((-1, 1))  #å°†rewardså†™æˆåˆ—å‘é‡å½¢å¼
    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P), rewards)
    return value


V = compute(P, rewards, gamma, 6)
print("MRPä¸­æ¯ä¸ªçŠ¶æ€ä»·å€¼åˆ†åˆ«ä¸º\n", V)
```



#### 4. è¿­ä»£ç®—æ³•ï¼š

ï¼ˆ1ï¼‰è’™ç‰¹å¡æ´›é‡‡æ ·ç®—æ³•ï¼š(ç›¸å½“äºå¯¹å›æŠ¥æ±‚å‡å€¼)

å½“å¾—åˆ°ä¸€ä¸ªé©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹åï¼Œæˆ‘ä»¬å¯ä»¥ä»æŸä¸ªçŠ¶æ€å¼€å§‹ï¼ŒæŠŠå°èˆ¹æ”¾åˆ°çŠ¶æ€è½¬ç§»çŸ©é˜µé‡Œé¢ï¼Œè®©å®ƒâ€œéšæ³¢é€æµâ€ï¼Œè¿™æ ·å°±ä¼šäº§ç”Ÿä¸€ä¸ªè½¨è¿¹ã€‚äº§ç”Ÿä¸€ä¸ªè½¨è¿¹ä¹‹åï¼Œå°±ä¼šå¾—åˆ°ä¸€ä¸ªå¥–åŠ±ï¼Œé‚£ä¹ˆç›´æ¥æŠŠæŠ˜æ‰£çš„å¥–åŠ±å³å›æŠ¥ g ç®—å‡ºæ¥ã€‚ç®—å‡ºæ¥ä¹‹åå°†å®ƒç§¯ç´¯èµ·æ¥ï¼Œå¾—åˆ°å›æŠ¥Gtã€‚ å½“ç§¯ç´¯äº†ä¸€å®šæ•°é‡çš„è½¨è¿¹ä¹‹åï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ Gté™¤ä»¥è½¨è¿¹æ•°é‡ï¼Œå°±ä¼šå¾—åˆ°æŸä¸ªçŠ¶æ€çš„ä»·å€¼

![alt text](image-2.png)



ï¼ˆ2ï¼‰åŠ¨æ€è§„åˆ’ç®—æ³•ï¼š

é€šè¿‡**è‡ªä¸¾ï¼ˆbootstrappingï¼‰**çš„æ–¹æ³•ä¸åœåœ°è¿­ä»£è´å°”æ›¼æ–¹ç¨‹ï¼Œå½“æœ€åæ›´æ–°çš„çŠ¶æ€ä¸æˆ‘ä»¬ä¸Šä¸€ä¸ªçŠ¶æ€çš„åŒºåˆ«å¹¶ä¸å¤§çš„æ—¶å€™ï¼Œæ›´æ–°å°±å¯ä»¥åœæ­¢ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¾“å‡ºæœ€æ–°çš„ Vâ€²(s) ä½œä¸ºå®ƒå½“å‰çš„çŠ¶æ€çš„ä»·å€¼ã€‚

![alt text](image-3.png)



### 2.3 é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰

1. ç»„æˆï¼šMDP = MRP + agent's action

![alt text](image-4.png)

$$\langle\mathcal{s},\mathcal{a},P,r,\gamma\rangle$$

$$\begin{aligned}&\mathcal{s}\text{æ˜¯çŠ¶æ€çš„é›†åˆ;}\\&\mathcal{a}\text{æ˜¯åŠ¨ä½œçš„é›†åˆ;}\\&\gamma\text{æ˜¯æŠ˜æ‰£å› å­;}\\&r(s,a)\text{æ˜¯å¥–åŠ±å‡½æ•°,æ­¤æ—¶å¥–åŠ±å¯ä»¥åŒæ—¶å–å†³äºçŠ¶æ€}s\text{å’ŒåŠ¨ä½œ}a\text{,åœ¨å¥–åŠ±å‡½æ•°åªå–å†³äºçŠ¶æ€}s\text{æ—¶,åˆ™}\\&\text{é€€åŒ–ä¸º}r(s)\mathrm{;}\\&P(s^{\prime}|s,a)\text{æ˜¯çŠ¶æ€è½¬ç§»å‡½æ•°,è¡¨ç¤ºåœ¨çŠ¶æ€}s\text{æ‰§è¡ŒåŠ¨ä½œ}a\text{ä¹‹ååˆ°è¾¾çŠ¶æ€}s^{\prime}\text{çš„æ¦‚ç‡ã€‚}\end{aligned}$$



#### 2. ç­–ç•¥ï¼š

ï¼ˆ1ï¼‰**çŠ¶æ€ã€åŠ¨ä½œæ¦‚ç‡**ï¼š

**ç­–ç•¥**ï¼ˆåœ¨æŸä¸ªçŠ¶æ€å¯èƒ½é‡‡å–æŸä¸ªè¡ŒåŠ¨çš„æ¦‚ç‡ï¼‰ ï¼š$\pi(a\mid s)=p\left(a_t=a\mid s_t=s\right)$

çŠ¶æ€è½¬ç§»ï¼š$p\left(s_{t+1}=s^{\prime}\mid s_t=s,a_t=a\right)$

MDPæ»¡è¶³æ¡ä»¶ï¼š$p\left(s_{t+1}\mid s_t,a_t\right)=p\left(s_{t+1}\mid h_t,a_t\right)$



ï¼ˆ2ï¼‰ç­–ç•¥è½¬åŒ–ï¼š

å¯¹åŠ¨ä½œè¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°MRPçš„**çŠ¶æ€è½¬ç§»æ¦‚ç‡**:

Â $P_\pi\left(s^{\prime}\mid s\right)=\sum_{a\in A}\pi(a\mid s)p\left(s^{\prime}\mid s,a\right)$

å¥–åŠ±å‡½æ•°ï¼š

$r_\pi(s)=\sum_{a\in A}\pi(a\mid s)r(s,a)$



#### 3. ä»·å€¼å‡½æ•°ï¼š

Qå‡½æ•°ï¼ˆåŠ¨ä½œä»·å€¼å‡½æ•°ï¼‰ï¼šåœ¨æŸä¸€ä¸ªçŠ¶æ€é‡‡å–æŸä¸€ä¸ªåŠ¨ä½œï¼Œå®ƒæœ‰å¯èƒ½å¾—åˆ°çš„å›æŠ¥çš„æœŸæœ›ã€‚

$Q_\pi(s,a)=\mathbb{E}_\pi\left[G_t\mid s_t=s,a_t=a\right]$

ä»·å€¼å‡½æ•°ï¼šå¯¹Qå‡½æ•°ä¸­çš„åŠ¨ä½œè¿›è¡ŒåŠ å’Œã€‚

$V_\pi(s)=\mathbb{E}_\pi[G_t\mid s_t=s]=\sum_{a\in A}\pi(a\mid s)Q_\pi(s,aï¼‰$

åŒç†äºMRPçš„ä»·å€¼å‡½æ•°æ¨å¯¼ï¼Œå¯å¾—ï¼š

$\begin{aligned}Q_\pi(s,a)=R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_\pi\left(s^{\prime}\right) \end{aligned}$

å¾—åˆ°è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼š

âœ¨$V_\pi(s)=\sum_{a\in A}\pi(a\mid s)\left(R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_\pi\left(s^{\prime}\right)\right)$



## Chapter 3 Dynamic Programming

### 3.1 æ¦‚å¿µ

1. è¦æ±‚äº‹å…ˆçŸ¥é“ç¯å¢ƒçš„çŠ¶æ€è½¬ç§»å‡½æ•°å’Œå¥–åŠ±å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯éœ€è¦çŸ¥é“æ•´ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹
2. ç±»å‹ï¼š

ç­–ç•¥è¿­ä»£ï¼šç­–ç•¥è¯„ä¼°+ç­–ç•¥æå‡

ä»·å€¼è¿­ä»£



### 3.2 æ‚¬å´–æ¼«æ­¥é—®é¢˜ï¼ˆQè¡¨æ ¼ï¼‰

![img](https://hrl.boyuai.com/static/540.f28e3c6f.png)

å¦‚å›¾æ‰€ç¤ºï¼Œæœ‰ä¸€ä¸ª4X12 çš„æ‚¬å´–ç½‘æ ¼ï¼Œæœ€ä¸‹é¢ä¸€æ’é™¤äº†èµ·ç‚¹å’Œç»ˆç‚¹éƒ½æ˜¯æ‚¬å´–ã€‚å¦‚æœæ™ºèƒ½ä½“é‡‡å–åŠ¨ä½œåè§¦ç¢°åˆ°è¾¹ç•Œå¢™å£åˆ™çŠ¶æ€ä¸å‘ç”Ÿæ”¹å˜ï¼Œå¦åˆ™å°±ä¼šç›¸åº”åˆ°è¾¾ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚ç¯å¢ƒä¸­æœ‰ä¸€æ®µæ‚¬å´–ï¼Œæ™ºèƒ½ä½“æ‰å…¥æ‚¬å´–æˆ–åˆ°è¾¾ç›®æ ‡çŠ¶æ€éƒ½ä¼šç»“æŸåŠ¨ä½œå¹¶å›åˆ°èµ·ç‚¹ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰å…¥æ‚¬å´–æˆ–è€…è¾¾åˆ°ç›®æ ‡çŠ¶æ€æ˜¯ç»ˆæ­¢çŠ¶æ€ã€‚æ™ºèƒ½ä½“æ¯èµ°ä¸€æ­¥çš„å¥–åŠ±æ˜¯ âˆ’1ï¼Œæ‰å…¥æ‚¬å´–çš„å¥–åŠ±æ˜¯ âˆ’100ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt


class CliffWalkingEnv:
    """ æ‚¬å´–æ¼«æ­¥ç¯å¢ƒ"""
    def __init__(self, ncol=12, nrow=4):
        self.ncol = ncol  # å®šä¹‰ç½‘æ ¼ä¸–ç•Œçš„åˆ—
        self.nrow = nrow  # å®šä¹‰ç½‘æ ¼ä¸–ç•Œçš„è¡Œ
        # è½¬ç§»çŸ©é˜µP[state][action] = [(p, next_state, reward, done)]åŒ…å«ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ï¼Œdoneè¡¨ç¤ºåŠ¨ä½œæ˜¯å¦ç»ˆæ­¢ï¼ˆæ˜¯å¦åˆ°è¾¾æ‚¬å´–æˆ–ç»ˆç‚¹ï¼‰
        self.P = self.createP()
    
    def createP(self):
        # åˆå§‹åŒ–
        P = [[[] for _ in range(4)] for _ in range(self.nrow * self.ncol)]
        # 4ç§åŠ¨ä½œ, change[0]:ä¸Š,change[1]:ä¸‹, change[2]:å·¦, change[3]:å³ã€‚åæ ‡ç³»åŸç‚¹(0,0)åœ¨å·¦ä¸Šè§’
        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]
        for i in range(self.nrow):
            for j in range(self.ncol):
                for a in range(4): #æ­¤æ—¶éå†changeé‡Œé¢çš„å››ä¸ªåŠ¨ä½œ
                    # ä½ç½®åœ¨æ‚¬å´–æˆ–ç»ˆç‚¹æ—¶æ— æ³•ç»§ç»­äº¤äº’
                    if i == self.nrow - 1 and j > 0:
                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0, True)]
                        continue
                    
                    # è®¡ç®—ä¸‹ä¸€ä¸ªä½ç½®
                    next_x = min(self.ncol - 1, max(0, j + change[a][0])) #é˜²æ­¢è¶…å‡ºå·¦æˆ–ä¸Šè¾¹ç•Œ
                    next_y = min(self.nrow - 1, max(0, i + change[a][1])) #é˜²æ­¢è¶…å‡ºå³æˆ–ä¸‹è¾¹ç•Œ
                    next_state = next_y * self.ncol + next_x #è½¬åŒ–æˆä¸€ç»´ç´¢å¼•
                    reward = -1
                    done = False
                    
                    # åˆ¤æ–­ä¸‹ä¸€ä¸ªä½ç½®æ˜¯å¦åœ¨æ‚¬å´–æˆ–ç»ˆç‚¹
                    if next_y == self.nrow - 1 and next_x > 0:
                        done = True
                        if next_x != self.ncol - 1:  # æ‚¬å´–
                            reward = -100
                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]
        return P


# åˆ›å»ºç¯å¢ƒå®ä¾‹
env = CliffWalkingEnv(ncol=12, nrow=4)

# åˆå§‹åŒ–Qè¡¨
n_states = env.nrow * env.ncol
n_actions = 4
Q = np.zeros((n_states, n_actions))

# è¶…å‚æ•°è®¾ç½®
alpha = 0.1    # å­¦ä¹ ç‡
gamma = 0.99   # æŠ˜æ‰£å› å­
epsilon = 0.1  # æ¢ç´¢ç‡
num_episodes = 500  # è®­ç»ƒçš„æ€»episodeæ•°

# è®­ç»ƒè¿‡ç¨‹
rewards = []  # è®°å½•æ¯ä¸ªepisodeçš„æ€»å¥–åŠ±

for episode in range(num_episodes):
    state = 3 * env.ncol + 0  # åˆå§‹çŠ¶æ€ï¼šå·¦ä¸‹è§’(3,0)
    done = False
    total_reward = 0
    
    while not done:
        # epsilon-greedyé€‰æ‹©åŠ¨ä½œ
        if np.random.rand() < epsilon:
            action = np.random.randint(n_actions)
        else:
            action = np.argmax(Q[state])
        
        # æ‰§è¡ŒåŠ¨ä½œï¼Œå¾—åˆ°è½¬ç§»ä¿¡æ¯
        p, next_state, reward, done = env.P[state][action][0]
        
        # æ›´æ–°Qè¡¨
        Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])
        
        # æ›´æ–°çŠ¶æ€å’Œæ€»å¥–åŠ±
        state = next_state
        total_reward += reward
        
    rewards.append(total_reward)

# æµ‹è¯•ç­–ç•¥
def test_policy(Q):
    state = 3 * env.ncol + 0  # åˆå§‹çŠ¶æ€
    path = []
    done = False
    while not done:
        action = np.argmax(Q[state])
        path.append((state // env.ncol, state % env.ncol))  # è®°å½•åæ ‡
        p, next_state, reward, done = env.P[state][action][0]
        state = next_state
        if len(path) > 100:  # é˜²æ­¢æ— é™å¾ªç¯
            break
    path.append((state // env.ncol, state % env.ncol))  # æ·»åŠ ç»ˆç‚¹
    return path

# å¯è§†åŒ–ç»“æœ
path = test_policy(Q)
print("æœ€ä¼˜è·¯å¾„åæ ‡åºåˆ—ï¼š")
print(path)

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Q-learning Training Performance')
plt.show()
```



### 3.3 ç­–ç•¥è¿­ä»£ç®—æ³•

1. **ç­–ç•¥è¯„ä¼°**ï¼šè®¡ç®—ä¸€ä¸ªç­–ç•¥Ï€ä¸‹ä»çŠ¶æ€så‡ºå‘å¯ä»¥å¾—åˆ°çš„**çŠ¶æ€ä»·å€¼å‡½æ•°**

è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼š$V^\pi(s)=\sum_{a\in A}\pi(a\mid s)\left(R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V^\pi\left(s^{\prime}\right)\right)$

ä½¿ç”¨ä¸Šä¸€è½®çš„çŠ¶æ€ä»·å€¼å‡½æ•°æ¥è®¡ç®—å½“å‰ä¸€è½®çš„çŠ¶æ€ä»·å€¼å‡½æ•°ï¼š$V^{k+1}(s)=\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)V^k(s^{\prime})\right)$

ä¸åŠ¨ç‚¹ï¼šV^k = V^piï¼Œå› æ­¤å¦‚æœæŸä¸€è½® $\max_{s\in\mathcal{S}}|V^{k+1}(s)-V^{k}(s)|$çš„å€¼éå¸¸å°ï¼Œå¯ä»¥æå‰ç»“æŸç­–ç•¥è¯„ä¼°ã€‚



2. **ç­–ç•¥æå‡**ï¼šæ ¹æ®çŠ¶æ€ä»·å€¼å‡½æ•°æ”¹è¿›å½“å‰ç­–ç•¥Ï€ï¼Œä»è€Œæé«˜æœ€ç»ˆçš„æœŸæœ›å›æŠ¥$V^\pi(s)$ï¼Œè¿›è€Œ**å¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„ç­–ç•¥Ï€â€™**

å‡è®¾æ™ºèƒ½ä½“åœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaï¼Œä¹‹åçš„åŠ¨ä½œä¾æ—§éµå¾ªç­–ç•¥Ï€ï¼Œæ­¤æ—¶çš„æœŸæœ›å›æŠ¥ä¸º$Q_{\pi}(s,a)$ï¼Œå¦‚æœ$Q^{\pi}(s,a)>V^{\pi}(s)$ï¼Œåˆ™è¯´æ˜åœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaä¼šæ¯”åŸæ¥çš„ç­–ç•¥Ï€å¾—åˆ°æ›´é«˜çš„æœŸæœ›å›æŠ¥ã€‚

ğŸ™Œæœ€åä¸€å¥è¯æ„æ€ï¼šé€šè¿‡åœ¨çŠ¶æ€sä¸‹åªé€‰æ‹©æŸä¸€ä¸ªåŠ¨ä½œï¼Œå»é™¤åŠ æƒå¹³å‡ï¼Œä»è€Œæé«˜æœŸæœ›å›æŠ¥ã€‚

![1743238395174](Reinforcement Learning.assets/1743238395174.png)



âœ¨ç­–ç•¥æå‡å®šç†ï¼šç°å‡è®¾ä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥Ï€â€˜ï¼Œåœ¨ä»»æ„ä¸€ä¸ªçŠ¶æ€ä¸‹ï¼Œéƒ½æ»¡è¶³ï¼š

$Q^\pi(s,\pi^{\prime}(s))\geq V^\pi(s)$

äºæ˜¯åœ¨ä»»æ„çŠ¶æ€ä¸‹ï¼š

$$V^{\pi^{\prime}}(s)\geq V^\pi(s)$$

å› æ­¤å¯ä»¥é€šè¿‡åœ¨æ¯ä¸€ä¸ªçŠ¶æ€ä¸‹é€‰æ‹©åŠ¨ä½œä»·å€¼æœ€å¤§çš„åŠ¨ä½œï¼Œä»è€Œæå‡æœ€ç»ˆçš„ä»·å€¼å‡½æ•°ï¼š

$\pi^{\prime}(s)=\arg\max_aQ^\pi(s,a)=\arg\max_a\{r(s,a)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V^\pi(s^{\prime})\}$

ğŸŒŸè¯æ˜ï¼šå…³é”®æ˜¯ç”¨åˆ°$V^{\pi}(S_{t+1})\leq Q^{\pi}(S_{t+1},\pi^{\prime}(S_{t+1}))$

$\begin{aligned}V^{\pi}(s)&\leq Q^{\pi}(s,\pi^{\prime}(s))\\&=\mathbb{E}_{\pi^{\prime}}[R_t+\gamma V^\pi(S_{t+1})|S_t=s]\\&\leq\mathbb{E}_{\pi^{\prime}}[R_t+\gamma Q^\pi(S_{t+1},\pi^{\prime}(S_{t+1}))|S_t=s]\\&=\mathbb{E}_{\pi^{\prime}}[R_t+\gamma R_{t+1}+\gamma^2V^\pi(S_{t+2})|S_t=s]\\&\leq\mathbb{E}_{\pi^{\prime}}[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\gamma^3V^\pi(S_{t+3})|S_t=s]\\&\leq\mathbb{E}_{\pi^{\prime}}[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\gamma^3R_{t+3}+\cdots|S_t=s]\\&=V^{\pi^{\prime}}(s)\end{aligned}$



3. **ç­–ç•¥è¿­ä»£ç®—æ³•ï¼š**

![1743249554905](Reinforcement Learning.assets/1743249554905.png)



```python
class PolicyIteration:
    """ ç­–ç•¥è¿­ä»£ç®—æ³• """
    def __init__(self, env, theta, gamma):
        self.env = env
        self.v = [0] * self.env.ncol * self.env.nrow  # åˆå§‹åŒ–ä»·å€¼ä¸º0
        self.pi = [[0.25, 0.25, 0.25, 0.25]
                   for i in range(self.env.ncol * self.env.nrow)]  # åˆå§‹åŒ–ä¸ºå‡åŒ€éšæœºç­–ç•¥
        self.theta = theta  # ç­–ç•¥è¯„ä¼°æ”¶æ•›é˜ˆå€¼
        self.gamma = gamma  # æŠ˜æ‰£å› å­

    def policy_evaluation(self):  # ç­–ç•¥è¯„ä¼°
        cnt = 1  # è®¡æ•°å™¨
        while 1:
            max_diff = 0
            new_v = [0] * self.env.ncol * self.env.nrow
            for s in range(self.env.ncol * self.env.nrow):
                qsa_list = []  # å¼€å§‹è®¡ç®—çŠ¶æ€sä¸‹çš„æ‰€æœ‰Q(s,a)ä»·å€¼
                for a in range(4):
                    qsa = 0
                    for res in self.env.P[s][a]:
                        p, next_state, r, done = res
                        qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))
                        # æœ¬ç« ç¯å¢ƒæ¯”è¾ƒç‰¹æ®Š,å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€æœ‰å…³,æ‰€ä»¥éœ€è¦å’ŒçŠ¶æ€è½¬ç§»æ¦‚ç‡ç›¸ä¹˜
                    qsa_list.append(self.pi[s][a] * qsa)
                new_v[s] = sum(qsa_list)  # çŠ¶æ€ä»·å€¼å‡½æ•°å’ŒåŠ¨ä½œä»·å€¼å‡½æ•°ä¹‹é—´çš„å…³ç³»
                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))
            self.v = new_v
            if max_diff < self.theta: break  # æ»¡è¶³æ”¶æ•›æ¡ä»¶,é€€å‡ºè¯„ä¼°è¿­ä»£
            cnt += 1
        print("ç­–ç•¥è¯„ä¼°è¿›è¡Œ%dè½®åå®Œæˆ" % cnt)

    def policy_improvement(self):  # ç­–ç•¥æå‡
        for s in range(self.env.nrow * self.env.ncol):
            qsa_list = []
            for a in range(4):
                qsa = 0
                for res in self.env.P[s][a]:
                    p, next_state, r, done = res
                    qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))
                qsa_list.append(qsa)
            maxq = max(qsa_list)
            cntq = qsa_list.count(maxq)  # è®¡ç®—æœ‰å‡ ä¸ªåŠ¨ä½œå¾—åˆ°äº†æœ€å¤§çš„Qå€¼
            # è®©è¿™äº›åŠ¨ä½œå‡åˆ†æ¦‚ç‡
            self.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]
        print("ç­–ç•¥æå‡å®Œæˆ")
        return self.pi

    def policy_iteration(self):  # ç­–ç•¥è¿­ä»£
        while 1:
            self.policy_evaluation()
            old_pi = copy.deepcopy(self.pi)  # å°†åˆ—è¡¨è¿›è¡Œæ·±æ‹·è´,æ–¹ä¾¿æ¥ä¸‹æ¥è¿›è¡Œæ¯”è¾ƒ
            new_pi = self.policy_improvement()
            if old_pi == new_pi: break
                
def print_agent(agent, action_meaning, disaster=[], end=[]):
    print("çŠ¶æ€ä»·å€¼ï¼š")
    for i in range(agent.env.nrow):
        for j in range(agent.env.ncol):
            # ä¸ºäº†è¾“å‡ºç¾è§‚,ä¿æŒè¾“å‡º6ä¸ªå­—ç¬¦
            print('%6.6s' % ('%.3f' % agent.v[i * agent.env.ncol + j]), end=' ')
        print()

    print("ç­–ç•¥ï¼š")
    for i in range(agent.env.nrow):
        for j in range(agent.env.ncol):
            # ä¸€äº›ç‰¹æ®Šçš„çŠ¶æ€,ä¾‹å¦‚æ‚¬å´–æ¼«æ­¥ä¸­çš„æ‚¬å´–
            if (i * agent.env.ncol + j) in disaster:
                print('****', end=' ')
            elif (i * agent.env.ncol + j) in end:  # ç›®æ ‡çŠ¶æ€
                print('EEEE', end=' ')
            else:
                a = agent.pi[i * agent.env.ncol + j]
                pi_str = ''
                for k in range(len(action_meaning)):
                    pi_str += action_meaning[k] if a[k] > 0 else 'o'
                print(pi_str, end=' ')
        print()


env = CliffWalkingEnv()
action_meaning = ['^', 'v', '<', '>']
theta = 0.001
gamma = 0.9
agent = PolicyIteration(env, theta, gamma)
agent.policy_iteration()
print_agent(agent, action_meaning, list(range(37, 47)), [47])

```



