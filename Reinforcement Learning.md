# Reinforcement Learning

[Python3 é¢å‘å¯¹è±¡ | èœé¸Ÿæ•™ç¨‹](https://www.runoob.com/python3/python3-class.html)

[è˜‘è‡ä¹¦EasyRL](https://datawhalechina.github.io/easy-rl/#/)

[åŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ ](https://hrl.boyuai.com/)

## Chapter 1 Intro

![alt text](./Reinforcement Learning.assets/image-1-1745210399731-2.png)



1.1å¥–åŠ±

ç”±ç¯å¢ƒç»™çš„ä¸€ç§æ ‡é‡çš„åé¦ˆä¿¡å·ï¼ˆscalar feedback signalï¼‰ï¼Œè¿™ç§ä¿¡å·å¯æ˜¾ç¤ºæ™ºèƒ½ä½“åœ¨æŸä¸€æ­¥é‡‡å–æŸä¸ªç­–ç•¥çš„è¡¨ç°å¦‚ä½•ã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®çš„å°±æ˜¯æœ€å¤§åŒ–æ™ºèƒ½ä½“å¯ä»¥è·å¾—çš„å¥–åŠ±ï¼Œæ™ºèƒ½ä½“åœ¨ç¯å¢ƒé‡Œé¢å­˜åœ¨çš„ç›® çš„å°±æ˜¯æœ€å¤§åŒ–å®ƒçš„æœŸæœ›çš„ç´¯ç§¯å¥–åŠ±ï¼ˆexpected cumulative rewardï¼‰ã€‚



1.2 æ™ºèƒ½ä½“ç»„æˆéƒ¨åˆ†

- **ç­–ç•¥ï¼ˆpolicyï¼‰**ï¼šå°†è¾“å…¥çš„çŠ¶æ€å˜æˆå¯èƒ½é‡‡å–çš„åŠ¨ä½œçš„æ¦‚ç‡ï¼Œé€šå¸¸é‡‡ç”¨éšæœºç­–ç•¥ï¼Œåœ¨å­¦ä¹ æ—¶å¯ä»¥é€šè¿‡å¼•å…¥ä¸€å®šçš„éšæœºæ€§æ¥æ›´å¥½åœ°æ¢ç´¢ç¯å¢ƒã€‚

**éšæœºæ€§ç­–ç•¥ï¼ˆÏ€å‡½æ•°ï¼‰**ï¼š$\pi(a|s)=p\left(a_{t}=a|s_{t}=s\right)$è¾“å…¥çŠ¶æ€sï¼Œè¾“å‡ºæ™ºèƒ½ä½“æ‰€æœ‰åŠ¨ä½œå¾—æ¦‚ç‡ã€‚

**ç¡®å®šæ€§ç­–ç•¥**ï¼š æ™ºèƒ½ä½“ç›´æ¥é‡‡å–æœ€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œ$a^*=\arg\max_a\pi(a\mid s)$



- **ä»·å€¼å‡½æ•°ï¼ˆvalue functionï¼‰**ã€‚æˆ‘ä»¬ç”¨ä»·å€¼å‡½æ•°æ¥å¯¹å½“å‰çŠ¶æ€è¿›è¡Œè¯„ä¼°ã€‚ä»·å€¼å‡½æ•°ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“è¿› å…¥æŸä¸ªçŠ¶æ€åï¼Œå¯ä»¥å¯¹åé¢çš„å¥–åŠ±å¸¦æ¥å¤šå¤§çš„å½±å“ã€‚ä»·å€¼å‡½æ•°å€¼è¶Šå¤§ï¼Œè¯´æ˜æ™ºèƒ½ä½“è¿›å…¥è¿™ä¸ªçŠ¶æ€è¶Šæœ‰åˆ©ã€‚

  

- **æ¨¡å‹ï¼ˆmodelï¼‰**ã€‚æ¨¡å‹è¡¨ç¤ºæ™ºèƒ½ä½“å¯¹ç¯å¢ƒçš„çŠ¶æ€è¿›è¡Œç†è§£ï¼Œå®ƒå†³å®šäº†ç¯å¢ƒä¸­ä¸–ç•Œçš„è¿è¡Œæ–¹å¼ã€‚Â 

  



1.3 æ™ºèƒ½ä½“ç±»å‹

* **åŸºäºä»·å€¼çš„æ™ºèƒ½ä½“**ï¼šæ˜¾å¼åœ°å­¦ä¹ ä»·å€¼å‡½æ•°ï¼Œéšå¼åœ°å­¦ä¹ å®ƒçš„ç­–ç•¥ã€‚ç­–ç•¥æ˜¯å…¶ä»å­¦åˆ°çš„ä»·å€¼å‡½æ•°é‡Œé¢æ¨ç®—å‡ºæ¥çš„ï¼ˆQ-learning/Sarsaï¼‰



* **åŸºäºç­–ç•¥çš„æ™ºèƒ½ä½“**ï¼šç›´æ¥å­¦ä¹ ç­–ç•¥ï¼Œæˆ‘ä»¬ç»™å®ƒä¸€ä¸ªçŠ¶æ€ï¼Œå®ƒå°±ä¼šè¾“å‡ºå¯¹åº”åŠ¨ä½œçš„æ¦‚ç‡ã€‚ï¼ˆPolicy Gradient/PGï¼‰

  

```python
import gymnasium as gym

#åˆ›å»ºä¸€ä¸ªenv å‚æ•°åŒ…æ‹¬åœºæ™¯ç±»å‹ã€æ¸²æŸ“æ¨¡å¼
env = gym.make("LunarLander-v3", render_mode="human")
observation, info = env.reset()

#é¦–å…ˆè®¾ç½®eposode_overä¸ºflaseï¼Œè¿›å…¥è®­ç»ƒå¾ªç¯
episode_over = False
while not episode_over:
    action = env.action_space.sample()  # agent policy that uses the observation and info
    observation, reward, terminated, truncated, info = env.step(action) #æ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œï¼Œæ™ºèƒ½ä½“ä»æ›´æ–°åçš„ç¯å¢ƒæ¥æ”¶åˆ°æ–°çš„è§‚æµ‹ä»¥åŠæ‰§è¡ŒåŠ¨ä½œçš„å¥–åŠ±ï¼›

    episode_over = terminated or truncated

env.close()
```



## Chapter 2Â Markov decision process(MDP)

### 2.1é©¬å°”ç§‘å¤«è¿‡ç¨‹

1ã€éšæœºè¿‡ç¨‹ï¼š

å·²çŸ¥å†å²ä¿¡æ¯ï¼ˆS1, ..., Stï¼‰æ—¶ä¸‹ä¸€ä¸ªæ—¶åˆ»çŠ¶æ€ä¸ºSt+1çš„æ¦‚ç‡è¡¨ç¤ºæˆ$P(S_{t+1}|S_1,\ldots,S_t)$

2ã€é©¬å°”ç§‘å¤«æ€§è´¨ï¼šï¼ˆ**çŠ¶æ€è½¬ç§»**ï¼‰

å½“ä¸”ä»…å½“æŸæ—¶åˆ»çš„çŠ¶æ€åªå–å†³äºä¸Šä¸€æ—¶åˆ»çš„çŠ¶æ€æ—¶$P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\ldots,S_t)$

3ã€é©¬å°”ç§‘å¤«é“¾ï¼š<S, P>

nä¸ªçŠ¶æ€ï¼šS =Â {S1, S2, ... , Sn}

çŠ¶æ€è½¬ç§»çŸ©é˜µPï¼š$\mathcal{P}=\begin{bmatrix}P(s_1|s_1)&\cdots&P(s_n|s_1)\\\vdots&\ddots&\vdots\\P(s_1|s_n)&\cdots&P(s_n|s_n)\end{bmatrix}$Â  ï¼ˆæ¯ä¸€è¡Œå’Œä¸º1ï¼‰



### 2.2é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ï¼ˆMRPï¼‰

1. é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ï¼š

$\langle\mathcal{s},\mathcal{P},r,\gamma\rangle$

* $\mathcal{s}$æ˜¯æœ‰é™çŠ¶æ€çš„é›†åˆã€‚

* $\mathcal{P}$æ˜¯çŠ¶æ€è½¬ç§»çŸ©é˜µã€‚

* $r$æ˜¯å¥–åŠ±å‡½æ•°ï¼Œ**æŸä¸ªçŠ¶æ€$s$çš„å¥–åŠ± $r(s)$æŒ‡è½¬ç§»åˆ°è¯¥çŠ¶æ€æ—¶å¯ä»¥è·å¾—å¥–åŠ±çš„æœŸæœ›**ã€‚

* $\gamma$æ˜¯æŠ˜æ‰£å› å­ (discount factor), $\gamma$çš„å–å€¼èŒƒå›´ä¸º[0,1)ã€‚å¼•å…¥æŠ˜æ‰£å› å­çš„ç†ç”±ä¸ºè¿œæœŸåˆ©ç›Šå…·æœ‰ä¸€å®šä¸ç¡®å®šæ€§ï¼Œæœ‰æ—¶æˆ‘ä»¬æ›´å¸Œæœ›èƒ½å¤Ÿå°½å¿«è·å¾—ä¸€äº›å¥–åŠ±ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹è¿œæœŸåˆ©ç›Šæ‰“ä¸€äº›æŠ˜æ‰£ã€‚æ¥è¿‘ 1 çš„Î³æ›´å…³æ³¨é•¿æœŸçš„ç´¯è®¡å¥–åŠ±ï¼Œæ¥è¿‘ 0 çš„Î³æ›´è€ƒè™‘çŸ­æœŸå¥–åŠ±ã€‚

  


#### 2. å›æŠ¥ï¼š

åœ¨ä¸€ä¸ªé©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ä¸­ï¼Œä»ç¬¬tæ—¶åˆ»çŠ¶æ€å¼€å§‹ï¼Œç›´åˆ°ç»ˆæ­¢çŠ¶æ€æ—¶ï¼Œæ‰€æœ‰å¥–åŠ±çš„è¡°å‡ä¹‹å’Œç§°ä¸º**å›æŠ¥**Gtï¼ˆReturnï¼‰

$$G_t=r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+\cdots=\sum_{k=0}^\infty\gamma^kr_{t+k}$$

å…¶ä¸­**r_tè¡¨ç¤ºåœ¨tæ—¶åˆ»è·å¾—çš„å¥–åŠ±**

```python 
import numpy as np
np.random.seed(0)
# å®šä¹‰çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µP
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0]  # å®šä¹‰å¥–åŠ±å‡½æ•°
gamma = 0.5  # å®šä¹‰æŠ˜æ‰£å› å­


# ç»™å®šä¸€æ¡åºåˆ—,è®¡ç®—ä»æŸä¸ªç´¢å¼•ï¼ˆèµ·å§‹çŠ¶æ€ï¼‰å¼€å§‹åˆ°åºåˆ—æœ€åï¼ˆç»ˆæ­¢çŠ¶æ€ï¼‰å¾—åˆ°çš„å›æŠ¥
def compute_return(start_index, chain, gamma):
    G = 0
    for i in range(len(chain)-1, -1, len(chain)):
        G = gamma * G + rewards[chain[i] - 1] #ä½¿ç”¨reversedå…ˆç®—chainæœ€åä¸€ä½ï¼Œæ¯ä¸€æ¬¡éƒ½ä¼šå¯¹å…¶è¿›è¡Œä¹˜gamma
    return G


# ä¸€ä¸ªçŠ¶æ€åºåˆ—,s1-s2-s3-s6
chain = [1, 2, 3, 6]
start_index = 0
G = compute_return(start_index, chain, gamma)
print("æ ¹æ®æœ¬åºåˆ—è®¡ç®—å¾—åˆ°å›æŠ¥ä¸ºï¼š%sã€‚" % G)
```



#### 3. ä»·å€¼å‡½æ•°ï¼š

ï¼ˆ1ï¼‰é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªçŠ¶æ€çš„æœŸæœ›å›æŠ¥ï¼ˆå³ä»è¿™ä¸ªçŠ¶æ€å‡ºå‘çš„æœªæ¥ç´¯ç§¯å¥–åŠ±çš„æœŸæœ›ï¼‰è¢«ç§°ä¸ºè¿™ä¸ªçŠ¶æ€çš„**ä»·å€¼**ï¼ˆvalueï¼‰ã€‚

ğŸ™Œæ³¨æ„ä»·å€¼æ˜¯é’ˆå¯¹æŸä¸ªèµ·å§‹çŠ¶æ€tçš„!!!

ï¼ˆ2ï¼‰æ‰€æœ‰çŠ¶æ€çš„ä»·å€¼å°±ç»„æˆäº†**ä»·å€¼å‡½æ•°**ï¼ˆvalue functionï¼‰**V(s)**ï¼Œä»·å€¼å‡½æ•°çš„è¾“å…¥ä¸ºæŸä¸ªçŠ¶æ€ï¼Œè¾“å‡ºä¸ºè¿™ä¸ªçŠ¶æ€çš„ä»·å€¼ã€‚

$$\begin{aligned}V(s)&=\mathbb{E}[G_t|s_t=s]\\&=\mathbb{E}[r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+\ldots|s_t=s]\\&=\mathbb{E}[r_t+\gamma(r_{t+1}+\gamma r_{t+2}+\ldots)|s_t=s]\\&=\mathbb{E}[r_t+\gamma G_{t+1}|s_t=s]\\&=\mathbb{E}[r_t+\gamma V(s_{t+1})|s_t=s]\end{aligned}$$

å¯¹æœ€åä¸€å¼è¿›è¡Œåˆ†è§£ï¼Œå…¶ä¸­ï¼Œå³æ—¶å¥–åŠ±çš„æœŸæœ›ä¸ºå¥–åŠ±å‡½æ•°çš„è¾“å‡ºï¼š$\mathbb{E}[r_t|s_t=s]=R(s)$

å‰©ä½™éƒ¨åˆ†$\mathbb{E}[\gamma V(s_{t+1})|s_{t}=s]$Â æ ¹æ®ä»çŠ¶æ€så‡ºå‘çš„è½¬ç§»æ¦‚ç‡å¯ä»¥å¾—åˆ°ï¼›

å› æ­¤ç­‰å¼ç­‰ä»·ä¸º**è´å°”æ›¼æ–¹ç¨‹**ï¼š

âœ¨$V(s)=R(s)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s)V(s^{\prime})$

ï¼ˆ3ï¼‰çŸ©é˜µå½¢å¼

$$\mathcal{V}=\mathcal{R}+\gamma\mathcal{PV}\\\begin{bmatrix}V(s_1)\\V(s_2)\\\cdots\\V(s_n)\end{bmatrix}=\begin{bmatrix}r(s_1)\\r(s_2)\\\cdots\\r(s_n)\end{bmatrix}+\gamma\begin{bmatrix}P(s_1|s_1)&p(s_2|s_1)&\ldots&P(s_n|s_1)\\P(s_1|s_2)&P(s_2|s_2)&\ldots&P(s_n|s_2)\\\cdots\\P(s_1|s_n)&P(s_2|s_n)&\ldots&P(s_n|s_n)\end{bmatrix}\begin{bmatrix}V(s_1)\\V(s_2)\\\ldots\\V(s_n)\end{bmatrix}$$

è§£æè§£ï¼š$$\mathcal{V}=(I-\gamma\mathcal{P})^{-1}\mathcal{R}$$Â æ—¶é—´å¤æ‚åº¦ä¸ºO(n^3)

```python
def compute(P, rewards, gamma, states_num):
    ''' åˆ©ç”¨è´å°”æ›¼æ–¹ç¨‹çš„çŸ©é˜µå½¢å¼è®¡ç®—è§£æè§£,states_numæ˜¯MRPçš„çŠ¶æ€æ•° '''
    rewards = np.array(rewards).reshape((-1, 1))  #å°†rewardså†™æˆåˆ—å‘é‡å½¢å¼
    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P), rewards)
    return value


V = compute(P, rewards, gamma, 6)
print("MRPä¸­æ¯ä¸ªçŠ¶æ€ä»·å€¼åˆ†åˆ«ä¸º\n", V)
```



#### 4. è¿­ä»£ç®—æ³•ï¼š

ï¼ˆ1ï¼‰è’™ç‰¹å¡æ´›é‡‡æ ·ç®—æ³•ï¼š(ç›¸å½“äºå¯¹å›æŠ¥æ±‚å‡å€¼)

å½“å¾—åˆ°ä¸€ä¸ªé©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹åï¼Œæˆ‘ä»¬å¯ä»¥ä»æŸä¸ªçŠ¶æ€å¼€å§‹ï¼ŒæŠŠå°èˆ¹æ”¾åˆ°çŠ¶æ€è½¬ç§»çŸ©é˜µé‡Œé¢ï¼Œè®©å®ƒâ€œéšæ³¢é€æµâ€ï¼Œè¿™æ ·å°±ä¼šäº§ç”Ÿä¸€ä¸ªè½¨è¿¹ã€‚äº§ç”Ÿä¸€ä¸ªè½¨è¿¹ä¹‹åï¼Œå°±ä¼šå¾—åˆ°ä¸€ä¸ªå¥–åŠ±ï¼Œé‚£ä¹ˆç›´æ¥æŠŠæŠ˜æ‰£çš„å¥–åŠ±å³å›æŠ¥ g ç®—å‡ºæ¥ã€‚ç®—å‡ºæ¥ä¹‹åå°†å®ƒç§¯ç´¯èµ·æ¥ï¼Œå¾—åˆ°å›æŠ¥Gtã€‚ å½“ç§¯ç´¯äº†ä¸€å®šæ•°é‡çš„è½¨è¿¹ä¹‹åï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ Gté™¤ä»¥è½¨è¿¹æ•°é‡ï¼Œå°±ä¼šå¾—åˆ°æŸä¸ªçŠ¶æ€çš„ä»·å€¼

![alt text](image-2.png)



ï¼ˆ2ï¼‰åŠ¨æ€è§„åˆ’ç®—æ³•ï¼š

é€šè¿‡**è‡ªä¸¾ï¼ˆbootstrappingï¼‰**çš„æ–¹æ³•ä¸åœåœ°è¿­ä»£è´å°”æ›¼æ–¹ç¨‹ï¼Œå½“æœ€åæ›´æ–°çš„çŠ¶æ€ä¸æˆ‘ä»¬ä¸Šä¸€ä¸ªçŠ¶æ€çš„åŒºåˆ«å¹¶ä¸å¤§çš„æ—¶å€™ï¼Œæ›´æ–°å°±å¯ä»¥åœæ­¢ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¾“å‡ºæœ€æ–°çš„ Vâ€²(s) ä½œä¸ºå®ƒå½“å‰çš„çŠ¶æ€çš„ä»·å€¼ã€‚

![alt text](image-3.png)



### 2.3 é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰

1. ç»„æˆï¼šMDP = MRP + agent's action

![alt text](./Reinforcement Learning.assets/image-4-1745210411421-5.png)

$$\langle\mathcal{s},\mathcal{a},P,r,\gamma\rangle$$

$$\begin{aligned}&\mathcal{s}\text{æ˜¯çŠ¶æ€çš„é›†åˆ;}\\&\mathcal{a}\text{æ˜¯åŠ¨ä½œçš„é›†åˆ;}\\&\gamma\text{æ˜¯æŠ˜æ‰£å› å­;}\\&r(s,a)\text{æ˜¯å¥–åŠ±å‡½æ•°,æ­¤æ—¶å¥–åŠ±å¯ä»¥åŒæ—¶å–å†³äºçŠ¶æ€}s\text{å’ŒåŠ¨ä½œ}a\text{,åœ¨å¥–åŠ±å‡½æ•°åªå–å†³äºçŠ¶æ€}s\text{æ—¶,åˆ™}\\&\text{é€€åŒ–ä¸º}r(s)\mathrm{;}\\&P(s^{\prime}|s,a)\text{æ˜¯çŠ¶æ€è½¬ç§»å‡½æ•°,è¡¨ç¤ºåœ¨çŠ¶æ€}s\text{æ‰§è¡ŒåŠ¨ä½œ}a\text{ä¹‹ååˆ°è¾¾çŠ¶æ€}s^{\prime}\text{çš„æ¦‚ç‡ã€‚}\end{aligned}$$



#### 2. ç­–ç•¥ï¼š

ï¼ˆ1ï¼‰**çŠ¶æ€ã€åŠ¨ä½œæ¦‚ç‡**ï¼š

**ç­–ç•¥**ï¼ˆåœ¨æŸä¸ªçŠ¶æ€å¯èƒ½é‡‡å–æŸä¸ªè¡ŒåŠ¨çš„æ¦‚ç‡ï¼‰ ï¼š$\pi(a\mid s)=p\left(a_t=a\mid s_t=s\right)$

çŠ¶æ€è½¬ç§»ï¼š$p\left(s_{t+1}=s^{\prime}\mid s_t=s,a_t=a\right)$

MDPæ»¡è¶³æ¡ä»¶ï¼š$p\left(s_{t+1}\mid s_t,a_t\right)=p\left(s_{t+1}\mid h_t,a_t\right)$



ï¼ˆ2ï¼‰ç­–ç•¥è½¬åŒ–ï¼š

å¯¹åŠ¨ä½œè¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°MRPçš„**çŠ¶æ€è½¬ç§»æ¦‚ç‡**:

Â $P_\pi\left(s^{\prime}\mid s\right)=\sum_{a\in A}\pi(a\mid s)p\left(s^{\prime}\mid s,a\right)$

å¥–åŠ±å‡½æ•°ï¼š

$r_\pi(s)=\sum_{a\in A}\pi(a\mid s)r(s,a)$



#### 3. ä»·å€¼å‡½æ•°ï¼š

Qå‡½æ•°ï¼ˆåŠ¨ä½œä»·å€¼å‡½æ•°ï¼‰ï¼šåœ¨æŸä¸€ä¸ªçŠ¶æ€é‡‡å–æŸä¸€ä¸ªåŠ¨ä½œï¼Œå®ƒæœ‰å¯èƒ½å¾—åˆ°çš„å›æŠ¥çš„æœŸæœ›ã€‚

$$
Q_\pi(s,a)=\mathbb{E}_\pi\left[G_t\mid s_t=s,a_t=a\right]
$$
ä»·å€¼å‡½æ•°ï¼šå¯¹Qå‡½æ•°ä¸­çš„åŠ¨ä½œè¿›è¡ŒåŠ å’Œã€‚

$$
V_\pi(s)=\mathbb{E}_\pi[G_t\mid s_t=s]=\sum_{a\in A}\pi(a\mid s)Q_\pi(s,aï¼‰
$$
åŒç†äºMRPçš„ä»·å€¼å‡½æ•°æ¨å¯¼ï¼Œå¯å¾—ï¼š

$$
\begin{aligned}Q_\pi(s,a)=R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_\pi\left(s^{\prime}\right) \end{aligned}
$$
å¾—åˆ°è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼š

$$
âœ¨V_\pi(s)=\sum_{a\in A}\pi(a\mid s)\left(R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_\pi\left(s^{\prime}\right)\right)
$$


4. çŠ¶æ€è®¿é—®åˆ†å¸ƒ

å‡è®¾MDPçš„åˆå§‹çŠ¶æ€åˆ†å¸ƒä¸º$v_{0}(s)$ï¼ˆè¯¥çŠ¶æ€åˆ†å¸ƒä¸ç­–ç•¥æ— å…³ï¼‰ï¼Œ$P_t^{\pi}$è¡¨ç¤º**é‡‡å–ç­–ç•¥$\pi$ä½¿å¾—æ™ºèƒ½ä½“åœ¨tæ—¶åˆ»çŠ¶æ€ä¸ºsçš„æ¦‚ç‡**:è€ƒè™‘Markovæ€§$P_t^\pi=\sum_{s_0\subset v_0(s)}pr[s_0\to s|t,\pi]$ï¼Œå…¶ä¸­$pr[s_0\to s|t,\pi]$è¡¨ç¤º**åœ¨ç­–ç•¥$\pi$ä¸‹ç»è¿‡tæ­¥è½¬ç§»åˆ°ç›¸åº”çŠ¶æ€çš„æ¦‚ç‡**ï¼Œå®šä¹‰**ä¸€ä¸ªç­–ç•¥çš„çŠ¶æ€è®¿é—®åˆ†å¸ƒ(è¡¨ç¤ºåœ¨ç­–ç•¥$\pi$ä¸‹ï¼ŒçŠ¶æ€sçš„é•¿æœŸè®¿é—®æ¦‚ç‡)**ï¼š
$$
âœ¨v^\pi(s)=(1-\gamma)\sum_{t=0}^\infty\gamma^tP_t^\pi
$$
(å…¶ä¸­$1-\gamma$ä¸ºå½’ä¸€åŒ–å› å­ï¼Œä½¿å¾—**æ‰€æœ‰çŠ¶æ€çš„çŠ¶æ€è®¿é—®åˆ†å¸ƒçš„å’Œä¸º1)

ğŸ™Œæ¨å¯¼ï¼š

å¯¹æ‰€æœ‰çŠ¶æ€sæ±‚å’Œï¼š
$$
\sum_{s\in S}v^{\pi}(s)=\sum_{s\in S}(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}P_{t}^{\pi}(s)
$$
æ ¹æ®æ¦‚ç‡çš„åŸºæœ¬æ€§è´¨ï¼Œå¯¹äºä»»æ„æ—¶åˆ»tï¼Œæ‰€æœ‰çŠ¶æ€çš„æ¦‚ç‡ä¹‹å’Œä¸º1ï¼Œå³$\sum_{s\in s}P_{t}^{n}(s)=1$
$$
\begin{aligned}\sum_{s\in S}v^{\pi}(s)&=\sum_{s\in S}(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}P_{t}^{\pi}(s)\\&=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\sum_{s\in s}P_{t}^{\pi}(s)\\&=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\\&=(1-\gamma)\frac{1}{1-\gamma}\\&=1\end{aligned}
$$










### 2.4 è’™ç‰¹å¡æ´›æ–¹æ³•

1. æ¦‚å¿µï¼š

é€šè¿‡ä½¿ç”¨é‡å¤éšæœºé‡‡æ ·ï¼Œç„¶åè¿ç”¨æ¦‚ç‡ç»Ÿè®¡æ–¹æ³•ä»æŠ½æ ·ç»“æœä¸­å½’çº³å‡ºæƒ³æ±‚çš„ç›®æ ‡çš„æ•°å€¼ä¼°è®¡ã€‚

2. ä¼°è®¡ç­–ç•¥çš„çŠ¶æ€ä»·å€¼å‡½æ•°ï¼š

(1) ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼å‡½æ•°æ˜¯å®ƒçš„æœŸæœ›å›æŠ¥ï¼Œå› æ­¤æ±‚è§£æ–¹æ³•æ˜¯ç”¨ç­–ç•¥åœ¨MDPä¸Šé‡‡æ ·å¾ˆå¤šåºåˆ—ï¼Œç„¶åè®¡ç®—ä»è¿™ä¸ªçŠ¶æ€å‡ºå‘çš„å›æŠ¥å†æ±‚å…¶æœŸæœ›ã€‚
$$
V^\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]\approx\frac{1}{N}\sum_{i=1}^NG_t^{(i)}
$$


(2) è®¡ç®—è¿‡ç¨‹ï¼š

![image-20250518141628981](./Reinforcement Learning.assets/image-20250518141628981-1747548994347-1.png)



## Chapter 3 Dynamic Programming

### 3.1 æ¦‚å¿µ

1. åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼šè¦æ±‚äº‹å…ˆçŸ¥é“æˆ–è€…æ ¹æ®æ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’é‡‡æ ·åˆ°çš„æ•°æ®å­¦ä¹ å¾—åˆ°ç¯å¢ƒçš„çŠ¶æ€è½¬ç§»å‡½æ•°å’Œå¥–åŠ±å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯éœ€è¦çŸ¥é“æ•´ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹
2. ç±»å‹ï¼š

**ç­–ç•¥è¿­ä»£ï¼šç­–ç•¥è¯„ä¼°+ç­–ç•¥æå‡**

ä»·å€¼è¿­ä»£



### 3.2 æ‚¬å´–æ¼«æ­¥é—®é¢˜ï¼ˆQè¡¨æ ¼ï¼‰

![img](./Reinforcement Learning.assets/540.f28e3c6f-1745210415566-9.png)

å¦‚å›¾æ‰€ç¤ºï¼Œæœ‰ä¸€ä¸ª4X12 çš„æ‚¬å´–ç½‘æ ¼ï¼Œæœ€ä¸‹é¢ä¸€æ’é™¤äº†èµ·ç‚¹å’Œç»ˆç‚¹éƒ½æ˜¯æ‚¬å´–ã€‚å¦‚æœæ™ºèƒ½ä½“é‡‡å–åŠ¨ä½œåè§¦ç¢°åˆ°è¾¹ç•Œå¢™å£åˆ™çŠ¶æ€ä¸å‘ç”Ÿæ”¹å˜ï¼Œå¦åˆ™å°±ä¼šç›¸åº”åˆ°è¾¾ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚ç¯å¢ƒä¸­æœ‰ä¸€æ®µæ‚¬å´–ï¼Œæ™ºèƒ½ä½“æ‰å…¥æ‚¬å´–æˆ–åˆ°è¾¾ç›®æ ‡çŠ¶æ€éƒ½ä¼šç»“æŸåŠ¨ä½œå¹¶å›åˆ°èµ·ç‚¹ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰å…¥æ‚¬å´–æˆ–è€…è¾¾åˆ°ç›®æ ‡çŠ¶æ€æ˜¯ç»ˆæ­¢çŠ¶æ€ã€‚æ™ºèƒ½ä½“æ¯èµ°ä¸€æ­¥çš„å¥–åŠ±æ˜¯ âˆ’1ï¼Œæ‰å…¥æ‚¬å´–çš„å¥–åŠ±æ˜¯ âˆ’100ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt


class CliffWalkingEnv:
    """ æ‚¬å´–æ¼«æ­¥ç¯å¢ƒ"""
    def __init__(self, ncol=12, nrow=4):
        self.ncol = ncol  # å®šä¹‰ç½‘æ ¼ä¸–ç•Œçš„åˆ—
        self.nrow = nrow  # å®šä¹‰ç½‘æ ¼ä¸–ç•Œçš„è¡Œ
        # è½¬ç§»çŸ©é˜µP[state][action] = [(p, next_state, reward, done)]åŒ…å«ä¸‹ä¸€ä¸ªçŠ¶æ€å’Œå¥–åŠ±ï¼Œdoneè¡¨ç¤ºåŠ¨ä½œæ˜¯å¦ç»ˆæ­¢ï¼ˆæ˜¯å¦åˆ°è¾¾æ‚¬å´–æˆ–ç»ˆç‚¹ï¼‰
        self.P = self.createP()
    
    def createP(self):
        # åˆå§‹åŒ–
        P = [[[] for _ in range(4)] for _ in range(self.nrow * self.ncol)]
        # 4ç§åŠ¨ä½œ, change[0]:ä¸Š,change[1]:ä¸‹, change[2]:å·¦, change[3]:å³ã€‚åæ ‡ç³»åŸç‚¹(0,0)åœ¨å·¦ä¸Šè§’
        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]
        for i in range(self.nrow):
            for j in range(self.ncol):
                for a in range(4): #æ­¤æ—¶éå†changeé‡Œé¢çš„å››ä¸ªåŠ¨ä½œ
                    # ä½ç½®åœ¨æ‚¬å´–æˆ–ç»ˆç‚¹æ—¶æ— æ³•ç»§ç»­äº¤äº’
                    if i == self.nrow - 1 and j > 0:
                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0, True)]
                        continue
                    
                    # è®¡ç®—ä¸‹ä¸€ä¸ªä½ç½®
                    next_x = min(self.ncol - 1, max(0, j + change[a][0])) #é˜²æ­¢è¶…å‡ºå·¦æˆ–ä¸Šè¾¹ç•Œ
                    next_y = min(self.nrow - 1, max(0, i + change[a][1])) #é˜²æ­¢è¶…å‡ºå³æˆ–ä¸‹è¾¹ç•Œ
                    next_state = next_y * self.ncol + next_x #è½¬åŒ–æˆä¸€ç»´ç´¢å¼•
                    reward = -1
                    done = False
                    
                    # åˆ¤æ–­ä¸‹ä¸€ä¸ªä½ç½®æ˜¯å¦åœ¨æ‚¬å´–æˆ–ç»ˆç‚¹
                    if next_y == self.nrow - 1 and next_x > 0:
                        done = True
                        if next_x != self.ncol - 1:  # æ‚¬å´–
                            reward = -100
                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]
        return P


# åˆ›å»ºç¯å¢ƒå®ä¾‹
env = CliffWalkingEnv(ncol=12, nrow=4)

# åˆå§‹åŒ–Qè¡¨
n_states = env.nrow * env.ncol
n_actions = 4
Q = np.zeros((n_states, n_actions))

# è¶…å‚æ•°è®¾ç½®
alpha = 0.1    # å­¦ä¹ ç‡
gamma = 0.99   # æŠ˜æ‰£å› å­
epsilon = 0.1  # æ¢ç´¢ç‡
num_episodes = 500  # è®­ç»ƒçš„æ€»episodeæ•°

# è®­ç»ƒè¿‡ç¨‹
rewards = []  # è®°å½•æ¯ä¸ªepisodeçš„æ€»å¥–åŠ±

for episode in range(num_episodes):
    state = 3 * env.ncol + 0  # åˆå§‹çŠ¶æ€ï¼šå·¦ä¸‹è§’(3,0)
    done = False
    total_reward = 0
    
    while not done:
        # epsilon-greedyé€‰æ‹©åŠ¨ä½œ
        if np.random.rand() < epsilon:
            action = np.random.randint(n_actions)
        else:
            action = np.argmax(Q[state])
        
        # æ‰§è¡ŒåŠ¨ä½œï¼Œå¾—åˆ°è½¬ç§»ä¿¡æ¯
        p, next_state, reward, done = env.P[state][action][0]
        
        # æ›´æ–°Qè¡¨
        Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])
        
        # æ›´æ–°çŠ¶æ€å’Œæ€»å¥–åŠ±
        state = next_state
        total_reward += reward
        
    rewards.append(total_reward)

# æµ‹è¯•ç­–ç•¥
def test_policy(Q):
    state = 3 * env.ncol + 0  # åˆå§‹çŠ¶æ€
    path = []
    done = False
    while not done:
        action = np.argmax(Q[state])
        path.append((state // env.ncol, state % env.ncol))  # è®°å½•åæ ‡
        p, next_state, reward, done = env.P[state][action][0]
        state = next_state
        if len(path) > 100:  # é˜²æ­¢æ— é™å¾ªç¯
            break
    path.append((state // env.ncol, state % env.ncol))  # æ·»åŠ ç»ˆç‚¹
    return path

# å¯è§†åŒ–ç»“æœ
path = test_policy(Q)
print("æœ€ä¼˜è·¯å¾„åæ ‡åºåˆ—ï¼š")
print(path)

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
plt.plot(rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Q-learning Training Performance')
plt.show()
```



### 3.3 ç­–ç•¥è¿­ä»£ç®—æ³•

1. **ç­–ç•¥è¯„ä¼°**ï¼šè®¡ç®—ä¸€ä¸ªç­–ç•¥Ï€ä¸‹ä»çŠ¶æ€så‡ºå‘å¯ä»¥å¾—åˆ°çš„**çŠ¶æ€ä»·å€¼å‡½æ•°**

è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼š$V^\pi(s)=\sum_{a\in A}\pi(a\mid s)\left(R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V^\pi\left(s^{\prime}\right)\right)$

ä½¿ç”¨ä¸Šä¸€è½®çš„çŠ¶æ€ä»·å€¼å‡½æ•°æ¥è®¡ç®—å½“å‰ä¸€è½®çš„çŠ¶æ€ä»·å€¼å‡½æ•°ï¼š$V^{k+1}(s)=\sum_{a\in A}\pi(a|s)\left(r(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)V^k(s^{\prime})\right)$

ä¸åŠ¨ç‚¹ï¼šV^k = V^piï¼Œå› æ­¤å¦‚æœæŸä¸€è½® $\max_{s\in\mathcal{S}}|V^{k+1}(s)-V^{k}(s)|$çš„å€¼éå¸¸å°ï¼Œå¯ä»¥æå‰ç»“æŸç­–ç•¥è¯„ä¼°ã€‚



2. **ç­–ç•¥æå‡**ï¼šæ ¹æ®çŠ¶æ€ä»·å€¼å‡½æ•°æ”¹è¿›å½“å‰ç­–ç•¥Ï€ï¼Œä»è€Œæé«˜æœ€ç»ˆçš„æœŸæœ›å›æŠ¥$V^\pi(s)$ï¼Œè¿›è€Œ**å¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„ç­–ç•¥Ï€â€™**

å‡è®¾æ™ºèƒ½ä½“åœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaï¼Œä¹‹åçš„åŠ¨ä½œä¾æ—§éµå¾ªç­–ç•¥Ï€ï¼Œæ­¤æ—¶çš„æœŸæœ›å›æŠ¥ä¸º$Q_{\pi}(s,a)$ï¼Œå¦‚æœ$Q^{\pi}(s,a)>V^{\pi}(s)$ï¼Œåˆ™è¯´æ˜åœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaä¼šæ¯”åŸæ¥çš„ç­–ç•¥Ï€å¾—åˆ°æ›´é«˜çš„æœŸæœ›å›æŠ¥ã€‚

ğŸ™Œæœ€åä¸€å¥è¯æ„æ€ï¼šé€šè¿‡åœ¨çŠ¶æ€sä¸‹åªé€‰æ‹©æŸä¸€ä¸ªåŠ¨ä½œï¼Œå»é™¤åŠ æƒå¹³å‡ï¼Œä»è€Œæé«˜æœŸæœ›å›æŠ¥ã€‚

![1743238395174](Reinforcement Learning.assets/1743238395174.png)



âœ¨ç­–ç•¥æå‡å®šç†ï¼šç°å‡è®¾ä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥Ï€â€˜ï¼Œåœ¨ä»»æ„ä¸€ä¸ªçŠ¶æ€ä¸‹ï¼Œéƒ½æ»¡è¶³ï¼š

$Q^\pi(s,\pi^{\prime}(s))\geq V^\pi(s)$

äºæ˜¯åœ¨ä»»æ„çŠ¶æ€ä¸‹ï¼š

$$V^{\pi^{\prime}}(s)\geq V^\pi(s)$$

å› æ­¤å¯ä»¥é€šè¿‡åœ¨æ¯ä¸€ä¸ªçŠ¶æ€ä¸‹é€‰æ‹©åŠ¨ä½œä»·å€¼æœ€å¤§çš„åŠ¨ä½œï¼Œä»è€Œæå‡æœ€ç»ˆçš„ä»·å€¼å‡½æ•°ï¼š

$\pi^{\prime}(s)=\arg\max_aQ^\pi(s,a)=\arg\max_a\{r(s,a)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,a)V^\pi(s^{\prime})\}$

ğŸŒŸè¯æ˜ï¼šå…³é”®æ˜¯ç”¨åˆ°$V^{\pi}(S_{t+1})\leq Q^{\pi}(S_{t+1},\pi^{\prime}(S_{t+1}))$

$\begin{aligned}V^{\pi}(s)&\leq Q^{\pi}(s,\pi^{\prime}(s))\\&=\mathbb{E}_{\pi^{\prime}}[R_t+\gamma V^\pi(S_{t+1})|S_t=s]\\&\leq\mathbb{E}_{\pi^{\prime}}[R_t+\gamma Q^\pi(S_{t+1},\pi^{\prime}(S_{t+1}))|S_t=s]\\&=\mathbb{E}_{\pi^{\prime}}[R_t+\gamma R_{t+1}+\gamma^2V^\pi(S_{t+2})|S_t=s]\\&\leq\mathbb{E}_{\pi^{\prime}}[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\gamma^3V^\pi(S_{t+3})|S_t=s]\\&\leq\mathbb{E}_{\pi^{\prime}}[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\gamma^3R_{t+3}+\cdots|S_t=s]\\&=V^{\pi^{\prime}}(s)\end{aligned}$



3. **ç­–ç•¥è¿­ä»£ç®—æ³•ï¼š**

![1743249554905](./Reinforcement Learning.assets/1743249554905-1745210420952-11-1745210422283-13-1745210423862-15.png)



```python
import copy
import numpy as np
import sys
from collections import defaultdict

class PolicyIteration:
    """ç­–ç•¥è¿­ä»£ç®—æ³•"""
    def __init__(self, env, theta, gamma):
        self.env = env
        self.v = [0] * (self.env.ncol * self.env.nrow)  # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        # åˆå§‹åŒ–å‡åŒ€éšæœºç­–ç•¥ï¼ˆæ¯ä¸ªåŠ¨ä½œæ¦‚ç‡ç›¸ç­‰ï¼‰
        self.pi = [[0.25] * 4 for _ in range(self.env.ncol * self.env.nrow)]
        self.theta = theta    # ç­–ç•¥è¯„ä¼°æ”¶æ•›é˜ˆå€¼
        self.gamma = gamma    # æŠ˜æ‰£å› å­

    def compute_q_values(self, s):
        """è®¡ç®—æŒ‡å®šçŠ¶æ€ä¸‹æ‰€æœ‰åŠ¨ä½œçš„Qå€¼"""
        q_values = []
        for a in range(4):
            q = 0
            for trans_prob, next_state, reward, done in self.env.P[s][a]:
                # è®¡ç®—Qå€¼ï¼šè€ƒè™‘è½¬ç§»æ¦‚ç‡å’Œç»ˆæ­¢çŠ¶æ€å¤„ç†
                q += trans_prob * (reward + self.gamma * self.v[next_state] * (1 - done))
            q_values.append(q)
        return q_values

    def policy_evaluation(self):
        """ç­–ç•¥è¯„ä¼°ï¼ˆä½¿ç”¨å½“å‰ç­–ç•¥æ›´æ–°ä»·å€¼å‡½æ•°ï¼‰"""
        iteration = 0
        while True:
            max_diff = 0
            new_v = [0] * len(self.v)
            for s in range(len(self.v)):
                # ä½¿ç”¨å½“å‰ç­–ç•¥è®¡ç®—çŠ¶æ€ä»·å€¼
                q_values = self.compute_q_values(s)
                new_v[s] = sum(self.pi[s][a] * q_values[a] for a in range(4))
                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))
            
            self.v = new_v
            iteration += 1
            
            if max_diff < self.theta:
                print(f"ç­–ç•¥è¯„ä¼°ç»è¿‡{iteration}è½®è¿­ä»£åæ”¶æ•›")
                return

    def policy_improvement(self):
        """ç­–ç•¥æå‡ï¼ˆæ ¹æ®å½“å‰ä»·å€¼å‡½æ•°ä¼˜åŒ–ç­–ç•¥ï¼‰"""
        policy_stable = True # ç­–ç•¥æ˜¯å¦ç¨³å®š
        for s in range(len(self.v)):
            # è·å–å½“å‰çŠ¶æ€çš„æ‰€æœ‰Qå€¼
            q_values = self.compute_q_values(s)
            
            # æ‰¾åˆ°æœ€å¤§Qå€¼å¯¹åº”çš„åŠ¨ä½œ
            max_q = max(q_values)
            best_actions = [a for a, q in enumerate(q_values) if q == max_q]
            
            # æ„é€ æ–°çš„ç­–ç•¥ï¼ˆå‡åŒ€åˆ†å¸ƒåœ¨æœ€ä¼˜åŠ¨ä½œä¸Šï¼‰
            new_policy = [0.] * 4
            prob = 1.0 / len(best_actions)
            for a in best_actions:
                new_policy[a] = prob
                
            # æ£€æŸ¥ç­–ç•¥æ˜¯å¦å˜åŒ–
            if self.pi[s] != new_policy:
                policy_stable = False
                
            self.pi[s] = new_policy
            
        return policy_stable

    def policy_iteration(self):
        """æ‰§è¡Œå®Œæ•´çš„ç­–ç•¥è¿­ä»£"""
        iteration = 0 # è¿­ä»£æ¬¡æ•°    
        while True:
            iteration += 1
            print(f"\n=== ç­–ç•¥è¿­ä»£ç¬¬{iteration}è½® ===")
            
            self.policy_evaluation()
            # è¿›è¡Œç­–ç•¥æå‡å¹¶æ£€æŸ¥ç­–ç•¥ç¨³å®šæ€§
            if self.policy_improvement():
                print("ç­–ç•¥å·²ç¨³å®šï¼Œåœæ­¢è¿­ä»£")
                return

class CliffWalkingEnv:
    """æ‚¬å´–æ¼«æ­¥ç¯å¢ƒ"""
    def __init__(self):
        self.ncol = 12  
        self.nrow = 4   
        self.P = self.createP()  

    def createP(self):
        P = defaultdict(dict)
        # éå†æ‰€æœ‰çŠ¶æ€
        for s in range(self.nrow * self.ncol):
            row, col = s // self.ncol, s % self.ncol
            P[s] = defaultdict(list)
            # éå†æ‰€æœ‰åŠ¨ä½œ
            for a in range(4):  # ä¸Šä¸‹å·¦å³å››ä¸ªåŠ¨ä½œ
                # è®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä½ç½®
                next_s = self.step(row, col, a)
                next_row, next_col = next_s

                # è®¡ç®—å›æŠ¥
                reward = -1.0
                # å¦‚æœä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯æ‚¬å´–æˆ–è€…ç»ˆç‚¹
                if self.is_cliff(next_row, next_col):
                    done = True
                    reward = -100.0
                    next_s = self.encode_state(3, 0)  # å›åˆ°èµ·ç‚¹
                elif next_row == 3 and next_col == 11:
                    done = True  # åˆ°è¾¾ç»ˆç‚¹
                else:
                    done = False

                next_s = self.encode_state(next_row, next_col)
                P[s][a].append((1.0, next_s, reward, done))
        return P

    def step(self, row, col, action):
        """æ‰§è¡ŒåŠ¨ä½œåçš„ä¸‹ä¸€ä¸ªä½ç½®"""
        if action == 0:  # ä¸Š
            next_row = max(row - 1, 0)
            next_col = col
        elif action == 1:  # ä¸‹
            next_row = min(row + 1, self.nrow - 1)
            next_col = col
        elif action == 2:  # å·¦
            next_row = row
            next_col = max(col - 1, 0)
        elif action == 3:  # å³
            next_row = row
            next_col = min(col + 1, self.ncol - 1)
        return next_row, next_col

    def encode_state(self, row, col):
        """å°†è¡Œåˆ—ä½ç½®ç¼–ç ä¸ºä¸€ç»´çŠ¶æ€ç´¢å¼•"""
        return row * self.ncol + col

    def is_cliff(self, row, col):
        """åˆ¤æ–­æ˜¯å¦æ˜¯æ‚¬å´–ä½ç½®"""
        return row == 3 and 1 <= col <= 10
    
def print_agent(agent, action_meaning, disaster=[], end=[]):
    """å¯è§†åŒ–çŠ¶æ€ä»·å€¼å’Œç­–ç•¥"""
    print("çŠ¶æ€ä»·å€¼ï¼š")
    grid_size = agent.env.ncol
    for i in range(agent.env.nrow):
        for j in range(grid_size):
            state = i * grid_size + j
            print(f"{agent.v[state]:6.3f}", end=" ")
        print()

    print("\nç­–ç•¥ï¼š")
    for i in range(agent.env.nrow):
        for j in range(grid_size):
            state = i * grid_size + j
            if state in disaster:
                print("XXXX", end=" ")
            elif state in end:
                print("EEEE", end=" ")
            else:
                policy = agent.pi[state]
                # å°†ç­–ç•¥è½¬æ¢ä¸ºç®­å¤´è¡¨ç¤º
                arrows = [action_meaning[a] if prob > 0 else "" for a, prob in enumerate(policy)]
                combined = "".join(arrows).ljust(4, 'o')  # ä¿æŒ4å­—ç¬¦å®½åº¦
                print(combined, end=" ")
        print()
# ç¤ºä¾‹ä½¿ç”¨ï¼ˆéœ€è¦CliffWalkingEnvç¯å¢ƒå®ç°ï¼‰
if __name__ == "__main__":
    # åˆ›å»ºæ‚¬å´–æ¼«æ­¥ç¯å¢ƒå®ä¾‹
    env = CliffWalkingEnv()
    action_meaning = ['â†‘', 'â†“', 'â†', 'â†’']  # åŠ¨ä½œå«ä¹‰
    
    # åˆå§‹åŒ–ç­–ç•¥è¿­ä»£å‚æ•°
    theta = 0.001
    gamma = 0.9
    agent = PolicyIteration(env, theta, gamma)
    
    # æ‰§è¡Œç­–ç•¥è¿­ä»£
    agent.policy_iteration()
    
    # æ‰“å°ç»“æœï¼ˆæ‚¬å´–çŠ¶æ€37-46ï¼Œç»ˆç‚¹47ï¼‰
    print_agent(agent, action_meaning, list(range(37, 47)), [47])
```



### 3.4 ä»·å€¼è¿­ä»£ç®—æ³•

1. èµ·å› ï¼š

ç­–ç•¥è¿­ä»£ä¸­çš„ç­–ç•¥è¯„ä¼°éœ€è¦è¿›è¡Œå¾ˆå¤šè½®æ‰èƒ½æ”¶æ•›å¾—åˆ°æŸä¸€ç­–ç•¥çš„çŠ¶æ€å‡½æ•°ï¼Œè¿™éœ€è¦å¾ˆå¤§çš„è®¡ç®—é‡ï¼Œå°¤å…¶æ˜¯åœ¨çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´æ¯”è¾ƒå¤§çš„æƒ…å†µä¸‹



2. ä»·å€¼è¿­ä»£ç®—æ³•ï¼š

è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹çš„æ›´æ–°å…¬å¼ï¼š

$$V^{k+1}(s) = \max_{a\in A}\{r(s,a) + \gamma \sum_{s'\in S} P(s'|s,a)V^k(s')\}$$

ä»·å€¼è¿­ä»£å°±æ˜¯æŒ‰ç…§ä¸Šè¿°æ›´æ–°æ–¹å¼è¿›è¡Œçš„ã€‚å½“ $V^{k+1}$ å’Œ $V^k$ ç›¸åŒæ—¶ï¼Œè¯´æ˜æ˜¯è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹çš„ä¸åŠ¨ç‚¹ï¼Œæ­¤æ—¶å¯¹åº”ç€æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•° $V^*$ã€‚ç„¶åæˆ‘ä»¬åˆ©ç”¨ï¼š

$$\pi(s) = \arg\max_a\{r(s,a) + \gamma \sum_{s'} p(s'|s,a)V^{k+1}(s')\}$$

ä»ä¸­æ¢å¤å‡ºæœ€ä¼˜ç­–ç•¥å³å¯ã€‚

ä»·å€¼è¿­ä»£ç®—æ³•æµç¨‹å¦‚ä¸‹ï¼š

- éšæœºåˆå§‹åŒ– $V(s)$
- while $\Delta > \theta$ do:
  - $\Delta \leftarrow 0$
  - å¯¹äºæ¯ä¸€ä¸ªçŠ¶æ€ $s \in S$:
    - $v \leftarrow V(s)$
    - $V(s) \leftarrow \max_a\{r(s,a) + \gamma \sum_{s'} P(s'|s,a)V(s')\}$
    - $\Delta \leftarrow \max(\Delta, |v - V(s)|)$
- end while
- è¿”å›ä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥ $\pi(s) = \arg\max_a\{r(s,a) + \gamma \sum_{s'} P(s'|s,a)V(s')\}$

```python
class ValueIteration:
    """ ä»·å€¼è¿­ä»£ç®—æ³• """
    def __init__(self, env, theta, gamma):
        self.env = env
        self.v = [0] * self.env.ncol * self.env.nrow  # åˆå§‹åŒ–ä»·å€¼ä¸º0
        self.theta = theta  # ä»·å€¼æ”¶æ•›é˜ˆå€¼
        self.gamma = gamma
        # ä»·å€¼è¿­ä»£ç»“æŸåå¾—åˆ°çš„ç­–ç•¥
        self.pi = [None for i in range(self.env.ncol * self.env.nrow)]

    def value_iteration(self):
        cnt = 0
        while 1:
            max_diff = 0
            new_v = [0] * self.env.ncol * self.env.nrow
            for s in range(self.env.ncol * self.env.nrow):
                qsa_list = []  # å¼€å§‹è®¡ç®—çŠ¶æ€sä¸‹çš„æ‰€æœ‰Q(s,a)ä»·å€¼
                for a in range(4):
                    qsa = 0
                    for res in self.env.P[s][a]:
                        p, next_state, r, done = res
                        qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))
                    qsa_list.append(qsa)  # è¿™ä¸€è¡Œå’Œä¸‹ä¸€è¡Œä»£ç æ˜¯ä»·å€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£çš„ä¸»è¦åŒºåˆ«
                new_v[s] = max(qsa_list)
                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))
            self.v = new_v
            if max_diff < self.theta: break  # æ»¡è¶³æ”¶æ•›æ¡ä»¶,é€€å‡ºè¯„ä¼°è¿­ä»£
            cnt += 1
        print("ä»·å€¼è¿­ä»£ä¸€å…±è¿›è¡Œ%dè½®" % cnt)
        self.get_policy()

    def get_policy(self):  # æ ¹æ®ä»·å€¼å‡½æ•°å¯¼å‡ºä¸€ä¸ªè´ªå©ªç­–ç•¥
        for s in range(self.env.nrow * self.env.ncol):
            qsa_list = []
            for a in range(4):
                qsa = 0
                for res in self.env.P[s][a]:
                    p, next_state, r, done = res
                    qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))
                qsa_list.append(qsa)
            maxq = max(qsa_list)
            cntq = qsa_list.count(maxq)  # è®¡ç®—æœ‰å‡ ä¸ªåŠ¨ä½œå¾—åˆ°äº†æœ€å¤§çš„Qå€¼
            # è®©è¿™äº›åŠ¨ä½œå‡åˆ†æ¦‚ç‡
            self.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]


env = CliffWalkingEnv()
action_meaning = ['^', 'v', '<', '>']
theta = 0.001
gamma = 0.9
agent = ValueIteration(env, theta, gamma)
agent.value_iteration()
print_agent(agent, action_meaning, list(range(37, 47)), [47])
```



### 3.5 å†°æ´é—®é¢˜

ç½‘æ ¼ä¸–ç•Œï¼Œå¤§å°ä¸º4 Ã— 4ã€‚æ¯ä¸€ä¸ªæ–¹æ ¼æ˜¯ä¸€ä¸ªçŠ¶æ€ï¼Œæ™ºèƒ½ä½“èµ·ç‚¹çŠ¶æ€Såœ¨å·¦ä¸Šè§’ï¼Œç›®æ ‡çŠ¶æ€Gåœ¨å³ä¸‹è§’ï¼Œä¸­é—´è¿˜æœ‰è‹¥å¹²å†°æ´Hã€‚åœ¨æ¯ä¸€ä¸ªçŠ¶æ€éƒ½å¯ä»¥é‡‡å–ä¸Šã€ä¸‹ã€å·¦ã€å³ 4 ä¸ªåŠ¨ä½œã€‚ç”±äºæ™ºèƒ½ä½“åœ¨å†°é¢è¡Œèµ°ï¼Œå› æ­¤æ¯æ¬¡è¡Œèµ°éƒ½æœ‰ä¸€å®šçš„æ¦‚ç‡æ»‘è¡Œåˆ°é™„è¿‘çš„å…¶å®ƒçŠ¶æ€ï¼Œå¹¶ä¸”åˆ°è¾¾å†°æ´æˆ–ç›®æ ‡çŠ¶æ€æ—¶è¡Œèµ°ä¼šæå‰ç»“æŸã€‚æ¯ä¸€æ­¥è¡Œèµ°çš„å¥–åŠ±æ˜¯ 0ï¼Œåˆ°è¾¾ç›®æ ‡çš„å¥–åŠ±æ˜¯ 1ã€‚

```python
import gymnasium as gym 

class PolicyIteration:
    def __init__(self, env, theta, gamma):
        self.env = env
        self.n_states = env.observation_space.n  # ä½¿ç”¨ observation_space.n æ›¿ä»£ nS
        self.n_actions = env.action_space.n      # ä½¿ç”¨ action_space.n æ›¿ä»£ nA
        self.v = [0] * self.n_states            # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        self.pi = [[0.25] * self.n_actions for _ in range(self.n_states)]  # åˆå§‹åŒ–å‡åŒ€éšæœºç­–ç•¥
        self.theta = theta
        self.gamma = gamma

    def policy_evaluation(self):
        while True:
            delta = 0
            for s in range(self.n_states):
                v = self.v[s]
                self.v[s] = sum(self.pi[s][a] * sum(p * (r + self.gamma * self.v[next_s])
                    for p, next_s, r, _ in self.env.P[s][a]) for a in range(self.n_actions))
                delta = max(delta, abs(v - self.v[s]))
            if delta < self.theta:
                break

    def policy_improvement(self):
        policy_stable = True
        for s in range(self.n_states):
            old = self.pi[s].copy()
            q_values = [sum(p * (r + self.gamma * self.v[next_s])
                for p, next_s, r, _ in self.env.P[s][a]) for a in range(self.n_actions)]
            best_a = max(range(self.n_actions), key=lambda a: q_values[a])
            self.pi[s] = [1.0 if a == best_a else 0.0 for a in range(self.n_actions)]
            if old != self.pi[s]:
                policy_stable = False
        return policy_stable

    def policy_iteration(self):
        while True:
            self.policy_evaluation()
            if self.policy_improvement():
                break

def print_agent(agent, action_meaning, disaster=[], end=[]):
    print("çŠ¶æ€ä»·å€¼ï¼š")
    for i in range(4):
        for j in range(4):
            print(f'{agent.v[i*4+j]:6.2f}', end=' ')
        print()

    print("ç­–ç•¥ï¼š")
    for i in range(4):
        for j in range(4):
            state = i * 4 + j
            if state in disaster:
                print('XXXX', end=' ')
            elif state in end:
                print('EEEE', end=' ')
            else:
                policy = agent.pi[state]
                arrows = [action_meaning[a] if prob > 0 else '' for a, prob in enumerate(policy)]
                print(''.join(arrows).ljust(4, 'o'), end=' ')
        print()



env = gym.make("FrozenLake-v1")  # åˆ›å»ºç¯å¢ƒ
env = env.unwrapped  # è§£å°è£…æ‰èƒ½è®¿é—®çŠ¶æ€è½¬ç§»çŸ©é˜µP
env.render()  # ç¯å¢ƒæ¸²æŸ“,é€šå¸¸æ˜¯å¼¹çª—æ˜¾ç¤ºæˆ–æ‰“å°å‡ºå¯è§†åŒ–çš„ç¯å¢ƒ

holes = set()
ends = set()
for s in env.P:
    for a in env.P[s]:
        for s_ in env.P[s][a]:
            if s_[2] == 1.0:  # è·å¾—å¥–åŠ±ä¸º1,ä»£è¡¨æ˜¯ç›®æ ‡
                ends.add(s_[1])
            if s_[3] == True:
                holes.add(s_[1])
holes = holes - ends
print("å†°æ´çš„ç´¢å¼•:", holes)
print("ç›®æ ‡çš„ç´¢å¼•:", ends)

for a in env.P[14]:  # æŸ¥çœ‹ç›®æ ‡å·¦è¾¹ä¸€æ ¼çš„çŠ¶æ€è½¬ç§»ä¿¡æ¯
    print(env.P[14][a])
    
    
# ç­–ç•¥è¿­ä»£ç®—æ³•
# è¿™ä¸ªåŠ¨ä½œæ„ä¹‰æ˜¯Gymåº“é’ˆå¯¹å†°æ¹–ç¯å¢ƒäº‹å…ˆè§„å®šå¥½çš„
action_meaning = ['<', 'v', '>', '^']
theta = 1e-5
gamma = 0.9
agent = PolicyIteration(env, theta, gamma)
agent.policy_iteration()
print_agent(agent, action_meaning, [5, 7, 11, 12], [15])



#ä»·å€¼è¿­ä»£ç®—æ³•
action_meaning = ['<', 'v', '>', '^']
theta = 1e-5
gamma = 0.9
agent = ValueIteration(env, theta, gamma)
agent.value_iteration()
print_agent(agent, action_meaning, [5, 7, 11, 12], [15])
```





## Chapter 4 Temporal Difference Algorithm

### 4.1 æ— æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ 

1. å¤§éƒ¨åˆ†å¼ºåŒ–å­¦ä¹ ç°å®åœºæ™¯ï¼Œå…¶é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„çŠ¶æ€è½¬ç§»æ¦‚ç‡æ˜¯

   æ— æ³•å†™å‡ºæ¥çš„ï¼Œä¹Ÿå°±æ— æ³•ç›´æ¥è¿›è¡ŒåŠ¨æ€è§„åˆ’ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ™ºèƒ½

   ä½“åªèƒ½å’Œç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œé€šè¿‡é‡‡æ ·åˆ°çš„æ•°æ®æ¥å­¦ä¹ ã€‚



2. ç‰¹ç‚¹ï¼š

* **ä¸éœ€è¦äº‹å…ˆçŸ¥é“ç¯å¢ƒçš„å¥–åŠ±å‡½æ•°å’ŒçŠ¶æ€è½¬ç§»å‡½æ•°**ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨å’Œç¯å¢ƒäº¤äº’çš„è¿‡ç¨‹ä¸­é‡‡æ ·åˆ°çš„æ•°æ®æ¥å­¦ä¹ ã€‚

![img](./Reinforcement Learning.assets/480.25b67b37-1748008231429-22-1748008232940-24.png)





**ç­–ç•¥è¯„ä¼°ï¼ˆæ—¶åºå·®åˆ†ç®—æ³•ï¼‰+ ç­–ç•¥æå‡ï¼ˆSarsaç®—æ³•ï¼‰**



### 4.2 æ—¶åºå·®åˆ†ç®—æ³•

1. ç­–ç•¥è¯„ä¼°ï¼š

æ—¶åºå·®åˆ†ç®—æ³•ç”¨å½“å‰è·å¾—çš„å¥–åŠ±åŠ ä¸Šä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ä¼°è®¡æ¥ä½œä¸ºåœ¨å½“å‰çŠ¶æ€ä¼š

è·å¾—çš„å›æŠ¥,å…¶è¡¨è¾¾å¼å¦‚ä¸‹ï¼Œç›¸å½“äºå°†è’™ç‰¹å¡æ´›ç®—æ³•ä¸­çš„Gtç”¨$r_t+\gamma V(s_{t+1})$ä»£æ›¿
$$
V(s_t)\leftarrow V(s_t)+\alpha[r_t+\gamma V(s_{t+1})-V(s_t)]
$$
æ³¨ï¼š$R_t+\gamma V(s_{t+1})-V(s_t)$ä¸ºæ—¶åºå·®åˆ†è¯¯å·®



2. ç­–ç•¥æå‡ï¼š

ä½¿ç”¨æ—¶åºå·®åˆ†ç®—æ³•æ¥ä¼°è®¡åŠ¨ä½œä»·å€¼å‡½æ•°Qï¼›
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]
$$
æ¥ç€ä½¿ç”¨è´ªå©ªç®—æ³•æ¥é€‰å–åœ¨æŸä¸ªçŠ¶æ€ä¸‹åŠ¨ä½œä»·å€¼æœ€å¤§çš„åŠ¨ä½œ - $\arg\max_{a}Q(s,a)$



3. å¯¹æ¯”ï¼š

|              |                           åå·®                           |                             æ–¹å·®                             |
| :----------: | :------------------------------------------------------: | :----------------------------------------------------------: |
| è’™ç‰¹å¡æ´›æ–¹æ³• | æ— åï¼ˆåˆ©ç”¨å½“å‰çŠ¶æ€ä¹‹åæ¯ä¸€æ­¥çš„å¥–åŠ±è€Œä¸ä½¿ç”¨ä»»ä½•ä»·å€¼ä¼°è®¡ï¼‰ | è¾ƒå¤§çš„æ–¹å·®ï¼ˆæ¯ä¸€æ­¥çš„çŠ¶æ€è½¬ç§»éƒ½æœ‰ä¸ç¡®å®šæ€§ï¼Œè€Œæ¯ä¸€æ­¥çŠ¶æ€é‡‡å–çš„åŠ¨ä½œæ‰€å¾—åˆ°çš„ä¸ä¸€æ ·çš„å¥–åŠ±æœ€ç»ˆéƒ½ä¼šåŠ èµ·æ¥ï¼Œè¿™ä¼šæå¤§å½±å“æœ€ç»ˆçš„ä»·å€¼ä¼°è®¡ï¼‰ |
| æ—¶åºå·®åˆ†æ–¹æ³• |   æœ‰åï¼ˆç”¨åˆ°äº†ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ä¼°è®¡è€Œä¸æ˜¯å…¶çœŸå®çš„ä»·å€¼ï¼‰   |       å°æ–¹å·®ï¼ˆåªå…³æ³¨äº†ä¸€æ­¥çŠ¶æ€è½¬ç§»ï¼Œç”¨åˆ°äº†ä¸€æ­¥çš„å¥–åŠ±ï¼‰       |











### 4.3 Sarsaç®—æ³•

1. ç®—æ³•ï¼š

(1) ä½¿ç”¨æ—¶åºå·®åˆ†ç®—æ³•æ¥ä¼°è®¡åŠ¨ä½œä»·å€¼å‡½æ•°Qï¼›
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]
$$
æ¥ç€ä½¿ç”¨è´ªå©ªç®—æ³•æ¥é€‰å–åœ¨æŸä¸ªçŠ¶æ€ä¸‹åŠ¨ä½œä»·å€¼æœ€å¤§çš„åŠ¨ä½œ - $\arg\max_{a}Q(s,a)$

$\epsilon$-è´ªå©ªç­–ç•¥ï¼šæœ‰$1-\epsilon$çš„æ¦‚ç‡é‡‡ç”¨åŠ¨ä½œä»·å€¼æœ€å¤§çš„é‚£ä¸ªåŠ¨ä½œï¼Œå¦å¤–æœ‰$\epsilon$çš„æ¦‚ç‡ä»åŠ¨ä½œç©ºé—´

ä¸­éšæœºé‡‡å–ä¸€ä¸ªåŠ¨ä½œã€‚
$$
\pi(a|s)=\begin{cases}\epsilon/|\mathcal{A}|+1-\epsilon&\quad\text{å¦‚æœ}a=\arg\max_{a^{\prime}}Q(s,a^{\prime})\\\epsilon/|\mathcal{A}|&\quad\text{å…¶ä»–åŠ¨ä½œ}&\end{cases}
$$


(2) ç®—æ³•è¿‡ç¨‹ï¼š

![image-20250518163008461](./Reinforcement Learning.assets/image-20250518163008461.png)

(3) ä»£ç ï¼š

```python
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm  # tqdmæ˜¯æ˜¾ç¤ºå¾ªç¯è¿›åº¦æ¡çš„åº“


class CliffWalkingEnv:
    def __init__(self, ncol, nrow):
        self.nrow = nrow
        self.ncol = ncol
        self.x = 0  # è®°å½•å½“å‰æ™ºèƒ½ä½“ä½ç½®çš„æ¨ªåæ ‡
        self.y = self.nrow - 1  # è®°å½•å½“å‰æ™ºèƒ½ä½“ä½ç½®çš„çºµåæ ‡

    def step(self, action):  # å¤–éƒ¨è°ƒç”¨è¿™ä¸ªå‡½æ•°æ¥æ”¹å˜å½“å‰ä½ç½®
        # 4ç§åŠ¨ä½œ, change[0]:ä¸Š, change[1]:ä¸‹, change[2]:å·¦, change[3]:å³ã€‚åæ ‡ç³»åŸç‚¹(0,0)
        # å®šä¹‰åœ¨å·¦ä¸Šè§’
        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]
        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))
        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))
        next_state = self.y * self.ncol + self.x
        reward = -1
        done = False
        if self.y == self.nrow - 1 and self.x > 0:  # ä¸‹ä¸€ä¸ªä½ç½®åœ¨æ‚¬å´–æˆ–è€…ç›®æ ‡
            done = True
            if self.x != self.ncol - 1:
                reward = -100
        return next_state, reward, done

    def reset(self):  # å›å½’åˆå§‹çŠ¶æ€,åæ ‡è½´åŸç‚¹åœ¨å·¦ä¸Šè§’
        self.x = 0
        self.y = self.nrow - 1
        return self.y * self.ncol + self.x

class Sarsa:
    """ Sarsaç®—æ³• """
    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):
        self.Q_table = np.zeros([nrow * ncol, n_action])  # åˆå§‹åŒ–Q(s,a)è¡¨æ ¼
        self.n_action = n_action  # åŠ¨ä½œä¸ªæ•°
        self.alpha = alpha  # å­¦ä¹ ç‡
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # epsilon-è´ªå©ªç­–ç•¥ä¸­çš„å‚æ•°

    def take_action(self, state):  # é€‰å–ä¸‹ä¸€æ­¥çš„æ“ä½œ,å…·ä½“å®ç°ä¸ºepsilon-è´ªå©ª
        if np.random.random() < self.epsilon:
            action = np.random.randint(self.n_action)
        else:
            action = np.argmax(self.Q_table[state])
        return action

    def best_action(self, state):  # ç”¨äºæ‰“å°ç­–ç•¥
        Q_max = np.max(self.Q_table[state])
        a = [0 for _ in range(self.n_action)]
        for i in range(self.n_action):  # è‹¥ä¸¤ä¸ªåŠ¨ä½œçš„ä»·å€¼ä¸€æ ·,éƒ½ä¼šè®°å½•ä¸‹æ¥
            if self.Q_table[state, i] == Q_max:
                a[i] = 1
        return a

    def update(self, s0, a0, r, s1, a1):
        td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]
        self.Q_table[s0, a0] += self.alpha * td_error

ncol = 12
nrow = 4
env = CliffWalkingEnv(ncol, nrow)
np.random.seed(0)
epsilon = 0.1
alpha = 0.1
gamma = 0.9
agent = Sarsa(ncol, nrow, epsilon, alpha, gamma)
num_episodes = 500  # æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿è¡Œçš„åºåˆ—çš„æ•°é‡

return_list = []  # è®°å½•æ¯ä¸€æ¡åºåˆ—çš„å›æŠ¥
for i in range(10):  # æ˜¾ç¤º10ä¸ªè¿›åº¦æ¡
    # tqdmçš„è¿›åº¦æ¡åŠŸèƒ½
    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:
        for i_episode in range(int(num_episodes / 10)):  # æ¯ä¸ªè¿›åº¦æ¡çš„åºåˆ—æ•°
            episode_return = 0
            state = env.reset()
            action = agent.take_action(state)
            done = False
            while not done:
                next_state, reward, done = env.step(action)
                next_action = agent.take_action(next_state)
                episode_return += reward  # è¿™é‡Œå›æŠ¥çš„è®¡ç®—ä¸è¿›è¡ŒæŠ˜æ‰£å› å­è¡°å‡
                agent.update(state, action, reward, next_state, next_action)
                state = next_state
                action = next_action
            return_list.append(episode_return)
            if (i_episode + 1) % 10 == 0:  # æ¯10æ¡åºåˆ—æ‰“å°ä¸€ä¸‹è¿™10æ¡åºåˆ—çš„å¹³å‡å›æŠ¥
                pbar.set_postfix({
                    'episode':
                    '%d' % (num_episodes / 10 * i + i_episode + 1),
                    'return':
                    '%.3f' % np.mean(return_list[-10:])
                })
            pbar.update(1)

episodes_list = list(range(len(return_list)))
plt.plot(episodes_list, return_list)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('Sarsa on {}'.format('Cliff Walking'))
plt.show()


#Sarsa ç®—æ³•å¾—åˆ°çš„ç­–ç•¥åœ¨å„ä¸ªçŠ¶æ€ä¸‹ä¼šä½¿æ™ºèƒ½ä½“é‡‡å–ä»€ä¹ˆæ ·çš„åŠ¨ä½œ
def print_agent(agent, env, action_meaning, disaster=[], end=[]):
    for i in range(env.nrow):
        for j in range(env.ncol):
            if (i * env.ncol + j) in disaster:
                print('****', end=' ')
            elif (i * env.ncol + j) in end:
                print('EEEE', end=' ')
            else:
                a = agent.best_action(i * env.ncol + j)
                pi_str = ''
                for k in range(len(action_meaning)):
                    pi_str += action_meaning[k] if a[k] > 0 else 'o'
                print(pi_str, end=' ')
        print()


action_meaning = ['^', 'v', '<', '>']
print('Sarsaç®—æ³•æœ€ç»ˆæ”¶æ•›å¾—åˆ°çš„ç­–ç•¥ä¸ºï¼š')
print_agent(agent, env, action_meaning, list(range(37, 47)), [47])
```



### 4.4 å¤šæ­¥Sarsaç®—æ³•

1. å¯¹æ¯”

|              |                           åå·®                           |                             æ–¹å·®                             |
| :----------: | :------------------------------------------------------: | :----------------------------------------------------------: |
| è’™ç‰¹å¡æ´›æ–¹æ³• | æ— åï¼ˆåˆ©ç”¨å½“å‰çŠ¶æ€ä¹‹åæ¯ä¸€æ­¥çš„å¥–åŠ±è€Œä¸ä½¿ç”¨ä»»ä½•ä»·å€¼ä¼°è®¡ï¼‰ | è¾ƒå¤§çš„æ–¹å·®ï¼ˆæ¯ä¸€æ­¥çš„çŠ¶æ€è½¬ç§»éƒ½æœ‰ä¸ç¡®å®šæ€§ï¼Œè€Œæ¯ä¸€æ­¥çŠ¶æ€é‡‡å–çš„åŠ¨ä½œæ‰€å¾—åˆ°çš„ä¸ä¸€æ ·çš„å¥–åŠ±æœ€ç»ˆéƒ½ä¼šåŠ èµ·æ¥ï¼Œè¿™ä¼šæå¤§å½±å“æœ€ç»ˆçš„ä»·å€¼ä¼°è®¡ï¼‰ |
| æ—¶åºå·®åˆ†æ–¹æ³• |   æœ‰åï¼ˆç”¨åˆ°äº†ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ä¼°è®¡è€Œä¸æ˜¯å…¶çœŸå®çš„ä»·å€¼ï¼‰   |       å°æ–¹å·®ï¼ˆåªå…³æ³¨äº†ä¸€æ­¥çŠ¶æ€è½¬ç§»ï¼Œç”¨åˆ°äº†ä¸€æ­¥çš„å¥–åŠ±ï¼‰       |

2. ç®—æ³•ï¼š

å…ˆè®¡ç®—næ­¥çš„å¥–åŠ±ï¼š
$$
å°†G_t=r_t+\gamma Q(s_{t+1},a_{t+1})
$$

$$
æ›¿æ¢æˆ:
G_t=r_t+\gamma r_{t+1}+\cdots+\gamma^nQ(s_{t+n},a_{t+n})
$$

äºæ˜¯Sarsaä¸­çš„åŠ¨ä½œä»·å€¼å‡½æ•°å˜ä¸ºï¼š
$$
$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]
$$

$$
\text{æ›¿æ¢æˆ}Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma r_{t+1}+\cdots+\gamma^nQ(s_{t+n},a_{t+n})-Q(s_t,a_t)]
$$

è¿è¡Œåå‘ç°äº”æ­¥æ¯”å•æ­¥æ”¶æ•›é€Ÿåº¦æ›´å¿«

```python
class nstep_Sarsa:
    """ næ­¥Sarsaç®—æ³• """
    def __init__(self, n, ncol, nrow, epsilon, alpha, gamma, n_action=4):
        self.Q_table = np.zeros([nrow * ncol, n_action])
        self.n_action = n_action
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.n = n  # é‡‡ç”¨næ­¥Sarsaç®—æ³•
        self.state_list = []  # ä¿å­˜ä¹‹å‰çš„çŠ¶æ€
        self.action_list = []  # ä¿å­˜ä¹‹å‰çš„åŠ¨ä½œ
        self.reward_list = []  # ä¿å­˜ä¹‹å‰çš„å¥–åŠ±

    def take_action(self, state):
        if np.random.random() < self.epsilon:
            action = np.random.randint(self.n_action)
        else:
            action = np.argmax(self.Q_table[state])
        return action

    def best_action(self, state):  # ç”¨äºæ‰“å°ç­–ç•¥
        Q_max = np.max(self.Q_table[state])
        a = [0 for _ in range(self.n_action)]
        for i in range(self.n_action):
            if self.Q_table[state, i] == Q_max:
                a[i] = 1
        return a
    
#næ­¥å®ç°è¿‡ç¨‹
    def update(self, s0, a0, r, s1, a1, done):
        self.state_list.append(s0)
        self.action_list.append(a0)
        self.reward_list.append(r)
        if len(self.state_list) == self.n:  # è‹¥ä¿å­˜çš„æ•°æ®å¯ä»¥è¿›è¡Œnæ­¥æ›´æ–°
            G = self.Q_table[s1, a1]  # å¾—åˆ°Q(s_{t+n}, a_{t+n})
            for i in reversed(range(self.n)):
                G = self.gamma * G + self.reward_list[i]  # ä¸æ–­å‘å‰è®¡ç®—æ¯ä¸€æ­¥çš„å›æŠ¥
                # å¦‚æœåˆ°è¾¾ç»ˆæ­¢çŠ¶æ€,æœ€åå‡ æ­¥è™½ç„¶é•¿åº¦ä¸å¤Ÿnæ­¥,ä¹Ÿå°†å…¶è¿›è¡Œæ›´æ–°
                if done and i > 0:
                    s = self.state_list[i]
                    a = self.action_list[i]
                    self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
            s = self.state_list.pop(0)  # å°†éœ€è¦æ›´æ–°çš„çŠ¶æ€åŠ¨ä½œä»åˆ—è¡¨ä¸­åˆ é™¤,ä¸‹æ¬¡ä¸å¿…æ›´æ–°
            a = self.action_list.pop(0)
            self.reward_list.pop(0)
            # næ­¥Sarsaçš„ä¸»è¦æ›´æ–°æ­¥éª¤
            self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
        if done:  # å¦‚æœåˆ°è¾¾ç»ˆæ­¢çŠ¶æ€,å³å°†å¼€å§‹ä¸‹ä¸€æ¡åºåˆ—,åˆ™å°†åˆ—è¡¨å…¨æ¸…ç©º
            self.state_list = []
            self.action_list = []
            self.reward_list = []
            
np.random.seed(0)
n_step = 5  # 5æ­¥Sarsaç®—æ³•
alpha = 0.1
epsilon = 0.1
gamma = 0.9
agent = nstep_Sarsa(n_step, ncol, nrow, epsilon, alpha, gamma)
num_episodes = 500  # æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿è¡Œçš„åºåˆ—çš„æ•°é‡

return_list = []  # è®°å½•æ¯ä¸€æ¡åºåˆ—çš„å›æŠ¥
for i in range(10):  # æ˜¾ç¤º10ä¸ªè¿›åº¦æ¡
    #tqdmçš„è¿›åº¦æ¡åŠŸèƒ½
    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:
        for i_episode in range(int(num_episodes / 10)):  # æ¯ä¸ªè¿›åº¦æ¡çš„åºåˆ—æ•°
            episode_return = 0
            state = env.reset()
            action = agent.take_action(state)
            done = False
            while not done:
                next_state, reward, done = env.step(action)
                next_action = agent.take_action(next_state)
                episode_return += reward  # è¿™é‡Œå›æŠ¥çš„è®¡ç®—ä¸è¿›è¡ŒæŠ˜æ‰£å› å­è¡°å‡
                agent.update(state, action, reward, next_state, next_action,
                             done)
                state = next_state
                action = next_action
            return_list.append(episode_return)
            if (i_episode + 1) % 10 == 0:  # æ¯10æ¡åºåˆ—æ‰“å°ä¸€ä¸‹è¿™10æ¡åºåˆ—çš„å¹³å‡å›æŠ¥
                pbar.set_postfix({
                    'episode':
                    '%d' % (num_episodes / 10 * i + i_episode + 1),
                    'return':
                    '%.3f' % np.mean(return_list[-10:])
                })
            pbar.update(1)

episodes_list = list(range(len(return_list)))
plt.plot(episodes_list, return_list)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('5-step Sarsa on {}'.format('Cliff Walking'))
plt.show()
```



### 4.5 Q-Learning

1. ç®—æ³•ï¼š

(1) æ—¶åºå·®åˆ†æ›´æ–°æ–¹å¼ï¼š
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[R_t+\gamma\max_aQ(s_{t+1},a_{t+1}-Q(s_t,a_t)]
$$
(2) æµç¨‹ï¼š

![image-20250518200109792](./Reinforcement Learning.assets/image-20250518200109792.png)

(3) ä»£ç ï¼š

```python
class QLearning:
    """ Q-learningç®—æ³• """
    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):
        self.Q_table = np.zeros([nrow * ncol, n_action])  # åˆå§‹åŒ–Q(s,a)è¡¨æ ¼
        self.n_action = n_action  # åŠ¨ä½œä¸ªæ•°
        self.alpha = alpha  # å­¦ä¹ ç‡
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # epsilon-è´ªå©ªç­–ç•¥ä¸­çš„å‚æ•°

    def take_action(self, state):  #é€‰å–ä¸‹ä¸€æ­¥çš„æ“ä½œ
        if np.random.random() < self.epsilon:
            action = np.random.randint(self.n_action)
        else:
            action = np.argmax(self.Q_table[state])
        return action

    def best_action(self, state):  # ç”¨äºæ‰“å°ç­–ç•¥
        Q_max = np.max(self.Q_table[state])
        a = [0 for _ in range(self.n_action)]
        for i in range(self.n_action):
            if self.Q_table[state, i] == Q_max:
                a[i] = 1
        return a

    def update(self, s0, a0, r, s1):
        td_error = r + self.gamma * self.Q_table[s1].max(
        ) - self.Q_table[s0, a0]
        self.Q_table[s0, a0] += self.alpha * td_error
        
        
np.random.seed(0)
epsilon = 0.1
alpha = 0.1
gamma = 0.9
agent = QLearning(ncol, nrow, epsilon, alpha, gamma)
num_episodes = 500  # æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿è¡Œçš„åºåˆ—çš„æ•°é‡

return_list = []  # è®°å½•æ¯ä¸€æ¡åºåˆ—çš„å›æŠ¥
for i in range(10):  # æ˜¾ç¤º10ä¸ªè¿›åº¦æ¡
    # tqdmçš„è¿›åº¦æ¡åŠŸèƒ½
    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:
        for i_episode in range(int(num_episodes / 10)):  # æ¯ä¸ªè¿›åº¦æ¡çš„åºåˆ—æ•°
            episode_return = 0
            state = env.reset()
            done = False
            while not done:
                action = agent.take_action(state)
                next_state, reward, done = env.step(action)
                episode_return += reward  # è¿™é‡Œå›æŠ¥çš„è®¡ç®—ä¸è¿›è¡ŒæŠ˜æ‰£å› å­è¡°å‡
                agent.update(state, action, reward, next_state)
                state = next_state
            return_list.append(episode_return)
            if (i_episode + 1) % 10 == 0:  # æ¯10æ¡åºåˆ—æ‰“å°ä¸€ä¸‹è¿™10æ¡åºåˆ—çš„å¹³å‡å›æŠ¥
                pbar.set_postfix({
                    'episode':
                    '%d' % (num_episodes / 10 * i + i_episode + 1),
                    'return':
                    '%.3f' % np.mean(return_list[-10:])
                })
            pbar.update(1)

episodes_list = list(range(len(return_list)))
plt.plot(episodes_list, return_list)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('Q-learning on {}'.format('Cliff Walking'))
plt.show()

action_meaning = ['^', 'v', '<', '>']
print('Q-learningç®—æ³•æœ€ç»ˆæ”¶æ•›å¾—åˆ°çš„ç­–ç•¥ä¸ºï¼š')
print_agent(agent, env, action_meaning, list(range(37, 47)), [47])
```





#### 2. åœ¨çº¿ç­–ç•¥å’Œç¦»çº¿ç­–ç•¥ï¼š

(1) Sarsa - on-policyï¼šæ›´æ–°å…¬å¼å¿…é¡»ä½¿ç”¨å½“å‰è´ªå¿ƒç­–ç•¥é‡‡æ ·å¾—åˆ°çš„äº”å…ƒç»„(s, a, r, s', a')æ¥æ›´æ–°å½“å‰çŠ¶æ€åŠ¨ä½œå¯¹çš„ä»·å€¼Q(s', a').

(å…¶ä¸­çš„a'æ˜¯å½“å‰ç­–ç•¥åœ¨s'ä¸‹çš„åŠ¨ä½œ)



(2) Q-Learning -off-policyï¼šæ›´æ–°å…¬å¼ä½¿ç”¨å››å…ƒç»„(s, a, r, s')æ¥æ›´æ–°å½“å‰çŠ¶æ€åŠ¨ä½œå¯¹çš„ä»·å€¼Q(s, a)ã€‚

(å…¶ä¸­s, aæ˜¯ç»™å®šçš„æ¡ä»¶ï¼Œrå’Œs'å‡æ˜¯ç¯å¢ƒä¸­é‡‡æ ·ï¼Œå› æ­¤ä¸éœ€è¦ä¸€å®šæ˜¯å½“å‰ç­–ç•¥é‡‡æ ·å¾—åˆ°çš„æ•°æ®ï¼Œä¹Ÿå¯ä»¥æ˜¯è‡ªè¡Œä¸ºç­–ç•¥)

![image-20250518203642167](./Reinforcement Learning.assets/image-20250518203642167.png)



(3) æ¦‚å¿µï¼šåœ¨çº¿ç­–ç•¥ï¼ˆon-policyï¼‰ç®—æ³•è¡¨ç¤ºè¡Œä¸ºç­–ç•¥å’Œç›®æ ‡ç­–ç•¥æ˜¯åŒä¸€ä¸ªç­–ç•¥ï¼›è€Œç¦»çº¿ç­–ç•¥ï¼ˆoff-policyï¼‰ç®—æ³•è¡¨ç¤ºè¡Œä¸ºç­–ç•¥å’Œç›®æ ‡ç­–ç•¥ä¸æ˜¯åŒä¸€ä¸ªç­–ç•¥

è¡Œä¸ºç­–ç•¥ï¼šé‡‡æ ·æ•°æ®çš„ç­–ç•¥

ç›®æ ‡ç­–ç•¥ï¼šç”¨è¿™äº›æ•°æ®æ¥æ›´æ–°çš„ç­–ç•¥



## Chapter 5 Dyna-Q Algorithm

### 5.1  Dyna-Q(åŸºäºæ¨¡å‹) 

1. ç®—æ³•ï¼š

* ä½¿ç”¨Q-planning çš„æ–¹æ³•æ¥åŸºäºæ¨¡å‹ç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿæ•°æ®ï¼Œç„¶åç”¨æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®ä¸€èµ·æ”¹è¿›ç­–ç•¥ã€‚

* åœ¨æ¯æ¬¡ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’æ‰§è¡Œä¸€æ¬¡ Q-learning ä¹‹åï¼ŒDyna-Q ä¼šåšnæ¬¡ Q-planning:

Q-planning æ¯æ¬¡é€‰å–ä¸€ä¸ªæ›¾ç»è®¿é—®è¿‡çš„çŠ¶æ€$s$ï¼Œé‡‡å–ä¸€ä¸ªæ›¾ç»åœ¨è¯¥çŠ¶æ€ä¸‹æ‰§è¡Œè¿‡çš„åŠ¨ä½œ$a$

ï¼Œé€šè¿‡æ¨¡å‹å¾—åˆ°è½¬ç§»åçš„çŠ¶æ€$s'$ä»¥åŠå¥–åŠ±$r$ï¼Œå¹¶æ ¹æ®è¿™ä¸ªæ¨¡æ‹Ÿæ•°æ®(s, a, r, s')ï¼Œç”¨ Q-

learning çš„æ›´æ–°æ–¹å¼æ¥æ›´æ–°åŠ¨ä½œä»·å€¼å‡½æ•°ã€‚

![image-20250523215054723](./Reinforcement Learning.assets/image-20250523215054723-1748008256498-26.png)

2. ä»£ç ï¼š

```python
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import random
import time


class CliffWalkingEnv:
    def __init__(self, ncol, nrow):
        self.nrow = nrow
        self.ncol = ncol
        self.x = 0  # è®°å½•å½“å‰æ™ºèƒ½ä½“ä½ç½®çš„æ¨ªåæ ‡
        self.y = self.nrow - 1  # è®°å½•å½“å‰æ™ºèƒ½ä½“ä½ç½®çš„çºµåæ ‡

    def step(self, action):  # å¤–éƒ¨è°ƒç”¨è¿™ä¸ªå‡½æ•°æ¥æ”¹å˜å½“å‰ä½ç½®
        # 4ç§åŠ¨ä½œ, change[0]:ä¸Š, change[1]:ä¸‹, change[2]:å·¦, change[3]:å³ã€‚åæ ‡ç³»åŸç‚¹(0,0)
        # å®šä¹‰åœ¨å·¦ä¸Šè§’
        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]
        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))
        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))
        next_state = self.y * self.ncol + self.x
        reward = -1
        done = False
        if self.y == self.nrow - 1 and self.x > 0:  # ä¸‹ä¸€ä¸ªä½ç½®åœ¨æ‚¬å´–æˆ–è€…ç›®æ ‡
            done = True
            if self.x != self.ncol - 1:
                reward = -100
        return next_state, reward, done

    def reset(self):  # å›å½’åˆå§‹çŠ¶æ€,èµ·ç‚¹åœ¨å·¦ä¸Šè§’
        self.x = 0
        self.y = self.nrow - 1
        return self.y * self.ncol + self.x
   
class DynaQ:
    """ Dyna-Qç®—æ³• """
    def __init__(self,
                 ncol,
                 nrow,
                 epsilon,
                 alpha,
                 gamma,
                 n_planning,
                 n_action=4):
        self.Q_table = np.zeros([nrow * ncol, n_action])  # åˆå§‹åŒ–Q(s,a)è¡¨æ ¼
        self.n_action = n_action  # åŠ¨ä½œä¸ªæ•°
        self.alpha = alpha  # å­¦ä¹ ç‡
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # epsilon-è´ªå©ªç­–ç•¥ä¸­çš„å‚æ•°

        self.n_planning = n_planning  #æ‰§è¡ŒQ-planningçš„æ¬¡æ•°, å¯¹åº”1æ¬¡Q-learning
        self.model = dict()  # ç¯å¢ƒæ¨¡å‹

    def take_action(self, state):  # é€‰å–ä¸‹ä¸€æ­¥çš„æ“ä½œ
        if np.random.random() < self.epsilon:
            action = np.random.randint(self.n_action)
        else:
            action = np.argmax(self.Q_table[state])
        return action

    def q_learning(self, s0, a0, r, s1):
        td_error = r + self.gamma * self.Q_table[s1].max(
        ) - self.Q_table[s0, a0]
        self.Q_table[s0, a0] += self.alpha * td_error

    def update(self, s0, a0, r, s1):
        self.q_learning(s0, a0, r, s1)
        self.model[(s0, a0)] = r, s1  # å°†æ•°æ®æ·»åŠ åˆ°æ¨¡å‹ä¸­
        for _ in range(self.n_planning):  # Q-planningå¾ªç¯
            # éšæœºé€‰æ‹©æ›¾ç»é‡åˆ°è¿‡çš„çŠ¶æ€åŠ¨ä½œå¯¹
            (s, a), (r, s_) = random.choice(list(self.model.items()))
            self.q_learning(s, a, r, s_)
            
def DynaQ_CliffWalking(n_planning):
    ncol = 12
    nrow = 4
    env = CliffWalkingEnv(ncol, nrow)
    epsilon = 0.01
    alpha = 0.1
    gamma = 0.9
    agent = DynaQ(ncol, nrow, epsilon, alpha, gamma, n_planning)
    num_episodes = 300  # æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿è¡Œå¤šå°‘æ¡åºåˆ—

    return_list = []  # è®°å½•æ¯ä¸€æ¡åºåˆ—çš„å›æŠ¥
    for i in range(10):  # æ˜¾ç¤º10ä¸ªè¿›åº¦æ¡
        # tqdmçš„è¿›åº¦æ¡åŠŸèƒ½
        with tqdm(total=int(num_episodes / 10),
                  desc='Iteration %d' % i) as pbar:
            for i_episode in range(int(num_episodes / 10)):  # æ¯ä¸ªè¿›åº¦æ¡çš„åºåˆ—æ•°
                episode_return = 0
                state = env.reset()
                done = False
                while not done:
                    action = agent.take_action(state)
                    next_state, reward, done = env.step(action)
                    episode_return += reward  # è¿™é‡Œå›æŠ¥çš„è®¡ç®—ä¸è¿›è¡ŒæŠ˜æ‰£å› å­è¡°å‡
                    agent.update(state, action, reward, next_state)
                    state = next_state
                return_list.append(episode_return)
                if (i_episode + 1) % 10 == 0:  # æ¯10æ¡åºåˆ—æ‰“å°ä¸€ä¸‹è¿™10æ¡åºåˆ—çš„å¹³å‡å›æŠ¥
                    pbar.set_postfix({
                        'episode':
                        '%d' % (num_episodes / 10 * i + i_episode + 1),
                        'return':
                        '%.3f' % np.mean(return_list[-10:])
                    })
                pbar.update(1)
    return return_list

np.random.seed(0)
random.seed(0)
n_planning_list = [0, 2, 20]
for n_planning in n_planning_list:
    print('Q-planningæ­¥æ•°ä¸ºï¼š%d' % n_planning)
    time.sleep(0.5)
    return_list = DynaQ_CliffWalking(n_planning)
    episodes_list = list(range(len(return_list)))
    plt.plot(episodes_list,
             return_list,
             label=str(n_planning) + ' planning steps')
plt.legend()
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('Dyna-Q on {}'.format('Cliff Walking'))
plt.show()
```



# Deep Reforcement Learning

[CS 285: Lecture 1, Introduction. Part 2](https://www.youtube.com/watch?v=BYh36cb92JQ&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=2)

[berkeleydeeprlcourse/homework_fall2023](https://github.com/berkeleydeeprlcourse/homework_fall2023)

[Welcome to Spinning Up in Deep RL! â€” Spinning Up documentation](https://spinningup.openai.com/en/latest/)

## 

## Chapter 6 DQN Algorithm

### 6.1 DQN

1. å‡½æ•°æ‹Ÿåˆï¼š

ç±»ä¼¼è½¦æ†çš„ç¯å¢ƒä¸­å¾—åˆ°åŠ¨ä½œä»·å€¼å‡½æ•°Q(s, a)ï¼Œç”±äºçŠ¶æ€æ¯ä¸€ç»´åº¦çš„å€¼éƒ½æ˜¯è¿ç»­çš„ï¼Œæ— æ³•ä½¿ç”¨è¡¨æ ¼è®°å½•ï¼Œå› æ­¤ä¸€ä¸ªå¸¸è§çš„è§£å†³æ–¹æ³•ä¾¿æ˜¯ä½¿ç”¨**å‡½æ•°æ‹Ÿåˆ**



2. ç¥ç»ç½‘ç»œï¼š

* åŠ¨ä½œè¿ç»­ï¼šè¾“å…¥ä¸ºçŠ¶æ€så’ŒåŠ¨ä½œaï¼Œè¾“å‡ºæ ‡é‡ï¼Œè¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹é‡‡å–åŠ¨ä½œaèƒ½è·å¾—çš„ä»·å€¼ã€‚

* åŠ¨ä½œç¦»æ•£ï¼šè¾“å…¥ä»…ä¸ºçŠ¶æ€sï¼Œå¹¶è¾“å‡ºæ¯ä¸€ä¸ªåŠ¨ä½œçš„Qå€¼ã€‚

* Qç½‘ç»œï¼šæ¯ä¸€ä¸ªçŠ¶æ€sä¸‹æ‰€æœ‰å¯èƒ½åŠ¨ä½œaçš„Qå€¼ä¸º$Q_\omega(s,a)$ï¼Œå…¶ä¸­$\omega$ä¸ºç¥ç»ç½‘ç»œç”¨æ¥æ‹Ÿåˆå‡½æ•°çš„å‚æ•°ã€‚

  ç”¨äºæ‹ŸåˆQå‡½æ•°çš„ç¥ç»ç½‘ç»œï¼ˆDQNç”±äºå…¶å‡½æ•°Qåœ¨æ›´æ–°è¿‡ç¨‹ä¸­æœ‰maxaæ“ä½œï¼Œå› æ­¤åªèƒ½å¤„ç†åŠ¨ä½œç¦»æ•£çš„æƒ…å†µï¼‰

<img src="./Reinforcement Learning.assets/640.46b13e89.png" alt="img" style="zoom: 80%;" />

3. æ·±åº¦Qç½‘ç»œ(DQN)ï¼š

$$
Q(s,a)\leftarrow Q(s,a)+\alpha\left[r+\gamma\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})-Q(s,a)\right]
$$

ä¸Šè¿°å…¬å¼ä½¿ç”¨TDå­¦ä¹ ç›®æ ‡$r+\gamma\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})$æ¥å¢é‡å¼æ›´æ–°Q(s, a)ï¼Œå› æ­¤éœ€**è¦è®©Q(s, a)å’ŒTDç›®æ ‡$r+\gamma\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})$é è¿‘**ï¼Œå¯ä»¥è®¾è®¡Qç½‘ç»œçš„æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·®çš„å½¢å¼ï¼š
$$
\omega^*=\arg\min_\omega\frac{1}{2N}\sum_{i=1}^N\left[Q_\omega\left(s_i,a_i\right)-\left(r_i+\gamma\max_{a^{\prime}}Q_\omega\left(s_i^{\prime},a^{\prime}\right)\right)\right]^2
$$




4. æ¨¡å—è§£é‡Šï¼š

(1) ç»éªŒå›æ”¾ï¼š

ç»´æŠ¤ä¸€ä¸ª**å›æ”¾ç¼“å†²åŒº**ï¼Œå°†æ¯æ¬¡ä»ç¯å¢ƒä¸­é‡‡æ ·å¾—åˆ°çš„å››å…ƒç»„æ•°æ®ï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼‰å­˜å‚¨åˆ°å›æ”¾ç¼“å†²åŒºï¼Œè®­ç»ƒQç½‘ç»œçš„æ—¶å€™å†ä»å›æ”¾ç¼“å†²åŒºä¸­éšæœºé‡‡æ ·è‹¥å¹²æ•°æ®æ¥è®­ç»ƒï¼Œä»è€Œå®ç°ï¼š

* ä½¿æ ·æœ¬æ»¡è¶³ç‹¬ç«‹å‡è®¾ã€‚åœ¨ MDP ä¸­äº¤äº’é‡‡æ ·å¾—åˆ°çš„æ•°æ®æœ¬èº«ä¸æ»¡è¶³ç‹¬ç«‹å‡è®¾ï¼Œå› ä¸ºè¿™ä¸€æ—¶åˆ»çš„çŠ¶æ€å’Œä¸Šä¸€æ—¶åˆ»çš„çŠ¶æ€æœ‰å…³ã€‚éç‹¬ç«‹åŒåˆ†å¸ƒçš„æ•°æ®å¯¹è®­ç»ƒç¥ç»ç½‘ç»œæœ‰å¾ˆå¤§çš„å½±å“ï¼Œä¼šä½¿ç¥ç»ç½‘ç»œæ‹Ÿåˆåˆ°æœ€è¿‘è®­ç»ƒçš„æ•°æ®ä¸Šã€‚é‡‡ç”¨ç»éªŒå›æ”¾å¯ä»¥æ‰“ç ´æ ·æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œè®©å…¶æ»¡è¶³ç‹¬ç«‹å‡è®¾ã€‚
* æé«˜æ ·æœ¬æ•ˆç‡ã€‚æ¯ä¸€ä¸ªæ ·æœ¬å¯ä»¥è¢«ä½¿ç”¨å¤šæ¬¡ï¼Œååˆ†é€‚åˆæ·±åº¦ç¥ç»ç½‘ç»œçš„æ¢¯åº¦å­¦ä¹ ã€‚



(2) ç›®æ ‡ç½‘ç»œï¼š

åœ¨æ›´æ–°ç½‘ç»œå‚æ•°æ—¶ç›®æ ‡ä¹Ÿåœ¨ä¸æ–­æ”¹å˜ï¼Œå› æ­¤å…ˆå°†TDç›®æ ‡ä¸­çš„Qç½‘ç»œå›ºå®šï¼Œéœ€è¦ä¸¤å¥—Qç½‘ç»œã€‚

* åŸæ¥çš„**è®­ç»ƒç½‘ç»œ**$Q_\omega(s,a)$ï¼šç”¨äºè®¡ç®—åŸæ¥çš„æŸå¤±å‡½æ•°$\frac{1}{2}[Q_{\omega}\left(s,a\right)-\left(r+\gamma\max_{a^{\prime}}Q_{\omega^{-}}\left(s^{\prime},a^{\prime}\right)\right)]^{2}$ä¸­çš„$Q_\omega(s,a)$ï¼Œå¹¶ä½¿ç”¨æ­£å¸¸æ¢¯åº¦ä¸‹é™æ–¹æ³•æ¥è¿›è¡Œæ›´æ–°ã€‚
* **ç›®æ ‡ç½‘ç»œ**$Q_{\omega^-}(s,a)$ï¼šç”¨äºè®¡ç®—åŸå…ˆæŸå¤±å‡½æ•°$\frac{1}{2}[Q_{\omega}\left(s,a\right)-\left(r+\gamma\max_{a^{\prime}}Q_{\omega^{-}}\left(s^{\prime},a^{\prime}\right)\right)]^{2}$ä¸­çš„$(r+\gamma\max_{a^{\prime}}Q_{\omega^{-}}(s^{\prime},a^{\prime}))$é¡¹ï¼Œå…¶ä¸­$\omega^-$è¡¨ç¤ºç›®æ ‡ç½‘ç»œä¸­çš„å‚æ•°ã€‚

âœ¨ä¸ºäº†è®©æ›´æ–°ç›®æ ‡ç¨³å®šï¼Œï¼š

ç›®æ ‡ç½‘ç»œä½¿ç”¨è®­ç»ƒç½‘ç»œçš„ä¸€å¥—è¾ƒæ—§çš„å‚æ•°ï¼Œè®­ç»ƒç½‘ç»œåœ¨è®­ç»ƒä¸­çš„æ¯

ä¸€æ­¥éƒ½ä¼šæ›´æ–°ï¼Œè€Œç›®æ ‡ç½‘ç»œçš„å‚æ•°æ¯éš”Cæ­¥æ‰ä¼šä¸è®­ç»ƒç½‘ç»œåŒæ­¥ä¸€

æ¬¡ï¼ˆ$\omega^{-}\leftarrow\omega$ï¼‰



5. ç®—æ³•æµç¨‹ï¼š

<img src="./Reinforcement Learning.assets/image-20250527150321952.png" alt="image-20250527150321952" style="zoom:150%;" />





### 6.2 DQNæ”¹è¿›ç®—æ³•

1. Double DQN

(1) æ™®é€šDQNç®—æ³•ä¼šå¯¼è‡´å¯¹Qå€¼çš„è¿‡é«˜ä¼°è®¡ï¼š

ä¼ ç»ŸDQNçš„TDä¼˜åŒ–ç›®æ ‡    $Q_{\omega^-}\left(s^\prime,\arg\max_{a^\prime}Q_{\omega^-}\left(s^\prime,a^\prime\right)\right)$

å½“ä¸¤éƒ¨åˆ†é‡‡ç”¨åŒä¸€ä¸ªQç½‘ç»œè¿›è¡Œè®¡ç®—æ—¶ï¼Œå¾—åˆ°çš„éƒ½æ˜¯ç¥ç»ç½‘ç»œå½“å‰ä¼°ç®—çš„æ‰€æœ‰åŠ¨ä½œä»·å€¼ä¸­çš„æœ€å¤§å€¼ã€‚

ç”±äºç¥ç»ç½‘ç»œè®¡ç®—çš„Qå€¼ä¼šæœ‰æ­£å‘æˆ–è´Ÿå‘çš„è¯¯å·®ï¼Œåœ¨DQNçš„æ›´æ–°æ–¹å¼ä¸‹ç¥ç»ç½‘ç»œä¼šå°†æ­£å‘è¯¯å·®ç´¯è®¡ã€‚

EGï¼š

![image-20250531154604373](./Reinforcement Learning.assets/image-20250531154604373.png)



(2) double DQNç®—æ³•ï¼š

âœ¨ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¼°ç®—ä»·å€¼å‡½æ•°$Q_{\omega^-}\left(s^\prime,\arg\max_{a^\prime}Q_{\omega}\left(s^\prime,a^\prime\right)\right)$

* è®­ç»ƒç½‘ç»œ$Q_{\omega}$çš„è¾“å‡ºé€‰å–ä»·å€¼æœ€å¤§çš„åŠ¨ä½œï¼š

$$
a^*=\arg\max_{a^\prime}Q_{\omega}\left(s^\prime,a^\prime\right)
$$



* ç›®æ ‡ç½‘ç»œ$Q_{\omega^-}$è®¡ç®—è¯¥åŠ¨ä½œçš„ä»·å€¼ï¼š

$$
Q_{\omega^{-}}\left(s^{\prime},\arg\max_{a^{\prime}}Q_{\omega}\left(s^{\prime},a^{\prime}\right)\right)
$$



* Double DQNçš„ä¼˜åŒ–ç›®æ ‡ï¼š

$$
r+\gamma Q_{\omega^-}\left(s^{\prime},\arg\max_{a^{\prime}}Q_\omega\left(s^{\prime},a^{\prime}\right)\right)
$$



2. Dueling DQN

(1) ä¼˜åŠ¿å‡½æ•°ï¼šåŒä¸€çŠ¶æ€ä¸‹ï¼Œæ‰€æœ‰åŠ¨ä½œçš„ä¼˜åŠ¿å€¼ä¹‹å’Œä¸º0ï¼Œå³æ‰€æœ‰åŠ¨ä½œçš„åŠ¨ä½œä»·å€¼çš„æœŸæœ›å°±æ˜¯è¿™ä¸ªçŠ¶æ€çš„çŠ¶æ€ä»·å€¼ã€‚
$$
A(s,a)=Q(s,a)-V(s)
$$


(2) Dueling DQNï¼š
$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)
$$
å…¶ä¸­ï¼ŒVä¸ºçŠ¶æ€ä»·å€¼å‡½æ•°ï¼›Aä¸ºè¯¥çŠ¶æ€ä¸‹é‡‡å–ä¸åŒåŠ¨ä½œçš„ä¼˜åŠ¿å‡½æ•°ï¼Œè¡¨ç¤ºé‡‡å–ä¸åŒåŠ¨ä½œçš„å·®å¼‚æ€§ã€‚



(3) ç½‘ç»œæ¶æ„ï¼š

è®­ç»ƒç¥ç»ç½‘ç»œçš„æœ€åå‡ å±‚çš„ä¸¤ä¸ªåˆ†æ”¯ï¼Œåˆ†åˆ«è¾“å‡ºçŠ¶æ€ä»·å€¼å‡½æ•°å’Œä¼˜åŠ¿å‡½æ•°ï¼Œå†æ±‚å’Œå¾—åˆ°Qå€¼ã€‚

![img](./Reinforcement Learning.assets/640.455bc383.png)



(4) ä¸å”¯ä¸€æ€§é—®é¢˜ï¼š

* ä»…ä»…æ˜¯ç®€å•åœ°å°† V(s) å’Œ A(s,a) ç›¸åŠ å¾—åˆ° Q(s,a)ï¼Œé‚£ä¹ˆå­˜åœ¨æ— é™å¤šç»„ (V(s),A(s,a)) çš„ç»„åˆå¯ä»¥å¾—åˆ°ç›¸åŒçš„ Q(s,a) 

â€‹       EGï¼šå¦‚æœç¥ç»ç½‘ç»œå‘ç°æŸä¸ª Q(s,a) è¢«ä½ä¼°äº†ï¼Œå®ƒå¯ä»¥é€‰æ‹©å¢åŠ  V(s)ï¼Œæˆ–è€…å¢åŠ  A(s,a)ï¼Œæˆ–è€…åŒæ—¶å¢åŠ ä¸¤è€…çš„ä¸€éƒ¨åˆ†ï¼Œè¿™å¯¼è‡´äº† V(s) å’Œ A(s,a) çš„ä¼°è®¡å¯èƒ½å˜å¾—ä¸ç¨³å®šæˆ–ä¸å‡†ç¡®ã€‚

* å‡å»æœ€å¤§ä¼˜åŠ¿å€¼ï¼šå¼ºåˆ¶æœ€ä¼˜åŠ¨ä½œçš„ä¼˜åŠ¿å‡½æ•°çš„å®é™…è¾“å‡ºä¸º0 ã€‚$V(s)=\max_aQ(s,a)$

$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-\max_{a^{\prime}}A_{\eta,\beta}\begin{pmatrix}s,a^{\prime}\end{pmatrix}
$$



* å‡å»å¹³å‡ä¼˜åŠ¿å€¼ï¼š$\sum_{a^\prime}(A(s,a^\prime;\theta,\alpha)-\frac{1}{|\mathcal{A}|}\sum_{a^{\prime\prime}}A(s,a^{\prime\prime};\theta,\alpha))=0$ å°†ä¼˜åŠ¿å€¼ç›¸å¯¹äºä»–ä»¬çš„å¹³å‡å€¼è¿›è¡Œä¸­å¿ƒåŒ–ï¼Œæ¶ˆé™¤äº†A(s, a)ä¸­çš„ä»»æ„å¸¸æ•°åç§»ï¼Œä»è€ŒV(s)èƒ½å¤Ÿå”¯ä¸€åœ°è¡¨ç¤ºçŠ¶æ€çš„çœŸå®ä»·å€¼ã€‚$V(s)=\frac{1}{|\mathcal{A}|}\sum_{a^{\prime}}Q(s,a^{\prime})$

$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-\frac{1}{|\mathcal{A}|}\sum_{a^{\prime}}A_{\eta,\beta}\left(s,a^{\prime}\right)
$$



## Chapter 7 ç­–ç•¥æ¢¯åº¦ç®—æ³•

**åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼šç›´æ¥æ˜¾å¼åœ°å­¦ä¹ ä¸€ä¸ªç›®æ ‡ç­–ç•¥**

### 7.1 ç­–ç•¥æ¢¯åº¦

1. å‡è®¾ç›®æ ‡ç­–ç•¥$\pi_{\theta}$æ˜¯ä¸€ä¸ªéšæœºæ€§ç­–ç•¥ï¼Œä¸”å¤„å¤„å¯å¾®ï¼Œå…¶ä¸­$\theta$æ˜¯å¯¹åº”çš„å‚æ•°ï¼Œå¯ä»¥ç”¨ä¸€ä¸ªçº¿æ€§æ¨¡å‹æˆ–ç¥ç»ç½‘ç»œæ¨¡å‹æ¥ä¸ºå…¶å»ºæ¨¡ï¼šè¾“å…¥æŸä¸ªçŠ¶æ€ï¼Œè¾“å‡ºä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œç›®æ ‡æ˜¯å¯»æ‰¾ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥å¹¶æœ€å¤§åŒ–è¿™ä¸ªç­–ç•¥åœ¨ç¯å¢ƒä¸­çš„æœŸæœ›å›æŠ¥ï¼ˆS0è¡¨ç¤ºåˆå§‹çŠ¶æ€ï¼‰


$$
J(\theta)=\mathbb{E}_{s_0}[V^{\pi_\theta}(s_0)]
$$

2. æ¢¯åº¦ä¸Šå‡ï¼šå°†ç›®æ ‡å‡½æ•°å¯¹$\theta$æ±‚å¯¼ï¼Œä»è€Œä½¿ç”¨**æ¢¯åº¦ä¸Šå‡**ä»è€Œæ‰¾åˆ°$\theta^*=\arg\max_\theta J(\theta)$æ¥æœ€å¤§åŒ–ç›®æ ‡å‡½æ•°(ç­‰ä»·äºè®©ç­–ç•¥æ›´å¤šåœ°å»é‡‡æ ·åˆ°å¸¦æ¥è¾ƒé«˜Qå€¼çš„åŠ¨ä½œ)ï¼Œä»è€Œå¾—åˆ°æœ€ä¼˜ç­–ç•¥

$$
\begin{aligned}\nabla_\theta J(\theta)&\propto\sum_{s\in S}\nu^{\pi_\theta}(s)\sum_{a\in A}Q^{\pi_\theta}(s,a)\nabla_\theta\pi_\theta(a|s)\\&=\sum_{s\in S}\nu^{\pi_\theta}(s)\sum_{a\in A}\pi_\theta(a|s)Q^{\pi_\theta}(s,a)\frac{\nabla_\theta\pi_\theta(a|s)}{\pi_\theta(a|s)}\\&=\mathbb{E}_{\pi_\theta}[Q^{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(a|s)]\end{aligned}
$$

3. åœ¨çº¿ç­–ç•¥ç®—æ³•ï¼šä¸Šå¼ä¸­æœŸæœ›Eçš„ä¸‹æ ‡ä¸º$\pi_{\theta}$ï¼Œæ•…å¿…é¡»ä½¿ç”¨å½“å‰ç­–ç•¥$\pi_{\theta}$é‡‡æ ·å¾—åˆ°çš„æ•°æ®æ¥è®¡ç®—æ¢¯åº¦ã€‚

4. Reinforceç®—æ³•ï¼šä½¿ç”¨è’™ç‰¹å¡æ´›ç®—æ³•ä¼°è®¡$Q^{\pi_\theta}(s,a)$





### 7.2 Reinforceç®—æ³•

1. è®¡ç®—ç­–ç•¥æ¢¯åº¦ï¼š

å¯¹äºä¸€ä¸ªæœ‰é™æ­¥æ•°çš„ç¯å¢ƒæ¥è¯´ï¼š
$$
\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^T\left(\sum_{t^\prime=t}^T\gamma^{t^\prime-t}r_{t^\prime}\right)\nabla_\theta\log\pi_\theta(a_t|s_t)\right]
$$
å…¶ä¸­ï¼ŒTæ˜¯å’Œç¯å¢ƒäº¤äº’çš„æœ€å¤§æ­¥æ•°



2. ç®—æ³•æµç¨‹ï¼š

![image-20250605212801214](./Reinforcement Learning.assets/image-20250605212801214-1749130085324-1.png)





3. è¯æ˜ï¼š

![image-20250608000308889](./Reinforcement Learning.assets/image-20250608000308889.png)







## Chapter 8 Actor-Critic ç®—æ³•

### 8.1 Actor-Critic

1. Introï¼š

æœ¬è´¨æ˜¯åŸºäºç­–ç•¥çš„ç®—æ³•ï¼Œä½†ä¼šé¢å¤–å­¦ä¹ ä»·å€¼å‡½æ•°ï¼Œä»è€Œå¸®åŠ©ç­–ç•¥å‡½æ•°æ›´å¥½åœ°å­¦ä¹ ã€‚

2. æ¢¯åº¦æ›´æ–°æ–¹å¼ï¼š

å°†ç­–ç•¥æ¢¯åº¦å†™æˆå¦‚ä¸‹å½¢å¼ï¼š
$$
g=\mathbb{E}\left[\sum_{t=0}^T\psi_t\nabla_\theta\log\pi_\theta(a_t|s_t)\right]
$$
$\psi_t$å½¢å¼å¦‚ä¸‹ï¼š
$$
\begin{aligned}&1.\sum_{t^{\prime}=0}^T\gamma^{t^{\prime}}r_{t^{\prime}}:\text{è½¨è¿¹çš„æ€»å›æŠ¥;}\\&2.\sum_{t^{\prime}=t}^T\gamma^{t^{\prime}-t}r_{t^{\prime}}:\text{åŠ¨ä½œ}a_t\text{ä¹‹åçš„å›æŠ¥;}\\&3.\sum_{t^{\prime}=t}^T\gamma^{t^{\prime}-t}r_{t^{\prime}}-b(s_t):\text{åŸºå‡†çº¿ç‰ˆæœ¬çš„æ”¹è¿›;}\\&4.Q^{\pi_\theta}(s_t,a_t):\text{åŠ¨ä½œä»·å€¼å‡½æ•°;}\\&5.A^{\pi_\theta}(s_t,a_t):\text{ä¼˜åŠ¿å‡½æ•°;}\\&6.r_t+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t):\text{æ—¶åºå·®åˆ†æ®‹å·®ã€‚}\end{aligned}
$$
(3) ç”±äºREINFORCEé€šè¿‡è’™ç‰¹å¡æ´›é‡‡æ ·æ–¹æ³•å¯¹ç­–ç•¥æ¢¯åº¦çš„ä¼°è®¡æ˜¯æ— åçš„ï¼Œä½†æ˜¯æ–¹å·®å¾ˆå¤§ã€‚å¯ä»¥å¼•å…¥åŸºçº¿å‡½æ•°b(st)æ¥å‡å°æ–¹å·®

(4) ä¼°è®¡åŠ¨ä½œä»·å€¼å‡½æ•°Qï¼Œä»£æ›¿è’™ç‰¹å¡æ´›é‡‡æ ·å¾—åˆ°çš„å›æŠ¥

(5) ä¼˜åŠ¿å‡½æ•°ï¼šå°†çŠ¶æ€ä»·å€¼å‡½æ•°Vä½œä¸ºåŸºçº¿ï¼ŒA  = Q - V

(6) æ—¶åºå·®åˆ†æ®‹å·®ï¼šQ = r + Î³V



3. ç®—æ³•æ­¥éª¤ï¼š

(1) Actor(ç­–ç•¥ç½‘ç»œ): ä¸ç¯å¢ƒäº¤äº’ï¼Œå¹¶åœ¨Criticä»·å€¼å‡½æ•°çš„æŒ‡å¯¼ä¸‹ç”¨**ç­–ç•¥æ¢¯åº¦**å­¦ä¹ ä¸€ä¸ªæ›´å¥½çš„ç­–ç•¥ã€‚



(2) Critic(ä»·å€¼ç½‘ç»œ): é€šè¿‡ Actor ä¸ç¯å¢ƒäº¤äº’æ”¶é›†çš„æ•°æ®å­¦ä¹ ä¸€ä¸ªä»·å€¼å‡½æ•°ï¼Œè¿™ä¸ªä»·å€¼å‡½æ•°ä¼šç”¨äºåˆ¤æ–­åœ¨å½“å‰çŠ¶æ€ä»€ä¹ˆåŠ¨ä½œæ˜¯å¥½çš„ï¼Œä»€ä¹ˆåŠ¨ä½œä¸æ˜¯å¥½çš„ï¼Œè¿›è€Œå¸®åŠ© Actor è¿›è¡Œç­–ç•¥æ›´æ–°ã€‚

æ›´æ–°æ–¹å¼ï¼šæ¢¯åº¦ä¸‹é™æ›´æ–°Criticä»·å€¼ç½‘ç»œå‚æ•°

ä»·å€¼å‡½æ•°çš„æŸå¤±å‡½æ•°ï¼š
$$
\mathcal{L}(\omega)=\frac{1}{2}(r+\gamma V_\omega(s_{t+1})-V_\omega(s_t))^2
$$
ä»·å€¼å‡½æ•°çš„æ¢¯åº¦ï¼šå°†ä¸Šè¿°$r+\gamma V_\omega(s_{t+1})$ä½œä¸ºæ—¶åºå·®åˆ†ç›®æ ‡ï¼Œä¸ä¼šäº§ç”Ÿæ¢¯åº¦æ¥æ›´æ–°ä»·å€¼å‡½æ•°
$$
\nabla_\omega\mathcal{L}(\omega)=-(r+\gamma V_\omega(s_{t+1})-V_\omega(s_t))\nabla_\omega V_\omega(s_t)
$$
![image-20250608163218142](./Reinforcement Learning.assets/image-20250608163218142.png)



## Chapter 9 TRPO ç®—æ³•

ç”±äºç­–ç•¥çš„æ”¹å˜å¯¼è‡´æ•°æ®åˆ†å¸ƒçš„æ”¹å˜ï¼Œè¿™å¤§å¤§å½±å“æ·±åº¦æ¨¡å‹å®ç°çš„ç­–ç•¥ç½‘ç»œçš„å­¦ä¹ æ•ˆæœï¼Œæ‰€ä»¥é€šè¿‡åˆ’å®šä¸€ä¸ªå¯ä¿¡ä»»çš„ç­–ç•¥å­¦ä¹ åŒºåŸŸï¼Œä¿è¯ç­–ç•¥å­¦ä¹ çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚

### 9.1 ç­–ç•¥ç›®æ ‡

1. Intro

Q:åŸºäºç­–ç•¥çš„æ–¹æ³•æ²¿ç€æ¢¯åº¦å»æ›´æ–°ç­–ç•¥å‚æ•°ï¼Œä½†æ˜¯å½“ç­–ç•¥ç½‘ç»œæ˜¯æ·±åº¦æ¨¡å‹ï¼Œæ²¿ç€ç­–ç•¥æ¢¯åº¦æ›´æ–°å‚æ•°ï¼Œä¼šç”±äºæ­¥é•¿å¤ªé•¿ï¼Œç­–ç•¥æ˜¾è‘—å˜å·®ï¼Œä»è€Œå½±å“è®­ç»ƒæ•ˆæœã€‚

åœ¨æ›´æ–°ç­–ç•¥æ—¶æ‰¾åˆ°ä¸€å—ä¿¡ä»»åŒºåŸŸï¼Œåœ¨è¿™ä¸ªåŒºåŸŸä¸Šæ›´æ–°ç­–ç•¥æ—¶èƒ½å¤Ÿå¾—åˆ°æŸç§ç­–ç•¥æ€§èƒ½çš„å®‰å…¨æ€§ä¿è¯



2. ç­–ç•¥ç›®æ ‡ï¼š

(1) å‡è®¾å½“å‰ç­–ç•¥ä¸º$\pi_{\theta}$ï¼Œå‚æ•°ä¸º$\theta$ï¼Œé‚£ä¹ˆä¼˜åŒ–çš„ç›®æ ‡ä¸ºå€ŸåŠ©å½“å‰çš„$\theta$æ‰¾åˆ°ä¸€ä¸ªæ›´ä¼˜çš„å‚æ•°$\theta^{\prime}$ï¼Œä½¿å¾—$J(\theta^{\prime})\geq J(\theta)$ã€‚ç”±äºåˆå§‹çŠ¶æ€S0çš„åˆ†å¸ƒä¸ç­–ç•¥æ— å…³ï¼Œå› æ­¤ä¸Šè¿°ç­–ç•¥$\pi_{\theta}$ä¸‹çš„ä¼˜åŒ–ç›®æ ‡$J(\theta)$å¯ä»¥å†™æˆåœ¨æ–°ç­–ç•¥$\pi_{\theta^{\prime}}$çš„æœŸæœ›å½¢å¼
$$
\begin{aligned}J(\theta)&=\mathbb{E}_{s_0}[V^{\pi_\theta}(s_0)]\\&=\mathbb{E}_{\pi_{\theta^{\prime}}}\left[\sum_{t=0}^\infty\gamma^tV^{\pi_\theta}(s_t)-\sum_{t=1}^\infty\gamma^tV^{\pi_\theta}(s_t)\right]\\&=-\mathbb{E}_{\pi_{\theta^{\prime}}}\left[\sum_{t=0}^\infty\gamma^t\left(\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t)\right)\right]\end{aligned}
$$
æ–°æ—§ç­–ç•¥çš„ç›®æ ‡å‡½æ•°å·®è·ï¼š
$$
\begin{aligned}J(\theta^{\prime})-J(\theta)&=\mathbb{E}_{s_0}\left[V^{\pi_{\theta^{\prime}}}(s_0)\right]-\mathbb{E}_{s_0}\left[V^{\pi_\theta}(s_0)\right]\\&=\mathbb{E}_{\pi_{\theta^{\prime}}}\left[\sum_{t=0}^\infty\gamma^tr(s_t,a_t)\right]+\mathbb{E}_{\pi_{\theta^{\prime}}}\left[\sum_{t=0}^\infty\gamma^t\left(\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t)\right)\right]\\&=\mathbb{E}_{\pi_{\theta^{\prime}}}\left[\sum_{t=0}^\infty\gamma^t\left[r(s_t,a_t)+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t)\right]\right]\\&=\mathbb{E}_{\pi_{\theta^{\prime}}}\left[\sum_{t=0}^\infty\gamma^tA^{\pi_\theta}(s_t,a_t)\right](å°†æ—¶åºå·®åˆ†æ®‹å·®å®šä¹‰ä¸ºä¼˜åŠ¿å‡½æ•°A)\\&=\sum_{t=0}^\infty\gamma^t\mathbb{E}_{s_t\sim P_t^{\pi_{\theta^{\prime}}}}\mathbb{E}_{a_t\sim\pi_{\theta^{\prime}}(\cdot|s_t)}\left[A^{\pi_\theta}(s_t,a_t)\right]\\&=\frac{1}{1-\gamma}\mathbb{E}_{s\sim\nu^{\pi_{\theta^{\prime}}}}\mathbb{E}_{a\sim\pi_{\theta^{\prime}}(\cdot|s)}\left[A^{\pi_\theta}(s,a)\right]\end{aligned}
$$
(æœ€åä¸€ä¸ªç­‰å·ç”±çŠ¶æ€ç©ºé—´çš„è®¿é—®åˆ†å¸ƒå¾—åˆ°ï¼Œæ‰€ä»¥åªéœ€è¦æ‰¾åˆ°ä¸€ä¸ªæ–°ç­–ç•¥ï¼Œä½¿å¾—$\mathbb{E}_{s\sim\nu^{\pi_{\theta^{\prime}}}}\mathbb{E}_{a\sim\pi_{\theta^{\prime}}(\cdot|s)}\left[A^{\pi_{\theta}}(s,a)\right]\geq0$å°±èƒ½ä¿è¯ç­–ç•¥æ€§å•è°ƒé€’å¢ï¼Œå³$J(\theta^{\prime})\geq J(\theta)$)



(2) è¿‘ä¼¼å¤„ç†ï¼š

å½“æ–°æ—§ç­–ç•¥éå¸¸æ¥è¿‘æ—¶ï¼ŒçŠ¶æ€è®¿é—®åˆ†å¸ƒå˜åŒ–å¾ˆå°ï¼Œå¯ä»¥è¿‘ä¼¼å¿½ç•¥ï¼Œç›´æ¥é‡‡ç”¨æ—§çš„ç­–ç•¥$\pi_{\theta}$çš„çŠ¶æ€åˆ†å¸ƒï¼Œå®šä¹‰å¦‚ä¸‹æ›¿ä»£ä¼˜åŒ–ç›®æ ‡ï¼š
$$
L_\theta(\theta^{\prime})=J(\theta)+\frac{1}{1-\gamma}\mathbb{E}_{s\sim\nu^{\pi_\theta}}\mathbb{E}_{a\sim\pi_{\theta^{\prime}}(\cdot|s)}\left[A^{\pi_\theta}(s,a)\right]
$$
ç”¨é‡è¦æ€§é‡‡æ ·å¯¹åŠ¨ä½œåˆ†å¸ƒè¿›è¡Œå¤„ç†:
$$
L_\theta(\theta^{\prime})=J(\theta)+\mathbb{E}_{s\sim\nu^{\pi_\theta}}\mathbb{E}_{a\sim\pi_\theta(\cdot|s)}\left[\frac{\pi_{\theta^{\prime}}(a|s)}{\pi_\theta(a|s)}A^{\pi_\theta}(s,a)\right]
$$
æ¥ä¸‹æ¥åŸºäºæ—§ç­–ç•¥$\pi_{\theta}$å·²ç»é‡‡æ ·å‡ºæ¥çš„æ•°æ®æ¥ä¼°è®¡å¹¶ä¼˜åŒ–æ–°ç­–ç•¥$\pi_{\theta^{\prime}}$ï¼›ä¸ºäº†ä¿è¯æ–°æ—§ç­–ç•¥è¶³å¤Ÿæ¥è¿‘ï¼Œä½¿ç”¨Kullback-Leibleræ•£åº¦æ¥è¡¡é‡ç­–ç•¥ä¹‹é—´çš„è·ç¦»ï¼Œå¹¶ç»™å‡ºäº†æ•´ä½“çš„ä¼˜åŒ–å…¬å¼ï¼š
$$
\begin{aligned}&\max_{\theta^{\prime}}L_{\theta}(\theta^{\prime})\\&\mathrm{s.t.}\mathbb{E}_{s\sim\nu^{\pi_{\theta_{k}}}}[D_{KL}(\pi_{\theta_{k}}(\cdot|s),\pi_{\theta^{\prime}}(\cdot|s))]\leq\delta\end{aligned}
$$
ç”±ä¸Šè¿°ä¸ç­‰å¼ï¼Œå®šä¹‰äº†ç­–ç•¥ç©ºé—´çš„ä¸€ä¸ªKLçƒâ€”ä¿¡ä»»åŒºåŸŸã€‚åœ¨è¯¥åŒºåŸŸå†…ï¼Œå¯ä»¥è®¤ä¸ºå½“å‰å­¦ä¹ ç­–ç•¥å’Œç¯å¢ƒäº¤äº’çš„çŠ¶æ€åˆ†å¸ƒä¸ä¸Šä¸€è½®ç­–ç•¥æœ€åé‡‡æ ·çš„çŠ¶æ€åˆ†å¸ƒä¸€è‡´ï¼Œè¿›è€Œå¯ä»¥åŸºäºä¸€æ­¥è¡ŒåŠ¨çš„é‡è¦æ€§é‡‡æ ·æ–¹æ³•ä½¿å½“å‰å­¦ä¹ ç­–ç•¥ç¨³å®šæå‡ã€‚

![image-20250703233710192](./Reinforcement Learning.assets/image-20250703233710192.png)



### 9.2 ç®—æ³•ä¼˜åŒ–

#### 1.è¿‘ä¼¼æ±‚è§£

åšäº†ä¸€æ­¥è¿‘ä¼¼æ“ä½œï¼Œç”¨$\theta_k$ä»£æ›¿$\theta$(è¡¨ç¤ºç¬¬kæ¬¡è¿­ä»£åçš„ç­–ç•¥)ï¼Œå¯¹ç›®æ ‡å’Œçº¦æŸåœ¨$\theta_k$è¿›è¡Œ**æ³°å‹’å±•å¼€**ï¼Œåˆ†åˆ«ç”¨1é˜¶å’Œ2é˜¶è¿›è¡Œè¿‘ä¼¼ï¼š
$$
\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}\mathbb{E}_{a\sim\pi_{\theta_k}(\cdot|s)}\left[\frac{\pi_{\theta^{\prime}}(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)\right]\approx g^T(\theta^{\prime}-\theta_k)
$$

$$
\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(\cdot|s),\pi_{\theta^{\prime}}(\cdot|s))]\approx\frac{1}{2}(\theta^{\prime}-\theta_k)^TH(\theta^{\prime}-\theta_k)
$$

å…¶ä¸­ï¼Œ$g=\nabla_{\theta^{\prime}}\mathbb{E}_{s\sim\nu}^{\pi_{\theta_k}}\mathbb{E}_{a\sim\pi_{\theta_k}(\cdot|s)}\left[\frac{\pi_{\theta^{\prime}}(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)\right]$è¡¨ç¤ºç›®æ ‡å‡½æ•°çš„æ¢¯åº¦

$H=\mathbf{H}[\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(\cdot|s),\pi_{\theta^{\prime}}(\cdot|s))]$è¡¨ç¤ºç­–ç•¥ä¹‹å‰å¹³å‡KLè·ç¦»çš„é»‘å¡çŸ©é˜µï¼ˆHessian matrixï¼‰

äºæ˜¯ä¼˜åŒ–ç›®æ ‡å˜æˆäº†KLè·ç¦»çº¦æŸæ¡ä»¶ï¼š
$$
âœ¨\theta_{k+1}=\underset{\theta^{\prime}}{\operatorname*{\operatorname*{\arg\max}}}g^T(\theta^{\prime}-\theta_k)\quad\mathrm{s.t.}\quad\frac{1}{2}(\theta^{\prime}-\theta_k)^TH(\theta^{\prime}-\theta_k)\leq\delta
$$
ä½¿ç”¨Karush-Kuhn-Tuckerï¼ˆKKTï¼‰æ¡ä»¶ç›´æ¥å¯¼å‡ºä¸Šè¿°é—®é¢˜çš„è§£ï¼š
$$
âœ¨\theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g
$$


#### 2. å…±è½­æ¢¯åº¦

å‡è®¾æ»¡è¶³KLè·ç¦»çº¦æŸçš„å‚æ•°æ›´æ–°æ—¶çš„æœ€å¤§æ­¥é•¿ä¸º$\beta$(æ­¥é•¿ä¸º$\theta^{\prime}-\theta_k$)ï¼Œæ ¹æ®KLè·ç¦»çº¦æŸæ¡ä»¶ï¼Œæœ‰$\frac{1}{2}(\beta x)^TH(\beta x)=\delta$ï¼Œè§£å‡º$\beta=\sqrt{\frac{2\delta}{x^{T}Hx}}$ã€‚æ­¤æ—¶å‚æ•°æ›´æ–°æ–¹å¼ä¸ºï¼š
$$
âœ¨\theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{x^THx}}x
$$
æ­¤æ—¶ï¼Œç›´æ¥è®¡ç®—$x=H^{-1}g$ (xä¸ºå‚æ•°æ›´æ–°æ–¹å‘ï¼ŒHä¸ºæ­£å®šçŸ©é˜µ - å¯ä»¥é€šè¿‡å…±è½­æ¢¯åº¦æ³•æ±‚è§£)ï¼Œå³å¯æ›´æ–°å‚æ•°ã€‚

âœ¨**å…±è½­æ¢¯åº¦æ³•**

<img src="./Reinforcement Learning.assets/image-20250705165508844.png" alt="image-20250705165508844" style="zoom: 67%;" />

ä¸ºäº†å‡å°è®¡ç®—é‡ï¼Œåªéœ€è®¡ç®—Hxå‘é‡ï¼Œè€Œä¸æ˜¯ç›´æ¥è®¡ç®—å’Œå­˜å‚¨HçŸ©é˜µï¼Œå› ä¸ºå¯¹ä»»æ„çš„åˆ—å‘é‡vï¼Œå¯ä»¥éªŒè¯ä¸‹å¼ï¼Œå³å…ˆç”¨æ¢¯åº¦å’Œå‘é‡vç‚¹ä¹˜åè®¡ç®—æ¢¯åº¦ï¼š
$$
Hv=\nabla_\theta\left(\left(\nabla_\theta(D_{KL}^{\nu^{\pi_{\theta_k}}}(\pi_{\theta_k},\pi_{\theta^{\prime}}))\right)^T\right)v=\nabla_\theta\left(\left(\nabla_\theta(D_{KL}^{\nu^{\pi_{\theta_k}}}(\pi_{\theta_k},\pi_{\theta^{\prime}}))\right)^Tv\right)
$$


#### 3. çº¿æ€§æœç´¢

TRPOåœ¨æ¯æ¬¡è¿­ä»£çš„æœ€åè¿›è¡Œä¸€æ¬¡çº¿æ€§æœç´¢ï¼Œæ‰¾åˆ°ä¸€ä¸ªæœ€å°çš„éè´Ÿæ•´æ•°iï¼Œä½¿å¾—æŒ‰ç…§
$$
\theta_{k+1}=\theta_k+\alpha^i\sqrt{\frac{2\delta}{x^THx}}x       \\\alpha\in(0,1) å†³å®šçº¿æ€§æœç´¢é•¿åº¦
$$
æ±‚å‡ºçš„$\theta_{k+1}$ä¾ç„¶æ»¡è¶³æœ€åˆçš„KLæ•£åº¦é™åˆ¶ï¼Œå¹¶ä¸”èƒ½å¤Ÿæå‡ç›®æ ‡å‡½æ•°$L_{\theta_k}$



âœ¨**TRPOç®—æ³•æµç¨‹**

![image-20250706000203119](./Reinforcement Learning.assets/image-20250706000203119.png)



## Chapter 10 PPO

### 10.1 PPO-æƒ©ç½š

ç”¨æ‹‰æ ¼æœ—æ—¥æ•°ä¹˜æ³•ç›´æ¥å°†KLæ•£åº¦çš„é™åˆ¶æ”¾è¿›äº†ç›®æ ‡å‡½æ•°ä¸­ï¼Œä»è€Œå˜æˆäº†ä¸€ä¸ªæ— çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼Œåœ¨è¿­ä»£çš„è¿‡ç¨‹ä¸­ä¸æ–­æ›´æ–°KLæ•£åº¦å‰çš„ç³»æ•°ï¼š
$$
\arg\max_{\theta}\mathbb{E}_{s\sim\nu^{\pi_{\theta_k}}}\mathbb{E}_{a\sim\pi_{\theta_k}(\cdot|s)}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)-\beta D_{KL}[\pi_{\theta_k}(\cdot|s),\pi_\theta(\cdot|s)]\right]
$$
ä»¤$d_k=D_{KL}^{\nu^{\pi_{\theta_k}}}(\pi_{\theta_k},\pi_\theta)$ï¼Œ$\beta$(ç”¨äºé™åˆ¶å­¦ä¹ ç­–ç•¥å’Œä¹‹å‰ä¸€è½®ç­–ç•¥çš„å·®è·)çš„æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š

<img src="./Reinforcement Learning.assets/image-20250707001012933.png" alt="image-20250707001012933" style="zoom:67%;" />





### 10.2 PPO-æˆªæ–­

åœ¨ç›®æ ‡å‡½æ•°ä¸­è¿›è¡Œé™åˆ¶ï¼Œä»¥ä¿è¯æ–°çš„å‚æ•°å’Œæ—§çš„å‚æ•°å·®è·ä¸ä¼šå¤ªå¤§ï¼š
$$
\arg\max_{\theta}\mathbb{E}_{s\sim\nu}\mathbb{E}_{a\sim\pi_{\theta_k}(\cdot|s)}\left[\min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a),\mathrm{clip}\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},1-\epsilon,1+\epsilon\right)A^{\pi_{\theta_k}}(s,a)\right)\right]
$$
å…¶ä¸­$\operatorname{clip}(x,l,r):=\max(\min(x,r),l)$ å°†xé™åˆ¶åœ¨[l, r]å†…ã€‚ä¸Šå¼ä¸­$\epsilon$æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œè¡¨ç¤ºè¿›è¡Œæˆªæ–­çš„èŒƒå›´

è‹¥$A^{\pi_{\theta_k}}(s,a)>0$ï¼Œåˆ™è¿™ä¸ªåŠ¨ä½œçš„ä»·å€¼é«˜äºå¹³å‡ï¼Œæœ€å¤§åŒ–è¿™ä¸ªå¼å­ä¼šå¢å¤§$\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{k}}(a|s)}$ï¼Œä½†ä¸ä¼šè®©å…¶è¶…è¿‡$1-\epsilon$ï¼›

è‹¥$A^{\pi_{\theta_k}}(s,a)<0$ ,åˆ™æœ€å¤§åŒ–è¿™ä¸ªå¼å­ä¼šå‡å°$\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{k}}(a|s)}$ï¼Œä½†ä¸ä¼šè®©å…¶è¶…è¿‡$1-\epsilon$

![image-20250707205522927](./Reinforcement Learning.assets/image-20250707205522927.png)













# Imitation Learning
