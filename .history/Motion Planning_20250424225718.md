# Mobile Robots

[可视化Python](https://github.com/zhm-real/PathPlanning)

[MRPT](https://docs.mrpt.org/reference/latest/index.html)

## Chapter 1 Mapping

### 1.structure

<img src="./Motion Planning.assets/image-20250417095645143.png" alt="image-20250417095645143" style="zoom: 50%;" />

### 2.Occupancy grid map 

[grid map code](https://github.com/ANYbotics/grid_map)

* 原理：将平面地图通过grid进行分割，每个栅格用0,1表示（0 - free，1 - occupied）

  ![image-20250418120646301](./Motion Planning.assets/image-20250418120646301.png)

* Notation Defination

![image-20250418121448522](./Motion Planning.assets/image-20250418121448522.png)

<img src="./Motion Planning.assets/image-20250418122148617.png" alt="image-20250418122148617" style="zoom: 80%;" />

* Recursive Update

🙌推导：由贝叶斯公式：
$$
\begin{aligned}p(m_{i}|z_{1:t}) & =\frac{p(z_{t}|z_{1:t-1},m_{i})p(m_{i}|z_{1:t-1})}{p(z_{t}|z_{1:t-1})}\\  & \left(马尔科夫过程\right)=\frac{p(z_{t}|m_{i})p(m_{i}|z_{1:t-1})}{p(z_{t}|z_{1:t-1})}\\  & (贝叶斯展开)=\frac{p(m_{i}|z_{t})p(z_{t})}{p(m_{i})}\frac{p(m_{i}|z_{1:t-1})}{p(z_{t}|z_{1:t-1})}\end{aligned}
$$

$$
p(\overline{m_i}|z_{1:t})=\frac{p(\overline{m_i}|z_t)p(z_t)}{p(\overline{m_i})}\frac{p(\overline{m_i}|z_{1:t-1})}{p(z_t|z_{1:t-1})}
$$

 对上述两式相除，并取log：
$$
\log\frac{p(m_i|z_{1:t})}{p(\overline{m_i}|z_{1:t})}=\log\frac{p(m_i|z_t)}{p(\overline{m_i}|z_t)}\frac{p(\overline{m_i})}{p(m_i)}\frac{p(m_i|z_{1:t-1})}{p(\overline{m_i}|z_{1:t-1})}
$$
递归更新表达式：（化简log -> l）
$$
l_{t}(m_{i})=\log\frac{p(m_{i}|z_{t})}{p(\overline{m_{i}}|z_{t})}-l_0(m_i)+l_{t-1}(m_{i})
$$
其中，

P(z|m)是传感器模型 - 表示基于当前栅格状态得到当前观测值为0/1的概率

P(m|z)是逆传感器模型，为了约去它，做如下变换，并带入到递归更新表达式中：
$$
\begin{cases}p(m_i|z_t)=\frac{p(z_t|m_i)p(m_i)}{p(z_t)}\\p(\overline{m_i}|z_t)=\frac{p(z_t|\overline{m_i})p(\overline{m_i})}{p(z_t)}&\end{cases}
$$

$$
\log\frac{p(m_i|z_t)}{p(\overline{m_i|z_t})}=\log\frac{p(z_t|m_i)}{p(z_t|\overline{m_i})}+l_0(m_i)
$$

$$
✨l_t(m_i)=\log\frac{p(z_t|m_i)}{p(z_t|\overline{m_i})}+l_{t-1}(m_i)
$$

![image-20250418210446286](./Motion Planning.assets/image-20250418210446286.png)

（由于上述两个值均是常量，因此✨看作是简单的加和运算）



🤔函数分析：
$$
l_t(m_i)=\log\frac{p(m_i|z_{1:t})}{p(\overline{m_i}|z_{1:t})}=\log\frac{p(m_i|z_{1:t})}{1-p(m_i|z_{1:t})}
$$
<img src="./Motion Planning.assets/image-20250418211554907.png" alt="image-20250418211554907" style="zoom: 80%;" />



### 3.Octo map（基于八叉树）

[OctoMap code](https://github.com/OctoMap/octomap)



### 4.Voxel Hashing 

[VoxelHashing](https://github.com/niessner/VoxelHashing)

<img src="./Motion Planning.assets/image-20250418025053216.png" alt="image-20250418025053216" style="zoom: 67%;" />

### 5.PointCloud Map



### 6.TSDF map(Truncated Signed Distance Function) 

[OpenChisel.](https://github.com/personalrobotics/OpenChisel)

* 概述：一个三维的TSDF模型由 L×W×H 个三维小方块组成，这些三维小方块被称为[体素](https://zhida.zhihu.com/search?content_id=234249343&content_type=Article&match_order=1&q=体素&zhida_source=entity)（Voxel）。每个体			素内包含两个变量，一是用于生成重建表面的**tsdf值**，二是用于重建表面贴纹理的**RGB值**。

​					转换物理坐标系到体素坐标系：**int (** ( x − x0 ) / voxel.x , ( y − y0 ) / voxel.y , ( z − z0 ) / voxel.z **)** 



* 计算步骤：
  
1. **划分体素** 
   a. 建立完全包围待重建物体的长方体包围盒  
   b. 划分网络体素，对包围盒划分 n 等分  

   ![image-20250422192802474](./Motion%20Planning.assets/image-20250422192802474.png)

2. **TSDF 值计算**
   a.TSDF 值的前身是 SDF 值：  
   如下图，白灰色的小方格表示 TSDF 地图中的各个体素。蓝色的三角形表示相机的视场范围。图中间有一条绿色的截线，表示一个物体的截面。

      <img src="./Motion%20Planning.assets/v2-cd492bf5890102d89b5f026d3dfae3c8_1440w.jpg" alt="img" style="zoom: 80%;" />  

   b.计算 $voxel$在**物理坐标**下的位置：  
   记体素$x$在TSDF地图上的坐标 $(v_x, v_y, v_z)$, $x$在物理世界坐标系下的位置是：  
   $$
   P_{x,wrd} = \big(x_0 + v_x \cdot \mathrm{voxel.x},~ y_0 + v_y \cdot \mathrm{voxel.y},~ z_0 + v_z \cdot \mathrm{voxel.z}\big)
   $$

   c.计算体素$x$在**相机坐标**系下的位置：  

   设相机相对于物理坐标系下的位姿是 \(R\) 和 \(T\)，体素 \(x\) 在相机坐标系下的位置是：  
   $$
   P_{x,cam} = \mathbf{R}P_{x,wrd} + T
   $$

   d.计算体素 $x$相对于**相机的深度** $cam_z(x)$：  

   根据相机成像模型  
   $$
   \mathrm{cam}_z(x) \cdot I_x = \mathbf{K}P_{x,cam}
   $$
      （$K$ 表示相机的内参数矩阵，$I_x$ 表示体素$x$投影在相机成像平面下的像素坐标）  

      沿着相机的光心和体素 \(x\) 作一条直线（图中深蓝色粗线），这条线会与**物体的截面**有一个交点，这个交点记为 \(P\) 点。\(P\) 点的深度记为 \(d_P\)。记当前的**深度图**为 \(D\)，在实际计算中取 \(d_P = D(I_x)\)。那么体素 \(x\) 的 SDF 值就可以计算出来：  
   $$
   \mathrm{sdf}(x) = d_P - d_x = D(I_x) - \mathrm{cam}_z(x)
   $$

   e. 计算 TSDF 值：  
   
      $sdf(x) > 0$表示体素 \(x\) 位于相机和物体表面之间；$sdf(x) < 0$表  示体素 $x$ 处于物体表面之后。  

      TSDF 计算公式如下：  
   $$
   \mathrm{tsdf}(x) = \max[-1, \min(1, \mathrm{sdf}(x) / t)]
   $$

      **物理意义如下：**
      $t$可以看作是**体素 $x$** 和**截面对应点 $P$ 深度**差值的阈值。当体素离表面非常近的时候，TSDF 值接近于零；当体素离表面非常远的时候，TSDF 值趋于 1 或者 -1。初始化时设所有体素的 TSDF 值为 1，相当于这个 TSDF 地图中没有任何表面。

3. **当前帧与全局融合结果进行融合**




7. ESDF map [HKUST-Aerial-Robotics/FIESTA: Fast Incremental Euclidean Distance Fields for Online Motion Planning of Aerial Robots](https://github.com/HKUST-Aerial-Robotics/FIESTA)





## Chapter 2 Path Finding

### 1.1 Search-based Method

### 1.2 Sampling-based Method

### 1.3



## Chapter 2 







# Autonomus Cars
