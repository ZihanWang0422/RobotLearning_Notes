## Reinforcement Learning

[Python3 é¢å‘å¯¹è±¡ | èœé¸Ÿæ•™ç¨‹](https://www.runoob.com/python3/python3-class.html)

[è˜‘è‡ä¹¦EasyRL](https://datawhalechina.github.io/easy-rl/#/)

## Chapter 1 Intro

![alt text](image-1.png)



1.1å¥–åŠ±

ç”±ç¯å¢ƒç»™çš„ä¸€ç§æ ‡é‡çš„åé¦ˆä¿¡å·ï¼ˆscalar feedback signalï¼‰ï¼Œè¿™ç§ä¿¡å·å¯æ˜¾ç¤ºæ™ºèƒ½ä½“åœ¨æŸä¸€æ­¥é‡‡å–æŸä¸ªç­–ç•¥çš„è¡¨ç°å¦‚ä½•ã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®çš„å°±æ˜¯æœ€å¤§åŒ–æ™ºèƒ½ä½“å¯ä»¥è·å¾—çš„å¥–åŠ±ï¼Œæ™ºèƒ½ä½“åœ¨ç¯å¢ƒé‡Œé¢å­˜åœ¨çš„ç›® çš„å°±æ˜¯æœ€å¤§åŒ–å®ƒçš„æœŸæœ›çš„ç´¯ç§¯å¥–åŠ±ï¼ˆexpected cumulative rewardï¼‰ã€‚



1.2 æ™ºèƒ½ä½“ç»„æˆéƒ¨åˆ†

- **ç­–ç•¥ï¼ˆpolicyï¼‰**ï¼šå°†è¾“å…¥çš„çŠ¶æ€å˜æˆå¯èƒ½é‡‡å–çš„åŠ¨ä½œçš„æ¦‚ç‡ï¼Œé€šå¸¸é‡‡ç”¨éšæœºç­–ç•¥ï¼Œåœ¨å­¦ä¹ æ—¶å¯ä»¥é€šè¿‡å¼•å…¥ä¸€å®šçš„éšæœºæ€§æ¥æ›´å¥½åœ°æ¢ç´¢ç¯å¢ƒã€‚

**éšæœºæ€§ç­–ç•¥ï¼ˆÏ€å‡½æ•°ï¼‰**ï¼š$\pi(a|s)=p\left(a_{t}=a|s_{t}=s\right)$è¾“å…¥çŠ¶æ€sï¼Œè¾“å‡ºæ™ºèƒ½ä½“æ‰€æœ‰åŠ¨ä½œå¾—æ¦‚ç‡ã€‚

**ç¡®å®šæ€§ç­–ç•¥**ï¼š æ™ºèƒ½ä½“ç›´æ¥é‡‡å–æœ€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œ$a^*=\arg\max_a\pi(a\mid s)$



- **ä»·å€¼å‡½æ•°ï¼ˆvalue functionï¼‰**ã€‚æˆ‘ä»¬ç”¨ä»·å€¼å‡½æ•°æ¥å¯¹å½“å‰çŠ¶æ€è¿›è¡Œè¯„ä¼°ã€‚ä»·å€¼å‡½æ•°ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“è¿› å…¥æŸä¸ªçŠ¶æ€åï¼Œå¯ä»¥å¯¹åé¢çš„å¥–åŠ±å¸¦æ¥å¤šå¤§çš„å½±å“ã€‚ä»·å€¼å‡½æ•°å€¼è¶Šå¤§ï¼Œè¯´æ˜æ™ºèƒ½ä½“è¿›å…¥è¿™ä¸ªçŠ¶æ€è¶Šæœ‰åˆ©ã€‚

  â€‹

- **æ¨¡å‹ï¼ˆmodelï¼‰**ã€‚æ¨¡å‹è¡¨ç¤ºæ™ºèƒ½ä½“å¯¹ç¯å¢ƒçš„çŠ¶æ€è¿›è¡Œç†è§£ï¼Œå®ƒå†³å®šäº†ç¯å¢ƒä¸­ä¸–ç•Œçš„è¿è¡Œæ–¹å¼ã€‚Â 

  â€‹



1.3 æ™ºèƒ½ä½“ç±»å‹

* **åŸºäºä»·å€¼çš„æ™ºèƒ½ä½“**ï¼šæ˜¾å¼åœ°å­¦ä¹ ä»·å€¼å‡½æ•°ï¼Œéšå¼åœ°å­¦ä¹ å®ƒçš„ç­–ç•¥ã€‚ç­–ç•¥æ˜¯å…¶ä»å­¦åˆ°çš„ä»·å€¼å‡½æ•°é‡Œé¢æ¨ç®—å‡ºæ¥çš„ï¼ˆQ-learning/Sarsaï¼‰



* **åŸºäºç­–ç•¥çš„æ™ºèƒ½ä½“**ï¼šç›´æ¥å­¦ä¹ ç­–ç•¥ï¼Œæˆ‘ä»¬ç»™å®ƒä¸€ä¸ªçŠ¶æ€ï¼Œå®ƒå°±ä¼šè¾“å‡ºå¯¹åº”åŠ¨ä½œçš„æ¦‚ç‡ã€‚ï¼ˆPolicy Gradient/PGï¼‰

  â€‹

```python
import gymnasium as gym

#åˆ›å»ºä¸€ä¸ªenv å‚æ•°åŒ…æ‹¬åœºæ™¯ç±»å‹ã€æ¸²æŸ“æ¨¡å¼
env = gym.make("LunarLander-v3", render_mode="human")
observation, info = env.reset()

#é¦–å…ˆè®¾ç½®eposode_overä¸ºflaseï¼Œè¿›å…¥è®­ç»ƒå¾ªç¯
episode_over = False
while not episode_over:
    action = env.action_space.sample()  # agent policy that uses the observation and info
    observation, reward, terminated, truncated, info = env.step(action) #æ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œï¼Œæ™ºèƒ½ä½“ä»æ›´æ–°åçš„ç¯å¢ƒæ¥æ”¶åˆ°æ–°çš„è§‚æµ‹ä»¥åŠæ‰§è¡ŒåŠ¨ä½œçš„å¥–åŠ±ï¼›

    episode_over = terminated or truncated

env.close()
```



## Chapter 2Â Markov decision process(MDP)

### 2.1é©¬å°”ç§‘å¤«è¿‡ç¨‹

1ã€éšæœºè¿‡ç¨‹ï¼š

å·²çŸ¥å†å²ä¿¡æ¯ï¼ˆS1, ..., Stï¼‰æ—¶ä¸‹ä¸€ä¸ªæ—¶åˆ»çŠ¶æ€ä¸ºSt+1çš„æ¦‚ç‡è¡¨ç¤ºæˆ$P(S_{t+1}|S_1,\ldots,S_t)$

2ã€é©¬å°”ç§‘å¤«æ€§è´¨ï¼šï¼ˆ**çŠ¶æ€è½¬ç§»**ï¼‰

å½“ä¸”ä»…å½“æŸæ—¶åˆ»çš„çŠ¶æ€åªå–å†³äºä¸Šä¸€æ—¶åˆ»çš„çŠ¶æ€æ—¶$P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\ldots,S_t)$

3ã€é©¬å°”ç§‘å¤«é“¾ï¼š<S, P>

nä¸ªçŠ¶æ€ï¼šS =Â {S1, S2, ... , Sn}

çŠ¶æ€è½¬ç§»çŸ©é˜µPï¼š$\mathcal{P}=\begin{bmatrix}P(s_1|s_1)&\cdots&P(s_n|s_1)\\\vdots&\ddots&\vdots\\P(s_1|s_n)&\cdots&P(s_n|s_n)\end{bmatrix}$Â  ï¼ˆæ¯ä¸€è¡Œå’Œä¸º1ï¼‰



### 2.2é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ï¼ˆMRPï¼‰

1. é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ï¼š

$\langle\mathcal{s},\mathcal{P},r,\gamma\rangle$

* $\mathcal{s}$æ˜¯æœ‰é™çŠ¶æ€çš„é›†åˆã€‚

* $\mathcal{P}$æ˜¯çŠ¶æ€è½¬ç§»çŸ©é˜µã€‚

* $r$æ˜¯å¥–åŠ±å‡½æ•°ï¼Œ**æŸä¸ªçŠ¶æ€$s$çš„å¥–åŠ± $r(s)$æŒ‡è½¬ç§»åˆ°è¯¥çŠ¶æ€æ—¶å¯ä»¥è·å¾—å¥–åŠ±çš„æœŸæœ›**ã€‚

* $\gamma$æ˜¯æŠ˜æ‰£å› å­ (discount factor), $\gamma$çš„å–å€¼èŒƒå›´ä¸º[0,1)ã€‚å¼•å…¥æŠ˜æ‰£å› å­çš„ç†ç”±ä¸ºè¿œæœŸåˆ©ç›Šå…·æœ‰ä¸€å®šä¸ç¡®å®šæ€§ï¼Œæœ‰æ—¶æˆ‘ä»¬æ›´å¸Œæœ›èƒ½å¤Ÿå°½å¿«è·å¾—ä¸€äº›å¥–åŠ±ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹è¿œæœŸåˆ©ç›Šæ‰“ä¸€äº›æŠ˜æ‰£ã€‚æ¥è¿‘ 1 çš„Î³æ›´å…³æ³¨é•¿æœŸçš„ç´¯è®¡å¥–åŠ±ï¼Œæ¥è¿‘ 0 çš„Î³æ›´è€ƒè™‘çŸ­æœŸå¥–åŠ±ã€‚

  â€‹


#### 2. å›æŠ¥ï¼š

åœ¨ä¸€ä¸ªé©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ä¸­ï¼Œä»ç¬¬tæ—¶åˆ»çŠ¶æ€å¼€å§‹ï¼Œç›´åˆ°ç»ˆæ­¢çŠ¶æ€æ—¶ï¼Œæ‰€æœ‰å¥–åŠ±çš„è¡°å‡ä¹‹å’Œç§°ä¸º**å›æŠ¥**Gtï¼ˆReturnï¼‰

$$G_t=r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+\cdots=\sum_{k=0}^\infty\gamma^kr_{t+k}$$

å…¶ä¸­**r_tè¡¨ç¤ºåœ¨tæ—¶åˆ»è·å¾—çš„å¥–åŠ±**

```python 
import numpy as np
np.random.seed(0)
# å®šä¹‰çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µP
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0]  # å®šä¹‰å¥–åŠ±å‡½æ•°
gamma = 0.5  # å®šä¹‰æŠ˜æ‰£å› å­


# ç»™å®šä¸€æ¡åºåˆ—,è®¡ç®—ä»æŸä¸ªç´¢å¼•ï¼ˆèµ·å§‹çŠ¶æ€ï¼‰å¼€å§‹åˆ°åºåˆ—æœ€åï¼ˆç»ˆæ­¢çŠ¶æ€ï¼‰å¾—åˆ°çš„å›æŠ¥
def compute_return(start_index, chain, gamma):
    G = 0
    for i in range(len(chain)-1, -1, len(chain)):
        G = gamma * G + rewards[chain[i] - 1] #ä½¿ç”¨reversedå…ˆç®—chainæœ€åä¸€ä½ï¼Œæ¯ä¸€æ¬¡éƒ½ä¼šå¯¹å…¶è¿›è¡Œä¹˜gamma
    return G


# ä¸€ä¸ªçŠ¶æ€åºåˆ—,s1-s2-s3-s6
chain = [1, 2, 3, 6]
start_index = 0
G = compute_return(start_index, chain, gamma)
print("æ ¹æ®æœ¬åºåˆ—è®¡ç®—å¾—åˆ°å›æŠ¥ä¸ºï¼š%sã€‚" % G)
```



#### 3. ä»·å€¼å‡½æ•°ï¼š

ï¼ˆ1ï¼‰é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªçŠ¶æ€çš„æœŸæœ›å›æŠ¥ï¼ˆå³ä»è¿™ä¸ªçŠ¶æ€å‡ºå‘çš„æœªæ¥ç´¯ç§¯å¥–åŠ±çš„æœŸæœ›ï¼‰è¢«ç§°ä¸ºè¿™ä¸ªçŠ¶æ€çš„**ä»·å€¼**ï¼ˆvalueï¼‰ã€‚

ğŸ™Œæ³¨æ„ä»·å€¼æ˜¯é’ˆå¯¹æŸä¸ªèµ·å§‹çŠ¶æ€tçš„!!!

ï¼ˆ2ï¼‰æ‰€æœ‰çŠ¶æ€çš„ä»·å€¼å°±ç»„æˆäº†**ä»·å€¼å‡½æ•°**ï¼ˆvalue functionï¼‰**V(s)**ï¼Œä»·å€¼å‡½æ•°çš„è¾“å…¥ä¸ºæŸä¸ªçŠ¶æ€ï¼Œè¾“å‡ºä¸ºè¿™ä¸ªçŠ¶æ€çš„ä»·å€¼ã€‚

$$\begin{aligned}V(s)&=\mathbb{E}[G_t|s_t=s]\\&=\mathbb{E}[r_t+\gamma r_{t+1}+\gamma^2r_{t+2}+\ldots|s_t=s]\\&=\mathbb{E}[r_t+\gamma(r_{t+1}+\gamma r_{t+2}+\ldots)|s_t=s]\\&=\mathbb{E}[r_t+\gamma G_{t+1}|s_t=s]\\&=\mathbb{E}[r_t+\gamma V(s_{t+1})|s_t=s]\end{aligned}$$

å¯¹æœ€åä¸€å¼è¿›è¡Œåˆ†è§£ï¼Œå…¶ä¸­ï¼Œå³æ—¶å¥–åŠ±çš„æœŸæœ›ä¸ºå¥–åŠ±å‡½æ•°çš„è¾“å‡ºï¼š$\mathbb{E}[r_t|s_t=s]=R(s)$

å‰©ä½™éƒ¨åˆ†$\mathbb{E}[\gamma V(s_{t+1})|s_{t}=s]$Â æ ¹æ®ä»çŠ¶æ€så‡ºå‘çš„è½¬ç§»æ¦‚ç‡å¯ä»¥å¾—åˆ°ï¼›

å› æ­¤ç­‰å¼ç­‰ä»·ä¸º**è´å°”æ›¼æ–¹ç¨‹**ï¼š

âœ¨$V(s)=R(s)+\gamma\sum_{s^{\prime}\in S}p(s^{\prime}|s)V(s^{\prime})$

ï¼ˆ3ï¼‰çŸ©é˜µå½¢å¼

$$\mathcal{V}=\mathcal{R}+\gamma\mathcal{PV}\\\begin{bmatrix}V(s_1)\\V(s_2)\\\cdots\\V(s_n)\end{bmatrix}=\begin{bmatrix}r(s_1)\\r(s_2)\\\cdots\\r(s_n)\end{bmatrix}+\gamma\begin{bmatrix}P(s_1|s_1)&p(s_2|s_1)&\ldots&P(s_n|s_1)\\P(s_1|s_2)&P(s_2|s_2)&\ldots&P(s_n|s_2)\\\cdots\\P(s_1|s_n)&P(s_2|s_n)&\ldots&P(s_n|s_n)\end{bmatrix}\begin{bmatrix}V(s_1)\\V(s_2)\\\ldots\\V(s_n)\end{bmatrix}$$

è§£æè§£ï¼š$$\mathcal{V}=(I-\gamma\mathcal{P})^{-1}\mathcal{R}$$Â æ—¶é—´å¤æ‚åº¦ä¸ºO(n^3)

```python
def compute(P, rewards, gamma, states_num):
    ''' åˆ©ç”¨è´å°”æ›¼æ–¹ç¨‹çš„çŸ©é˜µå½¢å¼è®¡ç®—è§£æè§£,states_numæ˜¯MRPçš„çŠ¶æ€æ•° '''
    rewards = np.array(rewards).reshape((-1, 1))  #å°†rewardså†™æˆåˆ—å‘é‡å½¢å¼
    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P), rewards)
    return value


V = compute(P, rewards, gamma, 6)
print("MRPä¸­æ¯ä¸ªçŠ¶æ€ä»·å€¼åˆ†åˆ«ä¸º\n", V)
```



4. è¿­ä»£ç®—æ³•ï¼š

ï¼ˆ1ï¼‰è’™ç‰¹å¡æ´›é‡‡æ ·ç®—æ³•ï¼š(ç›¸å½“äºå¯¹å›æŠ¥æ±‚å‡å€¼)

å½“å¾—åˆ°ä¸€ä¸ªé©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹åï¼Œæˆ‘ä»¬å¯ä»¥ä»æŸä¸ªçŠ¶æ€å¼€å§‹ï¼ŒæŠŠå°èˆ¹æ”¾åˆ°çŠ¶æ€è½¬ç§»çŸ©é˜µé‡Œé¢ï¼Œè®©å®ƒâ€œéšæ³¢é€æµâ€ï¼Œè¿™æ ·å°±ä¼šäº§ç”Ÿä¸€ä¸ªè½¨è¿¹ã€‚äº§ç”Ÿä¸€ä¸ªè½¨è¿¹ä¹‹åï¼Œå°±ä¼šå¾—åˆ°ä¸€ä¸ªå¥–åŠ±ï¼Œé‚£ä¹ˆç›´æ¥æŠŠæŠ˜æ‰£çš„å¥–åŠ±å³å›æŠ¥ g ç®—å‡ºæ¥ã€‚ç®—å‡ºæ¥ä¹‹åå°†å®ƒç§¯ç´¯èµ·æ¥ï¼Œå¾—åˆ°å›æŠ¥Gtã€‚ å½“ç§¯ç´¯äº†ä¸€å®šæ•°é‡çš„è½¨è¿¹ä¹‹åï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ Gté™¤ä»¥è½¨è¿¹æ•°é‡ï¼Œå°±ä¼šå¾—åˆ°æŸä¸ªçŠ¶æ€çš„ä»·å€¼

![alt text](image-2.png)



ï¼ˆ2ï¼‰åŠ¨æ€è§„åˆ’ç®—æ³•ï¼š

é€šè¿‡**è‡ªä¸¾ï¼ˆbootstrappingï¼‰**çš„æ–¹æ³•ä¸åœåœ°è¿­ä»£è´å°”æ›¼æ–¹ç¨‹ï¼Œå½“æœ€åæ›´æ–°çš„çŠ¶æ€ä¸æˆ‘ä»¬ä¸Šä¸€ä¸ªçŠ¶æ€çš„åŒºåˆ«å¹¶ä¸å¤§çš„æ—¶å€™ï¼Œæ›´æ–°å°±å¯ä»¥åœæ­¢ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¾“å‡ºæœ€æ–°çš„ Vâ€²(s) ä½œä¸ºå®ƒå½“å‰çš„çŠ¶æ€çš„ä»·å€¼ã€‚

![alt text](image-3.png)



### 2.3 é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰

1. ç»„æˆï¼šMDP = MRP + agent's action

![alt text](image-4.png)

$$\langle\mathcal{s},\mathcal{a},P,r,\gamma\rangle$$

$$\begin{aligned}&\mathcal{s}\text{æ˜¯çŠ¶æ€çš„é›†åˆ;}\\&\mathcal{a}\text{æ˜¯åŠ¨ä½œçš„é›†åˆ;}\\&\gamma\text{æ˜¯æŠ˜æ‰£å› å­;}\\&r(s,a)\text{æ˜¯å¥–åŠ±å‡½æ•°,æ­¤æ—¶å¥–åŠ±å¯ä»¥åŒæ—¶å–å†³äºçŠ¶æ€}s\text{å’ŒåŠ¨ä½œ}a\text{,åœ¨å¥–åŠ±å‡½æ•°åªå–å†³äºçŠ¶æ€}s\text{æ—¶,åˆ™}\\&\text{é€€åŒ–ä¸º}r(s)\mathrm{;}\\&P(s^{\prime}|s,a)\text{æ˜¯çŠ¶æ€è½¬ç§»å‡½æ•°,è¡¨ç¤ºåœ¨çŠ¶æ€}s\text{æ‰§è¡ŒåŠ¨ä½œ}a\text{ä¹‹ååˆ°è¾¾çŠ¶æ€}s^{\prime}\text{çš„æ¦‚ç‡ã€‚}\end{aligned}$$



#### 2. ç­–ç•¥ï¼š

ï¼ˆ1ï¼‰**çŠ¶æ€ã€åŠ¨ä½œæ¦‚ç‡**ï¼š

**ç­–ç•¥**ï¼ˆåœ¨æŸä¸ªçŠ¶æ€å¯èƒ½é‡‡å–æŸä¸ªè¡ŒåŠ¨çš„æ¦‚ç‡ï¼‰ ï¼š$\pi(a\mid s)=p\left(a_t=a\mid s_t=s\right)$

çŠ¶æ€è½¬ç§»ï¼š$p\left(s_{t+1}=s^{\prime}\mid s_t=s,a_t=a\right)$

MDPæ»¡è¶³æ¡ä»¶ï¼š$p\left(s_{t+1}\mid s_t,a_t\right)=p\left(s_{t+1}\mid h_t,a_t\right)$



ï¼ˆ2ï¼‰ç­–ç•¥è½¬åŒ–ï¼š

å¯¹åŠ¨ä½œè¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°MRPçš„**çŠ¶æ€è½¬ç§»æ¦‚ç‡**:

Â $P_\pi\left(s^{\prime}\mid s\right)=\sum_{a\in A}\pi(a\mid s)p\left(s^{\prime}\mid s,a\right)$

å¥–åŠ±å‡½æ•°ï¼š

$r_\pi(s)=\sum_{a\in A}\pi(a\mid s)r(s,a)$



#### 3. ä»·å€¼å‡½æ•°ï¼š

Qå‡½æ•°ï¼ˆåŠ¨ä½œä»·å€¼å‡½æ•°ï¼‰ï¼šåœ¨æŸä¸€ä¸ªçŠ¶æ€é‡‡å–æŸä¸€ä¸ªåŠ¨ä½œï¼Œå®ƒæœ‰å¯èƒ½å¾—åˆ°çš„å›æŠ¥çš„æœŸæœ›ã€‚

$Q_\pi(s,a)=\mathbb{E}_\pi\left[G_t\mid s_t=s,a_t=a\right]$

ä»·å€¼å‡½æ•°ï¼šå¯¹Qå‡½æ•°ä¸­çš„åŠ¨ä½œè¿›è¡ŒåŠ å’Œã€‚

$V_\pi(s)=\mathbb{E}_\pi[G_t\mid s_t=s]=\sum_{a\in A}\pi(a\mid s)Q_\pi(s,aï¼‰$

åŒç†äºMRPçš„ä»·å€¼å‡½æ•°æ¨å¯¼ï¼Œå¯å¾—ï¼š

$\begin{aligned}Q_\pi(s,a)=R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_\pi\left(s^{\prime}\right) \end{aligned}$

å¾—åˆ°è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼š

âœ¨$V_\pi(s)=\sum_{a\in A}\pi(a\mid s)\left(R(s,a)+\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s,a\right)V_\pi\left(s^{\prime}\right)\right)$



## Chapter 3 Tabular method

### 3.1 MDP

1. æœ‰æ¨¡å‹ï¼š

ä½¿ç”¨æ¦‚ç‡å‡½æ•°ï¼ˆçŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼‰å’Œå¥–åŠ±å‡½æ•°æ¥æè¿°ç¯å¢ƒã€‚

2. å…æ¨¡å‹ï¼š

ä¸€ç³»åˆ—çš„å†³ç­–çš„æ¦‚ç‡å‡½æ•°å’Œå¥–åŠ±å‡½æ•°æ˜¯æœªçŸ¥çš„ï¼Œç”¨äºæ¨¡å‹æœªçŸ¥æˆ–è€…æ¨¡å‹å¾ˆå¤§æ—¶



### 3.2 Qè¡¨æ ¼

1. â€‹

![alt text](image-5.png)